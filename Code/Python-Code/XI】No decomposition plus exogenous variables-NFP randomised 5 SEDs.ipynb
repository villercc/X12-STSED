{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b0c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConvLSTM-SED   multivariable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.python.keras.layers import ConvLSTM2D\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#matplotlib inline\n",
    "import time\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "#dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/PD-TVFEMD-IMF1.csv', usecols=[6], engine='python')\n",
    "dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_NFP.csv', engine='python')\n",
    "#print(dataframe)\n",
    "print(\"数据集的长度：\",len(dataframe))\n",
    "dataset = dataframe.values\n",
    "# 将整型变为float\n",
    "dataset = dataset.astype('float32')\n",
    "#print(dataset)\n",
    "\n",
    "timestep = 6\n",
    "dim = 6\n",
    "#数据缩放 拆分输入X（7维）&输出Y（1维）\n",
    "X = dataset[:,0:6]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "for i in range(dim):\n",
    "    Xdata = X[:,i]\n",
    "    Xdata = Xdata.reshape(-1,1)\n",
    "    Xdata = scaler.fit_transform(Xdata)\n",
    "    Xdata = Xdata.flatten()\n",
    "    X[:,i] = Xdata\n",
    "X_scaler = X.reshape(324,-1)\n",
    "\n",
    "Y = dataset[:,0]\n",
    "Y_scaler= (Y-np.min(Y))/(np.max(Y)-np.min(Y))\n",
    "Y_scaler = Y_scaler.reshape(-1)\n",
    "\n",
    "#print(X_scaler)\n",
    "#print(Y_scaler)\n",
    "#print(Y)\n",
    "\n",
    "\n",
    "# 将数据拆分成训练和测试，8/9作为训练数据\n",
    "train_size = int(len(dataset) * 0.78)\n",
    "test_size = len(dataset) - train_size\n",
    "trainX, testX = X_scaler[0:train_size,:], X_scaler[train_size:len(dataset),:]\n",
    "trainY, testY = Y_scaler[0:train_size], Y_scaler[train_size:len(dataset)]\n",
    "print(\"原始训练集的长度：\",train_size)\n",
    "print(\"原始测试集的长度：\",test_size)\n",
    "#print(trainX)\n",
    "#print(trainY)\n",
    "#print(testX)\n",
    "#print(testY)\n",
    "\n",
    "\n",
    "\n",
    "#重构数据集\n",
    "##timestep为时间步长\n",
    "def create_trainX(seq, timestep):\n",
    "    dataX = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[i:(i+timestep)]\n",
    "        # X按照顺序取值 每次在后面增加一个数据\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "\n",
    "def create_trainY(seq, timestep):\n",
    "    dataY = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        # Y向后移动一位取值\n",
    "        dataY.append(seq[i + timestep])\n",
    "    return np.array(dataY)\n",
    "\n",
    "def create_testX(seq, timestep):\n",
    "    dataX = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[(i):(i+timestep)]\n",
    "        # X按照顺序取值 每次在后面增加一个数据\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "\n",
    "\n",
    "\n",
    "#----------forecasting-----------\n",
    "\n",
    "pres=[]\n",
    "trainX = create_trainX(trainX, timestep)\n",
    "trainY = create_trainY(trainY, timestep)\n",
    "testX = create_testX(testX, timestep) \n",
    "testY = testY[timestep:len(testY)]\n",
    "print(\"转为监督学习，训练集数据长度：\", len(trainX))\n",
    "#print(trainX,trainY)\n",
    "print(\"转为监督学习，测试集数据长度：\",len(testX))\n",
    "#print(testX, testY )\n",
    "\n",
    "\n",
    "# 数据重构为5D [samples, timesteps, rows, columns, features]\n",
    "trainX_input5D = np.reshape(trainX, (trainX.shape[0], 2,1,timestep//2, dim))\n",
    "testX_input5D = np.reshape(testX, (testX.shape[0],2,1,timestep//2, dim))\n",
    "#trainX_input5D = np.reshape(trainX, (trainX.shape[0], timestep,1,1, dim))\n",
    "#testX_input5D = np.reshape(testX, (testX.shape[0],timestep,1,1, dim))\n",
    "print('构造得到模型的输入数据(训练数据已有标签trainY): ',trainX_input5D.shape,testX_input5D.shape)\n",
    "\n",
    "\n",
    "# create and fit the convlstm network\n",
    "if __name__ == '__main__':\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=48, kernel_size=(2,2), activation='relu', \n",
    "                         input_shape=(2,1,timestep//2, dim),\n",
    "#                         input_shape=(timestep,1,1, dim),\n",
    "                         padding='same', return_sequences=True))\n",
    "    model.add(ConvLSTM2D(filters=24, kernel_size=(2,2), activation='relu',\n",
    "                         padding='same', return_sequences=True))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.fit(trainX_input5D, trainY, epochs=1000)\n",
    "\n",
    "    # 打印模型\n",
    "    model.summary()\n",
    "\n",
    "    # 开始预测\n",
    "    trainPredict = model.predict(trainX_input5D)\n",
    "    testPredict = model.predict(testX_input5D)\n",
    "\n",
    "    # 逆缩放预测值\n",
    "    dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_NFP.csv', engine='python')\n",
    "    dataset = dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "    Y = dataset[:,0]\n",
    "    #trainPredict = trainPredict*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori = trainY*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori=trainY_re.reshape((246,1))\n",
    "    testPredict = testPredict*((np.max(Y)-np.min(Y)))+np.min(Y)\n",
    "    testPre = testPredict.reshape(-1)\n",
    "    Y = dataset[:,0]\n",
    "    Y = Y[train_size+timestep:len(dataset)]\n",
    "    #testY_ori = testY*((np.max(dataset)-np.min(dataset)))+np.min(dataset)\n",
    "    #testY_ori = testY_ori.reshape((-1,1))\n",
    "    testY_ori = Y.reshape(-1)\n",
    "\n",
    "    \n",
    "\n",
    "    # 计算误差\n",
    "    #trainScore = math.sqrt(mean_squared_error(trainY_ori[:,0], trainPredict[:,0]))\n",
    "    #print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    #testScore = math.sqrt(mean_squared_error(testY_ori[:,0], testPredict[:,0]))\n",
    "    #print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "\n",
    "    error = []#Y-Y'\n",
    "    error1= []#abs((Y-Y')/Y)\n",
    "    error2= []#Y*Y'\n",
    "    squared1 =[]#Y*Y\n",
    "    squared2 =[]#Y'*Y'\n",
    "    for i in range(len(testY_ori)):\n",
    "        error.append(testY_ori[i] - testPre[i])\n",
    "        error1.append(abs((testY_ori[i] - testPre[i])/testY_ori[i]))\n",
    "        error2.append(testY_ori[i]*testPre[i])\n",
    "        squared1.append(testY_ori[i]*testY_ori[i])\n",
    "        squared2.append(testPre[i]*testPre[i])\n",
    "        \n",
    "        \n",
    "    squaredError = []#(Y-Y')^2\n",
    "    absError = []#abs(Y-Y')\n",
    "    for val in error:    \n",
    "        squaredError.append(val * val)#target-prediction之差平方     \n",
    "        absError.append(abs(val))#误差绝对值\n",
    "        \n",
    "    MSE=sum(squaredError) / len(squaredError)\n",
    "    meannn=np.mean(testY_ori)\n",
    "    NMSEerror = []\n",
    "    IAerror = []\n",
    "    for i in range(len(testY_ori)):\n",
    "        NMSEerror.append(squaredError[i]/error2[i])\n",
    "        IAerror.append((abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn))*(abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn)))\n",
    "    \n",
    "    U2error1 = []\n",
    "    U2error2 = []\n",
    "    for i in range(len(testY_ori)-1):\n",
    "        U2error1.append(((testY_ori[i+1] - testPre[i+1])/testY_ori[i])*((testY_ori[i+1] - testPre[i+1])/testY_ori[i]))\n",
    "        U2error2.append(((testY_ori[i+1] - testPre[i])/testY_ori[i])*((testY_ori[i+1] - testPre[i])/testY_ori[i]))\n",
    "    print('\\n==========================')\n",
    "    MAE=sum(absError)/len(absError)\n",
    "    print('MAE=',MAE)\n",
    "    from math import sqrt\n",
    "    RMSE=sqrt(MSE)\n",
    "    print(\"RMSE = \", RMSE)#均方根误差RMSE\n",
    "    NMSE=sum(NMSEerror)\n",
    "    print(\"NMSE = \", NMSE)#误差平方的归一化平均值NMSE\n",
    "    MAPE=sum(error1)/len(error1)\n",
    "    print('MAPE=',MAPE)\n",
    "    IA=1-sum(squaredError)/sum(IAerror)\n",
    "    print('IA=',IA)#一致性指数\n",
    "    U1index=RMSE/(sqrt(sum(squared1)/len(squared1))+sqrt(sum(squared2)/len(squared2)))\n",
    "    print('U1=',U1index)\n",
    "    U2index=(sqrt(sum(U2error1)/len(testY_ori)))/(sqrt(sum(U2error2)/len(testY_ori)))\n",
    "    print('U2=',U2index)\n",
    "\n",
    "    #print(testPredict)\n",
    "    testPre=testPre.reshape(-1)\n",
    "    testPre=pd.DataFrame(testPre)\n",
    "    testPre.to_csv('C:/Users/Administrator/Desktop/XI-2/data/pre_result/convlstmoutput.csv', header=False)\n",
    "    import csv\n",
    "    Evaluation_index = [MAE,RMSE,NMSE,MAPE,IA,U1index,U2index]\n",
    "    with open(\"C:/Users/Administrator/Desktop/XI-2/data/pre_result/convlstmEvaluation.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file ,delimiter=',')\n",
    "        writer.writerow(Evaluation_index)\n",
    "    \n",
    "    plt.figure(facecolor='white')\n",
    "    plt.plot(testPre,color='green', label='Predict')\n",
    "    plt.plot(testY_ori,color='pink', label='Original')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    pres.append(testPre)\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaf298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TSRConvLSTM-SED-(2,3)\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.python.keras.layers import ConvLSTM2D\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#matplotlib inline\n",
    "import time\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# load the dataset\n",
    "#dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/PD-IMF1.csv', usecols=[6], engine='python')\n",
    "dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_NFP.csv', engine='python')\n",
    "#print(dataframe)\n",
    "print(\"数据集的长度：\",len(dataframe))\n",
    "dataset = dataframe.values\n",
    "# 将整型变为float\n",
    "dataset = dataset.astype('float32')\n",
    "#print(dataset)\n",
    "X11 = dataset[:,0]#Y\n",
    "X11_scaler= (X11-numpy.min(X11))/(numpy.max(X11)-numpy.min(X11))\n",
    "X11_scaler = X11_scaler.reshape(-1)\n",
    "\n",
    "X12 = dataset[:,1]\n",
    "X12_scaler= (X12-numpy.min(X12))/(numpy.max(X12)-numpy.min(X12))\n",
    "X12_scaler = X12_scaler.reshape(-1)\n",
    "\n",
    "X13 = dataset[:,2]\n",
    "X13_scaler= (X13-numpy.min(X13))/(numpy.max(X13)-numpy.min(X13))\n",
    "X13_scaler = X13_scaler.reshape(-1)\n",
    "\n",
    "X21 = dataset[:,3]\n",
    "X21_scaler= (X21-numpy.min(X21))/(numpy.max(X21)-numpy.min(X21))\n",
    "X21_scaler = X21_scaler.reshape(-1)\n",
    "\n",
    "X22 = dataset[:,4]\n",
    "X22_scaler= (X22-numpy.min(X22))/(numpy.max(X22)-numpy.min(X22))\n",
    "X22_scaler = X22_scaler.reshape(-1)\n",
    "\n",
    "X23 = dataset[:,5]\n",
    "X23_scaler= (X23-numpy.min(X23))/(numpy.max(X23)-numpy.min(X23))\n",
    "X23_scaler = X23_scaler.reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "XX = numpy.zeros([324,2,3])\n",
    "XX[:,0,0] = X11_scaler\n",
    "XX[:,0,1] = X12_scaler\n",
    "XX[:,0,2] = X13_scaler\n",
    "XX[:,1,0] = X21_scaler\n",
    "XX[:,1,1] = X22_scaler\n",
    "XX[:,1,2] = X23_scaler\n",
    "\n",
    "# 将数据拆分成训练和测试，7/9作为训练数据\n",
    "train_size = int(len(dataset) * 0.78)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = XX[0:train_size,:,:], XX[train_size:len(dataset),:,:]\n",
    "print(\"原始训练集的长度：\",train_size)\n",
    "print(\"原始测试集的长度：\",test_size)\n",
    "\n",
    "\n",
    "# 切片\n",
    "def create_train(seq, timestep):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[i:(i + timestep),:,:]\n",
    "        # X按照顺序取值\n",
    "        dataX.append(a)\n",
    "        # Y向后移动一位取值\n",
    "        dataY.append(seq[i + timestep,:,:])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "def create_testX(seq, timestep):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[i:(i + timestep),:,:]\n",
    "        # X按照顺序取值\n",
    "        dataX.append(a)\n",
    "    return numpy.array(dataX)\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "\n",
    "\n",
    "#----------forecasting-----------\n",
    "timestep = 6\n",
    "pres=[]\n",
    "trainX,trainY = create_train(train, timestep)\n",
    "testX = create_testX(test, timestep) \n",
    "testY = test[timestep:len(X11),0,0]\n",
    "print(\"转为监督学习，训练集数据长度：\", len(trainX))\n",
    "#print(trainX,trainY)\n",
    "print(\"转为监督学习，测试集数据长度：\",len(testX))\n",
    "#print(testX, testY )\n",
    "\n",
    "\n",
    "# 数据重构为5D [samples, timesteps, rows, columns, features]\n",
    "trainX_input5D = numpy.reshape(trainX, (trainX.shape[0], timestep,2,3, 1))\n",
    "testX_input5D = numpy.reshape(testX, (testX.shape[0],timestep, 2,3, 1))\n",
    "print('构造得到模型的输入数据(训练数据已有标签trainY): ',trainX_input5D.shape,testX_input5D.shape)\n",
    "\n",
    "\n",
    "# create and fit the convlstm network\n",
    "from tensorflow.python.keras.layers.convolutional import Conv3D ,Conv2D\n",
    "if __name__ == '__main__':\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=48, kernel_size=(2, 2), activation='relu', input_shape=(timestep, 2,3,1),\n",
    "                         padding='same',return_sequences=True))\n",
    "    model.add(ConvLSTM2D(filters=12, kernel_size=(1, 2),activation='relu',\n",
    "                         padding='same',return_sequences=False))\n",
    "    model.add(Conv2D(filters=1, kernel_size=(1, 1),\n",
    "               activation='sigmoid',\n",
    "               padding='same', data_format='channels_last'))\n",
    "#    model.add(Flatten())\n",
    "#    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.fit(trainX_input5D, trainY, epochs=1000)\n",
    "\n",
    "    # 打印模型\n",
    "    model.summary()\n",
    "\n",
    "    # 开始预测\n",
    "    trainPredict = model.predict(trainX_input5D)\n",
    "    testPredict = model.predict(testX_input5D)\n",
    "    \n",
    "    # 逆缩放预测值\n",
    "    testPredict1 = testPredict[:,0,0,:]\n",
    "    testPredict1 = testPredict1.reshape(-1)\n",
    "    dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_NFP.csv', engine='python')\n",
    "    dataset = dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "    Y = dataset[:,0]\n",
    "    #trainPredict = trainPredict*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori = trainY*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori=trainY_re.reshape((246,1))\n",
    "    testPredict = testPredict1*((np.max(Y)-np.min(Y)))+np.min(Y)\n",
    "    testPre = testPredict.reshape(-1)\n",
    "    Y = dataset[:,0]\n",
    "    Y = Y[train_size+timestep:len(dataset)]\n",
    "    #testY_ori = testY*((np.max(dataset)-np.min(dataset)))+np.min(dataset)\n",
    "    #testY_ori = testY_ori.reshape((-1,1))\n",
    "    testY_ori = Y.reshape(-1)\n",
    "\n",
    "    \n",
    "\n",
    "    # 计算误差\n",
    "    #trainScore = math.sqrt(mean_squared_error(trainY_ori[:,0], trainPredict[:,0]))\n",
    "    #print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    #testScore = math.sqrt(mean_squared_error(testY_ori[:,0], testPredict[:,0]))\n",
    "    #print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "\n",
    "    error = []#Y-Y'\n",
    "    error1= []#abs((Y-Y')/Y)\n",
    "    error2= []#Y*Y'\n",
    "    squared1 =[]#Y*Y\n",
    "    squared2 =[]#Y'*Y'\n",
    "    for i in range(len(testY_ori)):\n",
    "        error.append(testY_ori[i] - testPre[i])\n",
    "        error1.append(abs((testY_ori[i] - testPre[i])/testY_ori[i]))\n",
    "        error2.append(testY_ori[i]*testPre[i])\n",
    "        squared1.append(testY_ori[i]*testY_ori[i])\n",
    "        squared2.append(testPre[i]*testPre[i])\n",
    "        \n",
    "        \n",
    "    squaredError = []#(Y-Y')^2\n",
    "    absError = []#abs(Y-Y')\n",
    "    for val in error:    \n",
    "        squaredError.append(val * val)#target-prediction之差平方     \n",
    "        absError.append(abs(val))#误差绝对值\n",
    "        \n",
    "    MSE=sum(squaredError) / len(squaredError)\n",
    "    meannn=np.mean(testY_ori)\n",
    "    NMSEerror = []\n",
    "    IAerror = []\n",
    "    for i in range(len(testY_ori)):\n",
    "        NMSEerror.append(squaredError[i]/error2[i])\n",
    "        IAerror.append((abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn))*(abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn)))\n",
    "    \n",
    "    U2error1 = []\n",
    "    U2error2 = []\n",
    "    for i in range(len(testY_ori)-1):\n",
    "        U2error1.append(((testY_ori[i+1] - testPre[i+1])/testY_ori[i])*((testY_ori[i+1] - testPre[i+1])/testY_ori[i]))\n",
    "        U2error2.append(((testY_ori[i+1] - testPre[i])/testY_ori[i])*((testY_ori[i+1] - testPre[i])/testY_ori[i]))\n",
    "    print('\\n==========================')\n",
    "    MAE=sum(absError)/len(absError)\n",
    "    print('MAE=',MAE)\n",
    "    from math import sqrt\n",
    "    RMSE=sqrt(MSE)\n",
    "    print(\"RMSE = \", RMSE)#均方根误差RMSE\n",
    "    NMSE=sum(NMSEerror)\n",
    "    print(\"NMSE = \", NMSE)#误差平方的归一化平均值NMSE\n",
    "    MAPE=sum(error1)/len(error1)\n",
    "    print('MAPE=',MAPE)\n",
    "    IA=1-sum(squaredError)/sum(IAerror)\n",
    "    print('IA=',IA)#一致性指数\n",
    "    U1index=RMSE/(sqrt(sum(squared1)/len(squared1))+sqrt(sum(squared2)/len(squared2)))\n",
    "    print('U1=',U1index)\n",
    "    U2index=(sqrt(sum(U2error1)/len(testY_ori)))/(sqrt(sum(U2error2)/len(testY_ori)))\n",
    "    print('U2=',U2index)\n",
    "\n",
    "    #print(testPredict)\n",
    "    testPre=testPre.reshape(-1)\n",
    "    testPre=pd.DataFrame(testPre)\n",
    "    testPre.to_csv('C:/Users/Administrator/Desktop/XI-2/data/pre_result/convlstmoutput.csv', header=False)\n",
    "    import csv\n",
    "    Evaluation_index = [MAE,RMSE,NMSE,MAPE,IA,U1index,U2index]\n",
    "    with open(\"C:/Users/Administrator/Desktop/XI-2/data/pre_result/convlstmEvaluation.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file ,delimiter=',')\n",
    "        writer.writerow(Evaluation_index)\n",
    "    \n",
    "    plt.figure(facecolor='white')\n",
    "    plt.plot(testPre,color='green', label='Predict')\n",
    "    plt.plot(testY_ori,color='pink', label='Original')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    pres.append(testPre)\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a93a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TSRConvLSTM-SED-(3,3)\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.python.keras.layers import ConvLSTM2D\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#matplotlib inline\n",
    "import time\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# load the dataset\n",
    "#dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/PD-IMF1.csv', usecols=[6], engine='python')\n",
    "dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_NFP.csv', engine='python')\n",
    "#print(dataframe)\n",
    "print(\"数据集的长度：\",len(dataframe))\n",
    "dataset = dataframe.values\n",
    "# 将整型变为float\n",
    "dataset = dataset.astype('float32')\n",
    "#print(dataset)\n",
    "X1 = dataset[:,1]#Y\n",
    "X1_scaler= (X1-numpy.min(X1))/(numpy.max(X1)-numpy.min(X1))\n",
    "X1_scaler = X1_scaler.reshape(-1)\n",
    "\n",
    "X2 = dataset[:,2]\n",
    "X2_scaler= (X2-numpy.min(X2))/(numpy.max(X2)-numpy.min(X2))\n",
    "X2_scaler = X2_scaler.reshape(-1)\n",
    "\n",
    "X3 = dataset[:,3]\n",
    "X3_scaler= (X3-numpy.min(X3))/(numpy.max(X3)-numpy.min(X3))\n",
    "X3_scaler = X3_scaler.reshape(-1)\n",
    "\n",
    "X4 = dataset[:,4]\n",
    "X4_scaler= (X4-numpy.min(X4))/(numpy.max(X4)-numpy.min(X4))\n",
    "X4_scaler = X4_scaler.reshape(-1)\n",
    "\n",
    "X5 = dataset[:,5]\n",
    "X5_scaler= (X5-numpy.min(X5))/(numpy.max(X5)-numpy.min(X5))\n",
    "X5_scaler = X5_scaler.reshape(-1)\n",
    "\n",
    "Y = dataset[:,0]\n",
    "Y_scaler= (Y-numpy.min(Y))/(numpy.max(Y)-numpy.min(Y))\n",
    "Y_scaler = Y_scaler.reshape(-1)\n",
    "\n",
    "\n",
    "\n",
    "XX = numpy.zeros([324,3,3])\n",
    "XX[:,0,0] = X1_scaler\n",
    "XX[:,0,1] = Y_scaler\n",
    "XX[:,0,2] = X2_scaler\n",
    "XX[:,1,0] = Y_scaler\n",
    "XX[:,1,1] = X3_scaler\n",
    "XX[:,1,2] = Y_scaler\n",
    "XX[:,2,0] = X4_scaler\n",
    "XX[:,2,1] = Y_scaler\n",
    "XX[:,2,2] = X5_scaler\n",
    "\n",
    "# 将数据拆分成训练和测试，7/9作为训练数据\n",
    "train_size = int(len(dataset) * 0.78)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = XX[0:train_size,:,:], XX[train_size:len(dataset),:,:]\n",
    "print(\"原始训练集的长度：\",train_size)\n",
    "print(\"原始测试集的长度：\",test_size)\n",
    "\n",
    "\n",
    "# 切片\n",
    "def create_train(seq, timestep):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[i:(i + timestep),:,:]\n",
    "        # X按照顺序取值\n",
    "        dataX.append(a)\n",
    "        # Y向后移动一位取值\n",
    "        dataY.append(seq[i + timestep,:,:])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "def create_testX(seq, timestep):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[i:(i + timestep),:,:]\n",
    "        # X按照顺序取值\n",
    "        dataX.append(a)\n",
    "    return numpy.array(dataX)\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "\n",
    "\n",
    "#----------forecasting-----------\n",
    "timestep = 6\n",
    "pres=[]\n",
    "trainX,trainY = create_train(train, timestep)\n",
    "testX = create_testX(test, timestep) \n",
    "testY = test[timestep:len(X11),0,1]\n",
    "print(\"转为监督学习，训练集数据长度：\", len(trainX))\n",
    "#print(trainX,trainY)\n",
    "print(\"转为监督学习，测试集数据长度：\",len(testX))\n",
    "#print(testX, testY )\n",
    "\n",
    "\n",
    "# 数据重构为5D [samples, timesteps, rows, columns, features]\n",
    "trainX_input5D = numpy.reshape(trainX, (trainX.shape[0], timestep,3,3, 1))\n",
    "testX_input5D = numpy.reshape(testX, (testX.shape[0],timestep, 3,3, 1))\n",
    "print('构造得到模型的输入数据(训练数据已有标签trainY): ',trainX_input5D.shape,testX_input5D.shape)\n",
    "\n",
    "\n",
    "# create and fit the convlstm network\n",
    "from tensorflow.python.keras.layers.convolutional import Conv3D ,Conv2D\n",
    "if __name__ == '__main__':\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=48, kernel_size=(2, 2), activation='relu', input_shape=(timestep, 3,3,1),\n",
    "                         padding='same',return_sequences=True))\n",
    "    model.add(ConvLSTM2D(filters=12, kernel_size=(1, 1),activation='relu',\n",
    "                         padding='same',return_sequences=False))\n",
    "    model.add(Conv2D(filters=1, kernel_size=(1, 1),\n",
    "               activation='sigmoid',\n",
    "               padding='same', data_format='channels_last'))\n",
    "#    model.add(Flatten())\n",
    "#    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.fit(trainX_input5D, trainY, epochs=1000)\n",
    "\n",
    "    # 打印模型\n",
    "    model.summary()\n",
    "\n",
    "    # 开始预测\n",
    "    trainPredict = model.predict(trainX_input5D)\n",
    "    testPredict = model.predict(testX_input5D)\n",
    "    \n",
    "    # 逆缩放预测值\n",
    "    testPredict1 = testPredict[:,1,0,:]\n",
    "    testPredict1 = testPredict1.reshape(-1)\n",
    "    dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_NFP.csv', engine='python')\n",
    "    dataset = dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "    Y = dataset[:,0]#Y\n",
    "    #trainPredict = trainPredict*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori = trainY*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori=trainY_re.reshape((246,1))\n",
    "    testPre = testPredict1*((np.max(Y)-np.min(Y)))+np.min(Y)\n",
    "    testPre = testPre.reshape(-1)\n",
    "    Y = dataset[:,0]\n",
    "    Y = Y[train_size+timestep:len(dataset)]\n",
    "    #testY_ori = testY*((np.max(dataset)-np.min(dataset)))+np.min(dataset)\n",
    "    #testY_ori = testY_ori.reshape((-1,1))\n",
    "    testY_ori = Y.reshape(-1)\n",
    "\n",
    "    \n",
    "\n",
    "    # 计算误差\n",
    "    #trainScore = math.sqrt(mean_squared_error(trainY_ori[:,0], trainPredict[:,0]))\n",
    "    #print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    #testScore = math.sqrt(mean_squared_error(testY_ori[:,0], testPredict[:,0]))\n",
    "    #print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "\n",
    "    error = []#Y-Y'\n",
    "    error1= []#abs((Y-Y')/Y)\n",
    "    error2= []#Y*Y'\n",
    "    squared1 =[]#Y*Y\n",
    "    squared2 =[]#Y'*Y'\n",
    "    for i in range(len(testY_ori)):\n",
    "        error.append(testY_ori[i] - testPre[i])\n",
    "        error1.append(abs((testY_ori[i] - testPre[i])/testY_ori[i]))\n",
    "        error2.append(testY_ori[i]*testPre[i])\n",
    "        squared1.append(testY_ori[i]*testY_ori[i])\n",
    "        squared2.append(testPre[i]*testPre[i])\n",
    "        \n",
    "        \n",
    "    squaredError = []#(Y-Y')^2\n",
    "    absError = []#abs(Y-Y')\n",
    "    for val in error:    \n",
    "        squaredError.append(val * val)#target-prediction之差平方     \n",
    "        absError.append(abs(val))#误差绝对值\n",
    "        \n",
    "    MSE=sum(squaredError) / len(squaredError)\n",
    "    meannn=np.mean(testY_ori)\n",
    "    NMSEerror = []\n",
    "    IAerror = []\n",
    "    for i in range(len(testY_ori)):\n",
    "        NMSEerror.append(squaredError[i]/error2[i])\n",
    "        IAerror.append((abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn))*(abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn)))\n",
    "    \n",
    "    U2error1 = []\n",
    "    U2error2 = []\n",
    "    for i in range(len(testY_ori)-1):\n",
    "        U2error1.append(((testY_ori[i+1] - testPre[i+1])/testY_ori[i])*((testY_ori[i+1] - testPre[i+1])/testY_ori[i]))\n",
    "        U2error2.append(((testY_ori[i+1] - testPre[i])/testY_ori[i])*((testY_ori[i+1] - testPre[i])/testY_ori[i]))\n",
    "    print('\\n==========================')\n",
    "    MAE=sum(absError)/len(absError)\n",
    "    print('MAE=',MAE)\n",
    "    from math import sqrt\n",
    "    RMSE=sqrt(MSE)\n",
    "    print(\"RMSE = \", RMSE)#均方根误差RMSE\n",
    "    NMSE=sum(NMSEerror)\n",
    "    print(\"NMSE = \", NMSE)#误差平方的归一化平均值NMSE\n",
    "    MAPE=sum(error1)/len(error1)\n",
    "    print('MAPE=',MAPE)\n",
    "    IA=1-sum(squaredError)/sum(IAerror)\n",
    "    print('IA=',IA)#一致性指数\n",
    "    U1index=RMSE/(sqrt(sum(squared1)/len(squared1))+sqrt(sum(squared2)/len(squared2)))\n",
    "    print('U1=',U1index)\n",
    "    U2index=(sqrt(sum(U2error1)/len(testY_ori)))/(sqrt(sum(U2error2)/len(testY_ori)))\n",
    "    print('U2=',U2index)\n",
    "\n",
    "    #print(testPredict)\n",
    "    testPre=testPre.reshape(-1)\n",
    "    testPre=pd.DataFrame(testPre)\n",
    "    testPre.to_csv('C:/Users/Administrator/Desktop/XI-2/data/pre_result/convlstmoutput.csv', header=False)\n",
    "    import csv\n",
    "    Evaluation_index = [MAE,RMSE,NMSE,MAPE,IA,U1index,U2index]\n",
    "    with open(\"C:/Users/Administrator/Desktop/XI-2/data/pre_result/convlstmEvaluation.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file ,delimiter=',')\n",
    "        writer.writerow(Evaluation_index)\n",
    "    \n",
    "    plt.figure(facecolor='white')\n",
    "    plt.plot(testPre,color='green', label='Predict')\n",
    "    plt.plot(testY_ori,color='pink', label='Original')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    pres.append(testPre)\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30f0bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN-SED\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.python.keras.layers.convolutional import Conv1D\n",
    "from tensorflow.python.keras.layers.convolutional import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#matplotlib inline\n",
    "import time\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_NFP.csv', engine='python')\n",
    "#print(dataframe)\n",
    "print(\"数据集的长度：\",len(dataframe))\n",
    "dataset = dataframe.values\n",
    "# 将整型变为float\n",
    "dataset = dataset.astype('float32')\n",
    "#print(dataset)\n",
    "\n",
    "timestep = 6\n",
    "dim = 6\n",
    "#数据缩放 拆分输入X（7维）&输出Y（1维）\n",
    "X = dataset[:,0:6]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "for i in range(dim):\n",
    "    Xdata = X[:,i]\n",
    "    Xdata = Xdata.reshape(-1,1)\n",
    "    Xdata = scaler.fit_transform(Xdata)\n",
    "    Xdata = Xdata.flatten()\n",
    "    X[:,i] = Xdata\n",
    "X_scaler = X.reshape(324,-1)\n",
    "\n",
    "Y = dataset[:,0]\n",
    "Y_scaler= (Y-np.min(Y))/(np.max(Y)-np.min(Y))\n",
    "Y_scaler = Y_scaler.reshape(-1)\n",
    "\n",
    "\n",
    "# 将数据拆分成训练和测试，7/9作为训练数据\n",
    "train_size = int(len(dataset) * 0.78)\n",
    "test_size = len(dataset) - train_size\n",
    "trainX, testX = X_scaler[0:train_size,:], X_scaler[train_size:len(dataset),:]\n",
    "trainY, testY = Y_scaler[0:train_size], Y_scaler[train_size:len(dataset)]\n",
    "print(\"原始训练集的长度：\",train_size)\n",
    "print(\"原始测试集的长度：\",test_size)\n",
    "#print(trainX)\n",
    "#print(trainY)\n",
    "#print(testX)\n",
    "#print(testY)\n",
    "\n",
    "\n",
    "\n",
    "#重构数据集\n",
    "##timestep为时间步长\n",
    "def create_trainX(seq, timestep):\n",
    "    dataX = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[i:(i+timestep)]\n",
    "        # X按照顺序取值 每次在后面增加一个数据\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "\n",
    "def create_trainY(seq, timestep):\n",
    "    dataY = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        # Y向后移动一位取值\n",
    "        dataY.append(seq[i + timestep])\n",
    "    return np.array(dataY)\n",
    "\n",
    "def create_testX(seq, timestep):\n",
    "    dataX = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[(i):(i+timestep)]\n",
    "        # X按照顺序取值 每次在后面增加一个数据\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "\n",
    "\n",
    "\n",
    "#----------forecasting-----------\n",
    "pres=[]\n",
    "trainX = create_trainX(trainX, timestep)\n",
    "trainY = create_trainY(trainY, timestep)\n",
    "testX = create_testX(testX, timestep) \n",
    "testY = testY[timestep:len(testY)]\n",
    "print(\"转为监督学习，训练集数据长度：\", len(trainX))\n",
    "#print(trainX,trainY)\n",
    "print(\"转为监督学习，测试集数据长度：\",len(testX))\n",
    "#print(testX, testY )\n",
    "\n",
    "\n",
    "# 数据重构为4D [samples, subsequences, timesteps, features]\n",
    "trainX_input4D = np.reshape(trainX, (trainX.shape[0],1,timestep,dim))\n",
    "testX_input4D = np.reshape(testX, (testX.shape[0],1,timestep,dim))\n",
    "print('构造得到模型的输入数据(训练数据已有标签trainY): ',trainX_input4D.shape,testX_input4D.shape)\n",
    "\n",
    "\n",
    "# create and fit the convlstm network\n",
    "if __name__ == '__main__':\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=48, kernel_size=1, activation='relu', input_shape=(None,1, testX.shape[1]))))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=1)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "    #model.add(LSTM(4,activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.fit(trainX_input4D, trainY, epochs=1000)\n",
    "    \n",
    "\n",
    "\n",
    "    # 打印模型\n",
    "    model.summary()\n",
    "\n",
    "    # 开始预测\n",
    "    trainPredict = model.predict(trainX_input4D)\n",
    "    testPredict = model.predict(testX_input4D)\n",
    "\n",
    "    # 逆缩放预测值\n",
    "    dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_NFP.csv', engine='python')\n",
    "    dataset = dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "    Y = dataset[:,0]\n",
    "    #trainPredict = trainPredict*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori = trainY*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori=trainY_re.reshape((246,1))\n",
    "    testPredict = testPredict*((np.max(Y)-np.min(Y)))+np.min(Y)\n",
    "    testPre = testPredict.reshape(-1)\n",
    "    Y = dataset[:,0]\n",
    "    Y = Y[train_size+timestep:len(dataset)]\n",
    "    #testY_ori = testY*((np.max(dataset)-np.min(dataset)))+np.min(dataset)\n",
    "    #testY_ori = testY_ori.reshape((-1,1))\n",
    "    testY_ori = Y.reshape(-1)\n",
    "\n",
    "    \n",
    "\n",
    "    # 计算误差\n",
    "    #trainScore = math.sqrt(mean_squared_error(trainY_ori[:,0], trainPredict[:,0]))\n",
    "    #print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    #testScore = math.sqrt(mean_squared_error(testY_ori[:,0], testPredict[:,0]))\n",
    "    #print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "\n",
    "    error = []#Y-Y'\n",
    "    error1= []#abs((Y-Y')/Y)\n",
    "    error2= []#Y*Y'\n",
    "    squared1 =[]#Y*Y\n",
    "    squared2 =[]#Y'*Y'\n",
    "    for i in range(len(testY_ori)):\n",
    "        error.append(testY_ori[i] - testPre[i])\n",
    "        error1.append(abs((testY_ori[i] - testPre[i])/testY_ori[i]))\n",
    "        error2.append(testY_ori[i]*testPre[i])\n",
    "        squared1.append(testY_ori[i]*testY_ori[i])\n",
    "        squared2.append(testPre[i]*testPre[i])\n",
    "        \n",
    "        \n",
    "    squaredError = []#(Y-Y')^2\n",
    "    absError = []#abs(Y-Y')\n",
    "    for val in error:    \n",
    "        squaredError.append(val * val)#target-prediction之差平方     \n",
    "        absError.append(abs(val))#误差绝对值\n",
    "        \n",
    "    MSE=sum(squaredError) / len(squaredError)\n",
    "    meannn=np.mean(testY_ori)\n",
    "    NMSEerror = []\n",
    "    IAerror = []\n",
    "    for i in range(len(testY_ori)):\n",
    "        NMSEerror.append(squaredError[i]/error2[i])\n",
    "        IAerror.append((abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn))*(abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn)))\n",
    "    \n",
    "    U2error1 = []\n",
    "    U2error2 = []\n",
    "    for i in range(len(testY_ori)-1):\n",
    "        U2error1.append(((testY_ori[i+1] - testPre[i+1])/testY_ori[i])*((testY_ori[i+1] - testPre[i+1])/testY_ori[i]))\n",
    "        U2error2.append(((testY_ori[i+1] - testPre[i])/testY_ori[i])*((testY_ori[i+1] - testPre[i])/testY_ori[i]))\n",
    "    print('\\n==========================')\n",
    "    MAE=sum(absError)/len(absError)\n",
    "    print('MAE=',MAE)\n",
    "    from math import sqrt\n",
    "    RMSE=sqrt(MSE)\n",
    "    print(\"RMSE = \", RMSE)#均方根误差RMSE\n",
    "    NMSE=sum(NMSEerror)\n",
    "    print(\"NMSE = \", NMSE)#误差平方的归一化平均值NMSE\n",
    "    MAPE=sum(error1)/len(error1)\n",
    "    print('MAPE=',MAPE)\n",
    "    IA=1-sum(squaredError)/sum(IAerror)\n",
    "    print('IA=',IA)#一致性指数\n",
    "    U1index=RMSE/(sqrt(sum(squared1)/len(squared1))+sqrt(sum(squared2)/len(squared2)))\n",
    "    print('U1=',U1index)\n",
    "    U2index=(sqrt(sum(U2error1)/len(testY_ori)))/(sqrt(sum(U2error2)/len(testY_ori)))\n",
    "    print('U2=',U2index)\n",
    "\n",
    "    #print(testPredict)\n",
    "    testPre=testPre.reshape(-1)\n",
    "    testPre=pd.DataFrame(testPre)\n",
    "    testPre.to_csv('C:/Users/Administrator/Desktop/XI-2/data/pre_result/cnnoutput.csv', header=False)\n",
    "    import csv\n",
    "    Evaluation_index = [MAE,RMSE,NMSE,MAPE,IA,U1index,U2index]\n",
    "    with open(\"C:/Users/Administrator/Desktop/XI-2/data/pre_result/cnnEvaluation.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file ,delimiter=',')\n",
    "        writer.writerow(Evaluation_index)\n",
    "    \n",
    "    plt.figure(facecolor='white')\n",
    "    plt.plot(testPre,color='green', label='Predict')\n",
    "    plt.plot(testY_ori,color='pink', label='Original')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    pres.append(testPre)\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251e2b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN-LSTM-SED\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.python.keras.layers.convolutional import Conv1D\n",
    "from tensorflow.python.keras.layers.convolutional import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#matplotlib inline\n",
    "import time\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_NFP.csv', engine='python')\n",
    "#print(dataframe)\n",
    "print(\"数据集的长度：\",len(dataframe))\n",
    "dataset = dataframe.values\n",
    "# 将整型变为float\n",
    "dataset = dataset.astype('float32')\n",
    "#print(dataset)\n",
    "\n",
    "timestep = 6\n",
    "dim = 6\n",
    "#数据缩放 拆分输入X（7维）&输出Y（1维）\n",
    "X = dataset[:,0:6]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "for i in range(dim):\n",
    "    Xdata = X[:,i]\n",
    "    Xdata = Xdata.reshape(-1,1)\n",
    "    Xdata = scaler.fit_transform(Xdata)\n",
    "    Xdata = Xdata.flatten()\n",
    "    X[:,i] = Xdata\n",
    "X_scaler = X.reshape(324,-1)\n",
    "\n",
    "Y = dataset[:,0]\n",
    "Y_scaler= (Y-np.min(Y))/(np.max(Y)-np.min(Y))\n",
    "Y_scaler = Y_scaler.reshape(-1)\n",
    "\n",
    "\n",
    "# 将数据拆分成训练和测试，7/9作为训练数据\n",
    "train_size = int(len(dataset) * 0.78)\n",
    "test_size = len(dataset) - train_size\n",
    "trainX, testX = X_scaler[0:train_size,:], X_scaler[train_size:len(dataset),:]\n",
    "trainY, testY = Y_scaler[0:train_size], Y_scaler[train_size:len(dataset)]\n",
    "print(\"原始训练集的长度：\",train_size)\n",
    "print(\"原始测试集的长度：\",test_size)\n",
    "#print(trainX)\n",
    "#print(trainY)\n",
    "#print(testX)\n",
    "#print(testY)\n",
    "\n",
    "\n",
    "\n",
    "#重构数据集\n",
    "##timestep为时间步长\n",
    "def create_trainX(seq, timestep):\n",
    "    dataX = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[i:(i+timestep)]\n",
    "        # X按照顺序取值 每次在后面增加一个数据\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "\n",
    "def create_trainY(seq, timestep):\n",
    "    dataY = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        # Y向后移动一位取值\n",
    "        dataY.append(seq[i + timestep])\n",
    "    return np.array(dataY)\n",
    "\n",
    "def create_testX(seq, timestep):\n",
    "    dataX = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[(i):(i+timestep)]\n",
    "        # X按照顺序取值 每次在后面增加一个数据\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "\n",
    "\n",
    "\n",
    "#----------forecasting-----------\n",
    "pres=[]\n",
    "trainX = create_trainX(trainX, timestep)\n",
    "trainY = create_trainY(trainY, timestep)\n",
    "testX = create_testX(testX, timestep) \n",
    "testY = testY[timestep:len(testY)]\n",
    "print(\"转为监督学习，训练集数据长度：\", len(trainX))\n",
    "#print(trainX,trainY)\n",
    "print(\"转为监督学习，测试集数据长度：\",len(testX))\n",
    "#print(testX, testY )\n",
    "\n",
    "\n",
    "# 数据重构为4D [samples, subsequences, timesteps, features]\n",
    "trainX_input4D = np.reshape(trainX, (trainX.shape[0],1,timestep,dim))\n",
    "testX_input4D = np.reshape(testX, (testX.shape[0],1,timestep,dim))\n",
    "print('构造得到模型的输入数据(训练数据已有标签trainY): ',trainX_input4D.shape,testX_input4D.shape)\n",
    "\n",
    "\n",
    "# create and fit the convlstm network\n",
    "if __name__ == '__main__':\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=48, kernel_size=1, activation='relu', input_shape=(None,1, testX.shape[1]))))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=1)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "    model.add(LSTM(60,activation='relu',return_sequences=True))\n",
    "    model.add(LSTM(12,activation='relu',return_sequences=True))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.fit(trainX_input4D, trainY, epochs=1000)\n",
    "    \n",
    "\n",
    "\n",
    "    # 打印模型\n",
    "    model.summary()\n",
    "\n",
    "    # 开始预测\n",
    "    trainPredict = model.predict(trainX_input4D)\n",
    "    testPredict = model.predict(testX_input4D)\n",
    "\n",
    "\n",
    "    # 逆缩放预测值\n",
    "    dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_NFP.csv', engine='python')\n",
    "    dataset = dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "    Y = dataset[:,0]\n",
    "    #trainPredict = trainPredict*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori = trainY*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori=trainY_re.reshape((246,1))\n",
    "    testPredict = testPredict*((np.max(Y)-np.min(Y)))+np.min(Y)\n",
    "    testPre = testPredict.reshape(-1)\n",
    "    Y = dataset[:,0]\n",
    "    Y = Y[train_size+timestep:len(dataset)]\n",
    "    #testY_ori = testY*((np.max(dataset)-np.min(dataset)))+np.min(dataset)\n",
    "    #testY_ori = testY_ori.reshape((-1,1))\n",
    "    testY_ori = Y.reshape(-1)\n",
    "\n",
    "    \n",
    "\n",
    "    # 计算误差\n",
    "    #trainScore = math.sqrt(mean_squared_error(trainY_ori[:,0], trainPredict[:,0]))\n",
    "    #print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    #testScore = math.sqrt(mean_squared_error(testY_ori[:,0], testPredict[:,0]))\n",
    "    #print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "\n",
    "    error = []#Y-Y'\n",
    "    error1= []#abs((Y-Y')/Y)\n",
    "    error2= []#Y*Y'\n",
    "    squared1 =[]#Y*Y\n",
    "    squared2 =[]#Y'*Y'\n",
    "    for i in range(len(testY_ori)):\n",
    "        error.append(testY_ori[i] - testPre[i])\n",
    "        error1.append(abs((testY_ori[i] - testPre[i])/testY_ori[i]))\n",
    "        error2.append(testY_ori[i]*testPre[i])\n",
    "        squared1.append(testY_ori[i]*testY_ori[i])\n",
    "        squared2.append(testPre[i]*testPre[i])\n",
    "        \n",
    "        \n",
    "    squaredError = []#(Y-Y')^2\n",
    "    absError = []#abs(Y-Y')\n",
    "    for val in error:    \n",
    "        squaredError.append(val * val)#target-prediction之差平方     \n",
    "        absError.append(abs(val))#误差绝对值\n",
    "        \n",
    "    MSE=sum(squaredError) / len(squaredError)\n",
    "    meannn=np.mean(testY_ori)\n",
    "    NMSEerror = []\n",
    "    IAerror = []\n",
    "    for i in range(len(testY_ori)):\n",
    "        NMSEerror.append(squaredError[i]/error2[i])\n",
    "        IAerror.append((abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn))*(abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn)))\n",
    "    \n",
    "    U2error1 = []\n",
    "    U2error2 = []\n",
    "    for i in range(len(testY_ori)-1):\n",
    "        U2error1.append(((testY_ori[i+1] - testPre[i+1])/testY_ori[i])*((testY_ori[i+1] - testPre[i+1])/testY_ori[i]))\n",
    "        U2error2.append(((testY_ori[i+1] - testPre[i])/testY_ori[i])*((testY_ori[i+1] - testPre[i])/testY_ori[i]))\n",
    "    print('\\n==========================')\n",
    "    MAE=sum(absError)/len(absError)\n",
    "    print('MAE=',MAE)\n",
    "    from math import sqrt\n",
    "    RMSE=sqrt(MSE)\n",
    "    print(\"RMSE = \", RMSE)#均方根误差RMSE\n",
    "    NMSE=sum(NMSEerror)\n",
    "    print(\"NMSE = \", NMSE)#误差平方的归一化平均值NMSE\n",
    "    MAPE=sum(error1)/len(error1)\n",
    "    print('MAPE=',MAPE)\n",
    "    IA=1-sum(squaredError)/sum(IAerror)\n",
    "    print('IA=',IA)#一致性指数\n",
    "    U1index=RMSE/(sqrt(sum(squared1)/len(squared1))+sqrt(sum(squared2)/len(squared2)))\n",
    "    print('U1=',U1index)\n",
    "    U2index=(sqrt(sum(U2error1)/len(testY_ori)))/(sqrt(sum(U2error2)/len(testY_ori)))\n",
    "    print('U2=',U2index)\n",
    "\n",
    "    #print(testPredict)\n",
    "    testPre=testPre.reshape(-1)\n",
    "    testPre=pd.DataFrame(testPre)\n",
    "    testPre.to_csv('C:/Users/Administrator/Desktop/XI-2/data/pre_result/cnnoutput.csv', header=False)\n",
    "    import csv\n",
    "    Evaluation_index = [MAE,RMSE,NMSE,MAPE,IA,U1index,U2index]\n",
    "    with open(\"C:/Users/Administrator/Desktop/XI-2/data/pre_result/cnnEvaluation.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file ,delimiter=',')\n",
    "        writer.writerow(Evaluation_index)\n",
    "    \n",
    "    plt.figure(facecolor='white')\n",
    "    plt.plot(testPre,color='green', label='Predict')\n",
    "    plt.plot(testY_ori,color='pink', label='Original')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    pres.append(testPre)\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb8fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GRU_SED\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import merge\n",
    "from tensorflow.python.keras.layers.merge import Multiply\n",
    "from tensorflow.python.keras.layers.core import *\n",
    "from tensorflow.python.keras.models import *\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python.keras.layers import Input,Dense,Reshape,Dropout, Embedding, LSTM, Bidirectional,Permute\n",
    "from tensorflow.python.keras.layers import RepeatVector, TimeDistributed\n",
    "from tensorflow.python.keras.layers.recurrent import GRU\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import concat, read_csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sklearn.metrics as skm\n",
    "import math\n",
    "from math import sqrt\n",
    "import time\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "#1. load dataset\n",
    "dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_NFP.csv', engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "data = dataset.reshape(-1,6)\n",
    "\n",
    "\n",
    "timestep = 6\n",
    "dim = 6\n",
    "\n",
    "#数据缩放 拆分输入X（7维）&输出Y（1维）\n",
    "X = dataset[:,0:6]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "for i in range(dim):\n",
    "    Xdata = X[:,i]\n",
    "    Xdata = Xdata.reshape(-1,1)\n",
    "    Xdata = scaler.fit_transform(Xdata)\n",
    "    Xdata = Xdata.flatten()\n",
    "    X[:,i] = Xdata\n",
    "X_scaler = X.reshape(324,-1)\n",
    "print(X_scaler.shape)\n",
    "\n",
    "Y = dataset[:,0]\n",
    "Y_scaler= (Y-np.min(Y))/(np.max(Y)-np.min(Y))\n",
    "Y_scaler = Y_scaler.reshape(-1)\n",
    "print(Y_scaler.shape)\n",
    "\n",
    "\n",
    "#重构数据集\n",
    "##timestep为时间步长\n",
    "def create_X(seq, timestep):\n",
    "    dataX = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[i:(i+timestep)]\n",
    "        # X按照顺序取值 每次在后面增加一个数据\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "\n",
    "def create_Y(seq, timestep):\n",
    "    dataY = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        # Y向后移动一位取值\n",
    "        dataY.append(seq[i+timestep])\n",
    "    return np.array(dataY)\n",
    "\n",
    "#-------------------------------------------#\n",
    "#  建立注意力模型\n",
    "#-------------------------------------------#\n",
    "def get_activations(model, inputs, print_shape_only=False, layer_name=None):\n",
    "    # Documentation is available online on Github at the address below.\n",
    "    # From: https://github.com/philipperemy/keras-visualize-activations\n",
    "#    print('----- activations -----')\n",
    "    activations = []\n",
    "    inp = model.input\n",
    "    if layer_name is None:\n",
    "        outputs = [layer.output for layer in model.layers]\n",
    "    else:\n",
    "        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs\n",
    "    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "    layer_outputs = [func([inputs, 1])[0] for func in funcs]\n",
    "    for layer_activations in layer_outputs:\n",
    "        activations.append(layer_activations)\n",
    "#        if print_shape_only:\n",
    "#            print(layer_activations.shape)\n",
    "#        else:\n",
    "#            print(layer_activations)\n",
    "    return activations\n",
    "\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)#(batch_size, time_steps, input_dim)\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])#(batch_size, time_steps, input_dim)\n",
    "    return output_attention_mul\n",
    "\n",
    "\n",
    "def get_gru_model():\n",
    "    K.clear_session() #清除之前的模型，省得压满内存\n",
    "    inputs = Input(shape=(timestep, dim,))\n",
    "    gru_units1 = 60\n",
    "    gru_units2 = 12\n",
    "    # (batch_size, time_steps, INPUT_DIM) -> (batch_size, input_dim, lstm_units)\n",
    "    gur_out1 = GRU(gru_units1,return_sequences=True)(inputs)\n",
    "    gur_out2 = GRU(gru_units2,return_sequences=True)(gur_out1)\n",
    "    # (batch_size, input_dim, lstm_units) -> (batch_size, input_dim*lstm_units)\n",
    "    gur_out = Flatten()(gur_out2)\n",
    "    output = Dense(1, activation='sigmoid')(gur_out)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#预测\n",
    "pres=[]\n",
    "# 将数据拆分成训练和测试，7/9作为训练数据\n",
    "train_size = int(len(dataset) * 0.78)\n",
    "test_size = len(dataset) - train_size\n",
    "trainX, testX = X_scaler[0:train_size,:], X_scaler[train_size:len(dataset),:]\n",
    "trainY, testY = Y_scaler[0:train_size], Y_scaler[train_size:len(dataset)]\n",
    "print(\"原始训练集的长度：\",train_size)\n",
    "print(\"原始测试集的长度：\",test_size)\n",
    "#print(trainX,trainX.shape)\n",
    "#print(trainY,trainY.shape)\n",
    "#print(testX,testX.shape)\n",
    "#print(testY,testY.shape)\n",
    "\n",
    "train_X=create_X(trainX,timestep)#(246,6,6)\n",
    "#print(train_X,train_X.shape)\n",
    "train_Y=create_Y(trainY,timestep)#(246,)\n",
    "#print(train_Y,train_Y.shape)\n",
    "test_X=create_X(testX,timestep)#(66,6,6)\n",
    "#print(test_X,test_X.shape)\n",
    "test_Y=create_Y(testY,timestep)#(66,)\n",
    "#print(test_Y,test_Y.shape)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        \n",
    "    model = get_gru_model()\n",
    "    optimizer = Adam(0.01)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(train_X, train_Y, epochs=1000, batch_size=64)\n",
    "\n",
    "    \n",
    "    # 开始预测\n",
    "    trainPredict = model.predict(train_X)\n",
    "    testPredict = model.predict(test_X)\n",
    "\n",
    "\n",
    "    # 逆缩放预测值\n",
    "    dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_NFP.csv', engine='python')\n",
    "    dataset = dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "    Y = dataset[:,0]\n",
    "    #trainPredict = trainPredict*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori = trainY*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori=trainY_re.reshape((246,1))\n",
    "    testPre = testPredict*((np.max(Y)-np.min(Y)))+np.min(Y)\n",
    "    testPre = testPre.reshape(-1)\n",
    "    Y = dataset[:,0]\n",
    "    Y = Y[train_size+timestep:len(dataset)]\n",
    "    #testY_ori = testY*((np.max(dataset)-np.min(dataset)))+np.min(dataset)\n",
    "    #testY_ori = testY_ori.reshape((-1,1))\n",
    "    testY_ori = Y.reshape(-1)\n",
    "\n",
    "    \n",
    "\n",
    "    # 计算误差\n",
    "    #trainScore = math.sqrt(mean_squared_error(trainY_ori[:,0], trainPredict[:,0]))\n",
    "    #print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    #testScore = math.sqrt(mean_squared_error(testY_ori[:,0], testPredict[:,0]))\n",
    "    #print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "\n",
    "    error = []#Y-Y'\n",
    "    error1= []#abs((Y-Y')/Y)\n",
    "    error2= []#Y*Y'\n",
    "    squared1 =[]#Y*Y\n",
    "    squared2 =[]#Y'*Y'\n",
    "    for i in range(len(testY_ori)):\n",
    "        error.append(testY_ori[i] - testPre[i])\n",
    "        error1.append(abs((testY_ori[i] - testPre[i])/testY_ori[i]))\n",
    "        error2.append(testY_ori[i]*testPre[i])\n",
    "        squared1.append(testY_ori[i]*testY_ori[i])\n",
    "        squared2.append(testPre[i]*testPre[i])\n",
    "        \n",
    "        \n",
    "    squaredError = []#(Y-Y')^2\n",
    "    absError = []#abs(Y-Y')\n",
    "    for val in error:    \n",
    "        squaredError.append(val * val)#target-prediction之差平方     \n",
    "        absError.append(abs(val))#误差绝对值\n",
    "        \n",
    "    MSE=sum(squaredError) / len(squaredError)\n",
    "    meannn=np.mean(testY_ori)\n",
    "    NMSEerror = []\n",
    "    IAerror = []\n",
    "    for i in range(len(testY_ori)):\n",
    "        NMSEerror.append(squaredError[i]/error2[i])\n",
    "        IAerror.append((abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn))*(abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn)))\n",
    "    \n",
    "    U2error1 = []\n",
    "    U2error2 = []\n",
    "    for i in range(len(testY_ori)-1):\n",
    "        U2error1.append(((testY_ori[i+1] - testPre[i+1])/testY_ori[i])*((testY_ori[i+1] - testPre[i+1])/testY_ori[i]))\n",
    "        U2error2.append(((testY_ori[i+1] - testPre[i])/testY_ori[i])*((testY_ori[i+1] - testPre[i])/testY_ori[i]))\n",
    "    print('\\n==========================')\n",
    "    MAE=sum(absError)/len(absError)\n",
    "    print('MAE=',MAE)\n",
    "    from math import sqrt\n",
    "    RMSE=sqrt(MSE)\n",
    "    print(\"RMSE = \", RMSE)#均方根误差RMSE\n",
    "    NMSE=sum(NMSEerror)\n",
    "    print(\"NMSE = \", NMSE)#误差平方的归一化平均值NMSE\n",
    "    MAPE=sum(error1)/len(error1)\n",
    "    print('MAPE=',MAPE)\n",
    "    IA=1-sum(squaredError)/sum(IAerror)\n",
    "    print('IA=',IA)#一致性指数\n",
    "    U1index=RMSE/(sqrt(sum(squared1)/len(squared1))+sqrt(sum(squared2)/len(squared2)))\n",
    "    print('U1=',U1index)\n",
    "    U2index=(sqrt(sum(U2error1)/len(testY_ori)))/(sqrt(sum(U2error2)/len(testY_ori)))\n",
    "    print('U2=',U2index)\n",
    "\n",
    "    #print(testPredict)\n",
    "    testPre=testPre.reshape(-1)\n",
    "    testPre=pd.DataFrame(testPre)\n",
    "    testPre.to_csv('C:/Users/Administrator/Desktop/XI-2/data/pre_result/gruoutput.csv', header=False)\n",
    "    import csv\n",
    "    Evaluation_index = [MAE,RMSE,NMSE,MAPE,IA,U1index,U2index]\n",
    "    with open(\"C:/Users/Administrator/Desktop/XI-2/data/pre_result/gruEvaluation.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file ,delimiter=',')\n",
    "        writer.writerow(Evaluation_index)\n",
    "    \n",
    "    plt.figure(facecolor='white')\n",
    "    plt.plot(testPre,color='green', label='Predict')\n",
    "    plt.plot(testY_ori,color='pink', label='Original')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    pres.append(testPre)\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbf953b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(324, 6)\n",
      "(324,)\n",
      "原始训练集的长度： 252\n",
      "原始测试集的长度： 72\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 6, 6)]            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 6, 60)             16080     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 6, 12)             3504      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 73        \n",
      "=================================================================\n",
      "Total params: 19,657\n",
      "Trainable params: 19,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 246 samples\n",
      "Epoch 1/1000\n",
      "246/246 [==============================] - 4s 17ms/sample - loss: 0.0934\n",
      "Epoch 2/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0519\n",
      "Epoch 3/1000\n",
      "246/246 [==============================] - 0s 146us/sample - loss: 0.0300\n",
      "Epoch 4/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0245\n",
      "Epoch 5/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0156\n",
      "Epoch 6/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0146\n",
      "Epoch 7/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 0.0084\n",
      "Epoch 8/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 0.0100\n",
      "Epoch 9/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 0.0091\n",
      "Epoch 10/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0073\n",
      "Epoch 11/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0075\n",
      "Epoch 12/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 0.0069\n",
      "Epoch 13/1000\n",
      "246/246 [==============================] - 0s 146us/sample - loss: 0.0064\n",
      "Epoch 14/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 0.0061\n",
      "Epoch 15/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0061\n",
      "Epoch 16/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 0.0059\n",
      "Epoch 17/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 0.0057\n",
      "Epoch 18/1000\n",
      "246/246 [==============================] - 0s 142us/sample - loss: 0.0054\n",
      "Epoch 19/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0052\n",
      "Epoch 20/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 0.0052\n",
      "Epoch 21/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 0.0050\n",
      "Epoch 22/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0049\n",
      "Epoch 23/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 0.0047\n",
      "Epoch 24/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 0.0046\n",
      "Epoch 25/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 0.0045\n",
      "Epoch 26/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0045\n",
      "Epoch 27/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 0.0044\n",
      "Epoch 28/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 0.0043\n",
      "Epoch 29/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 0.0044\n",
      "Epoch 30/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 0.0042\n",
      "Epoch 31/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 0.0041\n",
      "Epoch 32/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 0.0041\n",
      "Epoch 33/1000\n",
      "246/246 [==============================] - 0s 150us/sample - loss: 0.0039\n",
      "Epoch 34/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 0.0038\n",
      "Epoch 35/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 0.0038\n",
      "Epoch 36/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 0.0038\n",
      "Epoch 37/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 0.0037\n",
      "Epoch 38/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 0.0041\n",
      "Epoch 39/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0039\n",
      "Epoch 40/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 0.0038\n",
      "Epoch 41/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0035\n",
      "Epoch 42/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0036\n",
      "Epoch 43/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 0.0037\n",
      "Epoch 44/1000\n",
      "246/246 [==============================] - 0s 179us/sample - loss: 0.0036\n",
      "Epoch 45/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0035\n",
      "Epoch 46/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 0.0035\n",
      "Epoch 47/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 0.0031\n",
      "Epoch 48/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 0.0033\n",
      "Epoch 49/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 0.0031\n",
      "Epoch 50/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 0.0031\n",
      "Epoch 51/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0029\n",
      "Epoch 52/1000\n",
      "246/246 [==============================] - ETA: 0s - loss: 0.003 - 0s 706us/sample - loss: 0.0030\n",
      "Epoch 53/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0030\n",
      "Epoch 54/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 0.0031\n",
      "Epoch 55/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0026\n",
      "Epoch 56/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 0.0031\n",
      "Epoch 57/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 0.0028\n",
      "Epoch 58/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0031\n",
      "Epoch 59/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 0.0031\n",
      "Epoch 60/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0026\n",
      "Epoch 61/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 0.0024\n",
      "Epoch 62/1000\n",
      "246/246 [==============================] - 0s 187us/sample - loss: 0.0023\n",
      "Epoch 63/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 0.0032\n",
      "Epoch 64/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 0.0031\n",
      "Epoch 65/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0026\n",
      "Epoch 66/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0022\n",
      "Epoch 67/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0020\n",
      "Epoch 68/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 0.0021\n",
      "Epoch 69/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 0.0020\n",
      "Epoch 70/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0020\n",
      "Epoch 71/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0024\n",
      "Epoch 72/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 0.0017\n",
      "Epoch 73/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0018\n",
      "Epoch 74/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 0.0018\n",
      "Epoch 75/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 0.0017\n",
      "Epoch 76/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 0.0018\n",
      "Epoch 77/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 0.0022\n",
      "Epoch 78/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 0.0025\n",
      "Epoch 79/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 0.0021\n",
      "Epoch 80/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 0.0019\n",
      "Epoch 81/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 178us/sample - loss: 0.0017\n",
      "Epoch 82/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 0.0017\n",
      "Epoch 83/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 0.0016\n",
      "Epoch 84/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0014\n",
      "Epoch 85/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 0.0013\n",
      "Epoch 86/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 0.0012\n",
      "Epoch 87/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 0.0012\n",
      "Epoch 88/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 0.0012\n",
      "Epoch 89/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 0.0012\n",
      "Epoch 90/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 0.0012\n",
      "Epoch 91/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 0.0011\n",
      "Epoch 92/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 0.0011\n",
      "Epoch 93/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 0.0013\n",
      "Epoch 94/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 0.0013\n",
      "Epoch 95/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 0.0013\n",
      "Epoch 96/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 0.0011\n",
      "Epoch 97/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 0.0011\n",
      "Epoch 98/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 0.0011\n",
      "Epoch 99/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 0.0011\n",
      "Epoch 100/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 0.0011\n",
      "Epoch 101/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 9.1976e-04\n",
      "Epoch 102/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0010\n",
      "Epoch 103/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0010\n",
      "Epoch 104/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 0.0012\n",
      "Epoch 105/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 0.0011\n",
      "Epoch 106/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 9.8994e-04\n",
      "Epoch 107/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 0.0010\n",
      "Epoch 108/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 0.0012\n",
      "Epoch 109/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 9.2492e-04\n",
      "Epoch 110/1000\n",
      "246/246 [==============================] - 0s 187us/sample - loss: 9.2232e-04\n",
      "Epoch 111/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 8.7439e-04\n",
      "Epoch 112/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 8.0477e-04\n",
      "Epoch 113/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 8.3522e-04\n",
      "Epoch 114/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 9.2969e-04\n",
      "Epoch 115/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 8.5746e-04\n",
      "Epoch 116/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 7.7346e-04\n",
      "Epoch 117/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 8.2455e-04\n",
      "Epoch 118/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 7.6042e-04\n",
      "Epoch 119/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 7.5028e-04\n",
      "Epoch 120/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 9.8314e-04\n",
      "Epoch 121/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 9.2842e-04\n",
      "Epoch 122/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 8.4698e-04\n",
      "Epoch 123/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 8.0773e-04\n",
      "Epoch 124/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 7.5948e-04\n",
      "Epoch 125/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 9.1262e-04\n",
      "Epoch 126/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 9.9700e-04\n",
      "Epoch 127/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 0.0011\n",
      "Epoch 128/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 9.8567e-04\n",
      "Epoch 129/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 0.0010\n",
      "Epoch 130/1000\n",
      "246/246 [==============================] - 0s 320us/sample - loss: 7.9437e-04\n",
      "Epoch 131/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 9.8524e-04\n",
      "Epoch 132/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 9.1257e-04\n",
      "Epoch 133/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 7.1285e-04\n",
      "Epoch 134/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 6.8051e-04\n",
      "Epoch 135/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 7.3620e-04\n",
      "Epoch 136/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 7.6938e-04\n",
      "Epoch 137/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 8.5016e-04\n",
      "Epoch 138/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 0.0012\n",
      "Epoch 139/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 0.0010\n",
      "Epoch 140/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 8.8459e-04\n",
      "Epoch 141/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 8.0576e-04\n",
      "Epoch 142/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 7.0244e-04\n",
      "Epoch 143/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 6.9333e-04\n",
      "Epoch 144/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 6.6682e-04\n",
      "Epoch 145/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 7.6234e-04\n",
      "Epoch 146/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 6.7581e-04\n",
      "Epoch 147/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 5.9058e-04\n",
      "Epoch 148/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 6.6579e-04\n",
      "Epoch 149/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 6.7253e-04\n",
      "Epoch 150/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 7.0814e-04\n",
      "Epoch 151/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 7.8066e-04\n",
      "Epoch 152/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 9.8242e-04\n",
      "Epoch 153/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 8.1538e-04\n",
      "Epoch 154/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 9.2752e-04\n",
      "Epoch 155/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 7.2944e-04\n",
      "Epoch 156/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 7.8464e-04\n",
      "Epoch 157/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 6.8112e-04\n",
      "Epoch 158/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 6.4383e-04\n",
      "Epoch 159/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 6.0919e-04\n",
      "Epoch 160/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 5.6539e-04\n",
      "Epoch 161/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 5.9649e-04\n",
      "Epoch 162/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 6.2320e-04\n",
      "Epoch 163/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 5.5487e-04\n",
      "Epoch 164/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 5.4018e-04\n",
      "Epoch 165/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 5.2216e-04\n",
      "Epoch 166/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 5.9554e-04\n",
      "Epoch 167/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 8.0566e-04\n",
      "Epoch 168/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 6.7286e-04\n",
      "Epoch 169/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 5.7346e-04\n",
      "Epoch 170/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 182us/sample - loss: 6.4422e-04\n",
      "Epoch 171/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 7.9625e-04\n",
      "Epoch 172/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 7.6266e-04\n",
      "Epoch 173/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 7.5619e-04\n",
      "Epoch 174/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 6.5974e-04\n",
      "Epoch 175/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 6.6785e-04\n",
      "Epoch 176/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 5.9211e-04\n",
      "Epoch 177/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 5.6116e-04\n",
      "Epoch 178/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 5.1289e-04\n",
      "Epoch 179/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 5.9880e-04\n",
      "Epoch 180/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 6.0551e-04\n",
      "Epoch 181/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 5.6239e-04\n",
      "Epoch 182/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 6.0355e-04\n",
      "Epoch 183/1000\n",
      "246/246 [==============================] - 0s 150us/sample - loss: 6.2587e-04\n",
      "Epoch 184/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 6.1063e-04\n",
      "Epoch 185/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 5.2580e-04\n",
      "Epoch 186/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 5.1177e-04\n",
      "Epoch 187/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 5.5145e-04\n",
      "Epoch 188/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 6.0380e-04\n",
      "Epoch 189/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 5.2854e-04\n",
      "Epoch 190/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 5.1935e-04\n",
      "Epoch 191/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 5.8090e-04\n",
      "Epoch 192/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 5.1833e-04\n",
      "Epoch 193/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 5.2543e-04\n",
      "Epoch 194/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 4.8399e-04\n",
      "Epoch 195/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 4.5833e-04\n",
      "Epoch 196/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 4.5699e-04\n",
      "Epoch 197/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 4.4378e-04\n",
      "Epoch 198/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 4.8106e-04\n",
      "Epoch 199/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 4.7345e-04\n",
      "Epoch 200/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 5.0816e-04\n",
      "Epoch 201/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 5.4094e-04\n",
      "Epoch 202/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 5.2861e-04\n",
      "Epoch 203/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 4.5664e-04\n",
      "Epoch 204/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 5.1437e-04\n",
      "Epoch 205/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 5.0491e-04\n",
      "Epoch 206/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 5.3424e-04\n",
      "Epoch 207/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 4.4763e-04\n",
      "Epoch 208/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 4.4545e-04\n",
      "Epoch 209/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 5.7706e-04\n",
      "Epoch 210/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 5.7412e-04\n",
      "Epoch 211/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 4.8374e-04\n",
      "Epoch 212/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 5.1928e-04\n",
      "Epoch 213/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 4.2878e-04\n",
      "Epoch 214/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 5.1351e-04\n",
      "Epoch 215/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 4.6980e-04\n",
      "Epoch 216/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 4.7079e-04\n",
      "Epoch 217/1000\n",
      "246/246 [==============================] - 0s 312us/sample - loss: 5.3716e-04\n",
      "Epoch 218/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 4.5539e-04\n",
      "Epoch 219/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 4.6428e-04\n",
      "Epoch 220/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 4.8664e-04\n",
      "Epoch 221/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 4.5666e-04\n",
      "Epoch 222/1000\n",
      "246/246 [==============================] - 0s 190us/sample - loss: 4.7196e-04\n",
      "Epoch 223/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 4.1980e-04\n",
      "Epoch 224/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 4.4418e-04\n",
      "Epoch 225/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 4.4209e-04\n",
      "Epoch 226/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 4.1500e-04\n",
      "Epoch 227/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 4.0945e-04\n",
      "Epoch 228/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 4.3950e-04\n",
      "Epoch 229/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 4.8802e-04\n",
      "Epoch 230/1000\n",
      "246/246 [==============================] - 0s 292us/sample - loss: 5.6210e-04\n",
      "Epoch 231/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 4.4720e-04\n",
      "Epoch 232/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 4.2532e-04\n",
      "Epoch 233/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 5.0179e-04\n",
      "Epoch 234/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 6.2780e-04\n",
      "Epoch 235/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 8.1830e-04\n",
      "Epoch 236/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 6.9867e-04\n",
      "Epoch 237/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 5.9869e-04\n",
      "Epoch 238/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 5.8372e-04\n",
      "Epoch 239/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 5.5308e-04\n",
      "Epoch 240/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 6.2024e-04\n",
      "Epoch 241/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 5.2691e-04\n",
      "Epoch 242/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 6.5533e-04\n",
      "Epoch 243/1000\n",
      "246/246 [==============================] - 0s 300us/sample - loss: 5.0188e-04\n",
      "Epoch 244/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 5.0562e-04\n",
      "Epoch 245/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 4.4754e-04\n",
      "Epoch 246/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 5.2990e-04\n",
      "Epoch 247/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 4.6157e-04\n",
      "Epoch 248/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 4.5915e-04\n",
      "Epoch 249/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 4.3837e-04\n",
      "Epoch 250/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 5.2370e-04\n",
      "Epoch 251/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 4.2136e-04\n",
      "Epoch 252/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 4.8606e-04\n",
      "Epoch 253/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 4.6210e-04\n",
      "Epoch 254/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 5.1018e-04\n",
      "Epoch 255/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 4.2703e-04\n",
      "Epoch 256/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 4.9234e-04\n",
      "Epoch 257/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 5.4238e-04\n",
      "Epoch 258/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 162us/sample - loss: 4.9640e-04\n",
      "Epoch 259/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 4.9624e-04\n",
      "Epoch 260/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 4.3807e-04\n",
      "Epoch 261/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 4.0408e-04\n",
      "Epoch 262/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 4.3359e-04\n",
      "Epoch 263/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 4.1905e-04\n",
      "Epoch 264/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 3.5582e-04\n",
      "Epoch 265/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 3.6960e-04\n",
      "Epoch 266/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 3.9199e-04\n",
      "Epoch 267/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 4.2828e-04\n",
      "Epoch 268/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 4.3333e-04\n",
      "Epoch 269/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 4.1736e-04\n",
      "Epoch 270/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 3.9850e-04\n",
      "Epoch 271/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 4.2203e-04\n",
      "Epoch 272/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 4.6370e-04\n",
      "Epoch 273/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 4.0243e-04\n",
      "Epoch 274/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 3.5168e-04\n",
      "Epoch 275/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 3.4898e-04\n",
      "Epoch 276/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 4.0389e-04\n",
      "Epoch 277/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 3.4674e-04\n",
      "Epoch 278/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 3.9461e-04\n",
      "Epoch 279/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 3.5620e-04\n",
      "Epoch 280/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 4.4588e-04\n",
      "Epoch 281/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 3.5256e-04\n",
      "Epoch 282/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 3.8721e-04\n",
      "Epoch 283/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 5.3711e-04\n",
      "Epoch 284/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 4.5724e-04\n",
      "Epoch 285/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 5.0915e-04\n",
      "Epoch 286/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 5.5328e-04\n",
      "Epoch 287/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 5.0585e-04\n",
      "Epoch 288/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 4.3401e-04\n",
      "Epoch 289/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 4.0405e-04\n",
      "Epoch 290/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 3.8355e-04\n",
      "Epoch 291/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 4.6688e-04\n",
      "Epoch 292/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 4.3131e-04\n",
      "Epoch 293/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 3.7816e-04\n",
      "Epoch 294/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 3.7553e-04\n",
      "Epoch 295/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 3.3956e-04\n",
      "Epoch 296/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 3.5859e-04\n",
      "Epoch 297/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 3.5355e-04\n",
      "Epoch 298/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 3.6214e-04\n",
      "Epoch 299/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 3.2783e-04\n",
      "Epoch 300/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 3.4412e-04\n",
      "Epoch 301/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 3.5914e-04\n",
      "Epoch 302/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 5.4555e-04\n",
      "Epoch 303/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 4.3502e-04\n",
      "Epoch 304/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 4.0298e-04\n",
      "Epoch 305/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 3.3713e-04\n",
      "Epoch 306/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 3.0994e-04\n",
      "Epoch 307/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 3.5929e-04\n",
      "Epoch 308/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 3.5114e-04\n",
      "Epoch 309/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 4.1281e-04\n",
      "Epoch 310/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 3.9749e-04\n",
      "Epoch 311/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 3.4765e-04\n",
      "Epoch 312/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 3.3149e-04\n",
      "Epoch 313/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 3.4639e-04\n",
      "Epoch 314/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 3.3352e-04\n",
      "Epoch 315/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 2.9925e-04\n",
      "Epoch 316/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 3.2719e-04\n",
      "Epoch 317/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 3.8133e-04\n",
      "Epoch 318/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 3.4026e-04\n",
      "Epoch 319/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 3.0280e-04\n",
      "Epoch 320/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 3.5278e-04\n",
      "Epoch 321/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 3.2564e-04\n",
      "Epoch 322/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 3.1184e-04\n",
      "Epoch 323/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 3.4061e-04\n",
      "Epoch 324/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 3.6213e-04\n",
      "Epoch 325/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 3.4079e-04\n",
      "Epoch 326/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 3.4203e-04\n",
      "Epoch 327/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 3.8417e-04\n",
      "Epoch 328/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 4.4796e-04\n",
      "Epoch 329/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 4.6093e-04\n",
      "Epoch 330/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 3.3582e-04\n",
      "Epoch 331/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 3.5019e-04\n",
      "Epoch 332/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 4.1355e-04\n",
      "Epoch 333/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 4.2655e-04\n",
      "Epoch 334/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 6.1642e-04\n",
      "Epoch 335/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 5.5697e-04\n",
      "Epoch 336/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 3.7584e-04\n",
      "Epoch 337/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 3.0331e-04\n",
      "Epoch 338/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 3.0992e-04\n",
      "Epoch 339/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 3.3427e-04\n",
      "Epoch 340/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 3.6496e-04\n",
      "Epoch 341/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 3.0360e-04\n",
      "Epoch 342/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 2.9425e-04\n",
      "Epoch 343/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 2.7802e-04\n",
      "Epoch 344/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 2.8350e-04\n",
      "Epoch 345/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.7975e-04\n",
      "Epoch 346/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 178us/sample - loss: 2.7922e-04\n",
      "Epoch 347/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 2.8107e-04\n",
      "Epoch 348/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 3.2269e-04\n",
      "Epoch 349/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 2.9831e-04\n",
      "Epoch 350/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 3.0734e-04\n",
      "Epoch 351/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 3.2087e-04\n",
      "Epoch 352/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 3.3978e-04\n",
      "Epoch 353/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.9779e-04\n",
      "Epoch 354/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 3.3805e-04\n",
      "Epoch 355/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 4.3505e-04\n",
      "Epoch 356/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 3.1791e-04\n",
      "Epoch 357/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 5.2982e-04\n",
      "Epoch 358/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 6.3180e-04\n",
      "Epoch 359/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 3.6500e-04\n",
      "Epoch 360/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 3.8423e-04\n",
      "Epoch 361/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 3.3795e-04\n",
      "Epoch 362/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 4.1710e-04\n",
      "Epoch 363/1000\n",
      "246/246 [==============================] - 0s 276us/sample - loss: 4.6522e-04\n",
      "Epoch 364/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 3.4261e-04\n",
      "Epoch 365/1000\n",
      "246/246 [==============================] - ETA: 0s - loss: 3.1234e-0 - 0s 251us/sample - loss: 3.1438e-04\n",
      "Epoch 366/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 3.0180e-04\n",
      "Epoch 367/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 2.9386e-04\n",
      "Epoch 368/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 3.2574e-04\n",
      "Epoch 369/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 3.0395e-04\n",
      "Epoch 370/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 2.7929e-04\n",
      "Epoch 371/1000\n",
      "246/246 [==============================] - 0s 292us/sample - loss: 3.2105e-04\n",
      "Epoch 372/1000\n",
      "246/246 [==============================] - 0s 300us/sample - loss: 3.0070e-04\n",
      "Epoch 373/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 3.0675e-04\n",
      "Epoch 374/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 3.5154e-04\n",
      "Epoch 375/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 3.1314e-04\n",
      "Epoch 376/1000\n",
      "246/246 [==============================] - 0s 288us/sample - loss: 3.4935e-04\n",
      "Epoch 377/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 3.3696e-04\n",
      "Epoch 378/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 3.2498e-04\n",
      "Epoch 379/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 2.9177e-04\n",
      "Epoch 380/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 2.9384e-04\n",
      "Epoch 381/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 2.7658e-04\n",
      "Epoch 382/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 2.5793e-04\n",
      "Epoch 383/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 2.4780e-04\n",
      "Epoch 384/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 2.7320e-04\n",
      "Epoch 385/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 2.7192e-04\n",
      "Epoch 386/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 2.7893e-04\n",
      "Epoch 387/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 3.3671e-04\n",
      "Epoch 388/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 3.5558e-04\n",
      "Epoch 389/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 3.1627e-04\n",
      "Epoch 390/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 3.4676e-04\n",
      "Epoch 391/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 3.1961e-04\n",
      "Epoch 392/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 3.3155e-04\n",
      "Epoch 393/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 3.7453e-04\n",
      "Epoch 394/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 4.0916e-04\n",
      "Epoch 395/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 3.3687e-04\n",
      "Epoch 396/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 2.9753e-04\n",
      "Epoch 397/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 2.6886e-04\n",
      "Epoch 398/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 2.8064e-04\n",
      "Epoch 399/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 3.1347e-04\n",
      "Epoch 400/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 2.9555e-04\n",
      "Epoch 401/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 2.6243e-04\n",
      "Epoch 402/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 2.6011e-04\n",
      "Epoch 403/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 2.6658e-04\n",
      "Epoch 404/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 2.5599e-04\n",
      "Epoch 405/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 2.9635e-04\n",
      "Epoch 406/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 2.3767e-04\n",
      "Epoch 407/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 2.4149e-04\n",
      "Epoch 408/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 2.7490e-04\n",
      "Epoch 409/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 2.6707e-04\n",
      "Epoch 410/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 2.8215e-04\n",
      "Epoch 411/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 3.8108e-04\n",
      "Epoch 412/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 4.4647e-04\n",
      "Epoch 413/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 4.5437e-04\n",
      "Epoch 414/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 3.2971e-04\n",
      "Epoch 415/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 2.8368e-04\n",
      "Epoch 416/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 3.2124e-04\n",
      "Epoch 417/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.7581e-04\n",
      "Epoch 418/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 2.9266e-04\n",
      "Epoch 419/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 2.6789e-04\n",
      "Epoch 420/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 2.6608e-04\n",
      "Epoch 421/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.9736e-04\n",
      "Epoch 422/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 3.0921e-04\n",
      "Epoch 423/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 2.5725e-04\n",
      "Epoch 424/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 2.4826e-04\n",
      "Epoch 425/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 2.9945e-04\n",
      "Epoch 426/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 2.5669e-04\n",
      "Epoch 427/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 2.4197e-04\n",
      "Epoch 428/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 2.7552e-04\n",
      "Epoch 429/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 2.7692e-04\n",
      "Epoch 430/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 3.0150e-04\n",
      "Epoch 431/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 2.7791e-04\n",
      "Epoch 432/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 2.6387e-04\n",
      "Epoch 433/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 2.5031e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 434/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 2.5885e-04\n",
      "Epoch 435/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 2.4698e-04\n",
      "Epoch 436/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 2.7932e-04\n",
      "Epoch 437/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 2.4729e-04\n",
      "Epoch 438/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 2.5962e-04\n",
      "Epoch 439/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 2.3601e-04\n",
      "Epoch 440/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.1777e-04\n",
      "Epoch 441/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 2.1398e-04\n",
      "Epoch 442/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.5167e-04\n",
      "Epoch 443/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 2.4813e-04\n",
      "Epoch 444/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 2.2498e-04\n",
      "Epoch 445/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 2.4987e-04\n",
      "Epoch 446/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 2.6291e-04\n",
      "Epoch 447/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 2.6788e-04\n",
      "Epoch 448/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 2.3258e-04\n",
      "Epoch 449/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 2.5240e-04\n",
      "Epoch 450/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 2.5214e-04\n",
      "Epoch 451/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 2.2251e-04\n",
      "Epoch 452/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 2.8739e-04\n",
      "Epoch 453/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 2.3960e-04\n",
      "Epoch 454/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 2.3318e-04\n",
      "Epoch 455/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 2.5812e-04\n",
      "Epoch 456/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 2.5419e-04\n",
      "Epoch 457/1000\n",
      "246/246 [==============================] - 0s 304us/sample - loss: 2.6959e-04\n",
      "Epoch 458/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 3.1886e-04\n",
      "Epoch 459/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 3.0775e-04\n",
      "Epoch 460/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 3.2491e-04\n",
      "Epoch 461/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 3.9385e-04\n",
      "Epoch 462/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 5.7875e-04\n",
      "Epoch 463/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 5.4485e-04\n",
      "Epoch 464/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 4.0588e-04\n",
      "Epoch 465/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 4.0440e-04\n",
      "Epoch 466/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 3.3289e-04\n",
      "Epoch 467/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 3.1009e-04\n",
      "Epoch 468/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 2.5586e-04\n",
      "Epoch 469/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 2.9506e-04\n",
      "Epoch 470/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 2.5463e-04\n",
      "Epoch 471/1000\n",
      "246/246 [==============================] - 0s 276us/sample - loss: 3.1340e-04\n",
      "Epoch 472/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 3.6798e-04\n",
      "Epoch 473/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 3.1928e-04\n",
      "Epoch 474/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 3.6434e-04\n",
      "Epoch 475/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 3.6323e-04\n",
      "Epoch 476/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 2.5206e-04\n",
      "Epoch 477/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 2.8373e-04\n",
      "Epoch 478/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 3.2454e-04\n",
      "Epoch 479/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 2.6688e-04\n",
      "Epoch 480/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 2.8360e-04\n",
      "Epoch 481/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.9423e-04\n",
      "Epoch 482/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 2.8457e-04\n",
      "Epoch 483/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.4356e-04\n",
      "Epoch 484/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 2.2187e-04\n",
      "Epoch 485/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.0878e-04\n",
      "Epoch 486/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 2.0097e-04\n",
      "Epoch 487/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 1.8504e-04\n",
      "Epoch 488/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 1.9914e-04\n",
      "Epoch 489/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 1.9676e-04\n",
      "Epoch 490/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.9498e-04\n",
      "Epoch 491/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 1.9367e-04\n",
      "Epoch 492/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 2.9279e-04\n",
      "Epoch 493/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 3.5007e-04\n",
      "Epoch 494/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 3.4631e-04\n",
      "Epoch 495/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 2.3937e-04\n",
      "Epoch 496/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 2.0498e-04\n",
      "Epoch 497/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 2.1626e-04\n",
      "Epoch 498/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 2.1266e-04\n",
      "Epoch 499/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 2.4577e-04\n",
      "Epoch 500/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 2.2621e-04\n",
      "Epoch 501/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.2928e-04\n",
      "Epoch 502/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 1.9242e-04\n",
      "Epoch 503/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 1.9323e-04\n",
      "Epoch 504/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 1.8562e-04\n",
      "Epoch 505/1000\n",
      "246/246 [==============================] - 0s 150us/sample - loss: 2.1360e-04\n",
      "Epoch 506/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 1.9252e-04\n",
      "Epoch 507/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 1.9032e-04\n",
      "Epoch 508/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.9045e-04\n",
      "Epoch 509/1000\n",
      "246/246 [==============================] - 0s 150us/sample - loss: 1.8315e-04\n",
      "Epoch 510/1000\n",
      "246/246 [==============================] - 0s 146us/sample - loss: 1.8003e-04\n",
      "Epoch 511/1000\n",
      "246/246 [==============================] - 0s 146us/sample - loss: 1.7430e-04\n",
      "Epoch 512/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 1.7012e-04\n",
      "Epoch 513/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 1.6790e-04\n",
      "Epoch 514/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 2.1191e-04\n",
      "Epoch 515/1000\n",
      "246/246 [==============================] - 0s 150us/sample - loss: 1.9993e-04\n",
      "Epoch 516/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 2.0786e-04\n",
      "Epoch 517/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 2.1015e-04\n",
      "Epoch 518/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 3.2373e-04\n",
      "Epoch 519/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 3.0716e-04\n",
      "Epoch 520/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 3.2546e-04\n",
      "Epoch 521/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 3.1144e-04\n",
      "Epoch 522/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 162us/sample - loss: 3.3093e-04\n",
      "Epoch 523/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.7264e-04\n",
      "Epoch 524/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 2.3748e-04\n",
      "Epoch 525/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.5293e-04\n",
      "Epoch 526/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.4200e-04\n",
      "Epoch 527/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 2.7940e-04\n",
      "Epoch 528/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 2.4051e-04\n",
      "Epoch 529/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 2.7617e-04\n",
      "Epoch 530/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.2417e-04\n",
      "Epoch 531/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 2.7559e-04\n",
      "Epoch 532/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 3.6749e-04\n",
      "Epoch 533/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 3.8118e-04\n",
      "Epoch 534/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 2.6551e-04\n",
      "Epoch 535/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 2.8712e-04\n",
      "Epoch 536/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 2.2102e-04\n",
      "Epoch 537/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.5149e-04\n",
      "Epoch 538/1000\n",
      "246/246 [==============================] - 0s 150us/sample - loss: 1.8607e-04\n",
      "Epoch 539/1000\n",
      "246/246 [==============================] - 0s 146us/sample - loss: 2.1545e-04\n",
      "Epoch 540/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 1.7864e-04\n",
      "Epoch 541/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 1.8352e-04\n",
      "Epoch 542/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 1.8180e-04\n",
      "Epoch 543/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 2.0651e-04\n",
      "Epoch 544/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 2.5116e-04\n",
      "Epoch 545/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 2.3228e-04\n",
      "Epoch 546/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 2.7029e-04\n",
      "Epoch 547/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 2.8933e-04\n",
      "Epoch 548/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 2.5589e-04\n",
      "Epoch 549/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 3.8705e-04\n",
      "Epoch 550/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 3.9187e-04\n",
      "Epoch 551/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 2.8779e-04\n",
      "Epoch 552/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 3.1208e-04\n",
      "Epoch 553/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 2.7282e-04\n",
      "Epoch 554/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.3040e-04\n",
      "Epoch 555/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 2.0822e-04\n",
      "Epoch 556/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.3568e-04\n",
      "Epoch 557/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 2.1921e-04\n",
      "Epoch 558/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 2.2786e-04\n",
      "Epoch 559/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 1.7595e-04\n",
      "Epoch 560/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 1.7360e-04\n",
      "Epoch 561/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 1.9405e-04\n",
      "Epoch 562/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.2732e-04\n",
      "Epoch 563/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 2.1944e-04\n",
      "Epoch 564/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 2.0743e-04\n",
      "Epoch 565/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 2.6195e-04\n",
      "Epoch 566/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 2.5190e-04\n",
      "Epoch 567/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 2.1431e-04\n",
      "Epoch 568/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 2.0009e-04\n",
      "Epoch 569/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 1.7404e-04\n",
      "Epoch 570/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 1.9139e-04\n",
      "Epoch 571/1000\n",
      "246/246 [==============================] - 0s 300us/sample - loss: 2.2345e-04\n",
      "Epoch 572/1000\n",
      "246/246 [==============================] - ETA: 0s - loss: 1.4055e-0 - 0s 203us/sample - loss: 2.0322e-04\n",
      "Epoch 573/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 2.2634e-04\n",
      "Epoch 574/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 2.2005e-04\n",
      "Epoch 575/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.8082e-04\n",
      "Epoch 576/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 1.6535e-04\n",
      "Epoch 577/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 1.8216e-04\n",
      "Epoch 578/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 1.9908e-04\n",
      "Epoch 579/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 1.6161e-04\n",
      "Epoch 580/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 1.8113e-04\n",
      "Epoch 581/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 1.6552e-04\n",
      "Epoch 582/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 1.7589e-04\n",
      "Epoch 583/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 1.5479e-04\n",
      "Epoch 584/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 1.6263e-04\n",
      "Epoch 585/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 1.7611e-04\n",
      "Epoch 586/1000\n",
      "246/246 [==============================] - 0s 357us/sample - loss: 1.9806e-04\n",
      "Epoch 587/1000\n",
      "246/246 [==============================] - 0s 296us/sample - loss: 2.9233e-04\n",
      "Epoch 588/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 2.6137e-04\n",
      "Epoch 589/1000\n",
      "246/246 [==============================] - 0s 300us/sample - loss: 1.8893e-04\n",
      "Epoch 590/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 1.8382e-04\n",
      "Epoch 591/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 1.8315e-04\n",
      "Epoch 592/1000\n",
      "246/246 [==============================] - 0s 357us/sample - loss: 1.9171e-04\n",
      "Epoch 593/1000\n",
      "246/246 [==============================] - 0s 296us/sample - loss: 1.6429e-04\n",
      "Epoch 594/1000\n",
      "246/246 [==============================] - 0s 292us/sample - loss: 1.4077e-04\n",
      "Epoch 595/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 1.4904e-04\n",
      "Epoch 596/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 1.6133e-04\n",
      "Epoch 597/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 1.6961e-04\n",
      "Epoch 598/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 1.6463e-04\n",
      "Epoch 599/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 1.7307e-04\n",
      "Epoch 600/1000\n",
      "246/246 [==============================] - 0s 316us/sample - loss: 1.4237e-04\n",
      "Epoch 601/1000\n",
      "246/246 [==============================] - 0s 349us/sample - loss: 1.4590e-04\n",
      "Epoch 602/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 1.6690e-04\n",
      "Epoch 603/1000\n",
      "246/246 [==============================] - 0s 288us/sample - loss: 1.3436e-04\n",
      "Epoch 604/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 1.5158e-04\n",
      "Epoch 605/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 1.8659e-04\n",
      "Epoch 606/1000\n",
      "246/246 [==============================] - 0s 438us/sample - loss: 2.1257e-04\n",
      "Epoch 607/1000\n",
      "246/246 [==============================] - 0s 292us/sample - loss: 2.4326e-04\n",
      "Epoch 608/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 1.9275e-04\n",
      "Epoch 609/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 1.8166e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 610/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 1.3705e-04\n",
      "Epoch 611/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 1.3552e-04\n",
      "Epoch 612/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 1.2494e-04\n",
      "Epoch 613/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 1.1266e-04\n",
      "Epoch 614/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 1.2631e-04\n",
      "Epoch 615/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 1.6963e-04\n",
      "Epoch 616/1000\n",
      "246/246 [==============================] - 0s 365us/sample - loss: 1.4880e-04\n",
      "Epoch 617/1000\n",
      "246/246 [==============================] - 0s 401us/sample - loss: 1.5279e-04\n",
      "Epoch 618/1000\n",
      "246/246 [==============================] - 0s 308us/sample - loss: 1.3695e-04\n",
      "Epoch 619/1000\n",
      "246/246 [==============================] - 0s 414us/sample - loss: 1.5839e-04\n",
      "Epoch 620/1000\n",
      "246/246 [==============================] - 0s 365us/sample - loss: 1.3021e-04\n",
      "Epoch 621/1000\n",
      "246/246 [==============================] - 0s 328us/sample - loss: 1.5124e-04\n",
      "Epoch 622/1000\n",
      "246/246 [==============================] - 0s 405us/sample - loss: 1.8768e-04\n",
      "Epoch 623/1000\n",
      "246/246 [==============================] - ETA: 0s - loss: 1.1847e-0 - 0s 369us/sample - loss: 1.8650e-04\n",
      "Epoch 624/1000\n",
      "246/246 [==============================] - 0s 365us/sample - loss: 2.0217e-04\n",
      "Epoch 625/1000\n",
      "246/246 [==============================] - 0s 377us/sample - loss: 1.9724e-04\n",
      "Epoch 626/1000\n",
      "246/246 [==============================] - 0s 332us/sample - loss: 1.8064e-04\n",
      "Epoch 627/1000\n",
      "246/246 [==============================] - 0s 349us/sample - loss: 1.5803e-04\n",
      "Epoch 628/1000\n",
      "246/246 [==============================] - 0s 353us/sample - loss: 2.2073e-04\n",
      "Epoch 629/1000\n",
      "246/246 [==============================] - 0s 341us/sample - loss: 2.0380e-04\n",
      "Epoch 630/1000\n",
      "246/246 [==============================] - 0s 349us/sample - loss: 2.4832e-04\n",
      "Epoch 631/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 2.1708e-04\n",
      "Epoch 632/1000\n",
      "246/246 [==============================] - 0s 385us/sample - loss: 2.7332e-04\n",
      "Epoch 633/1000\n",
      "246/246 [==============================] - 0s 349us/sample - loss: 2.4522e-04\n",
      "Epoch 634/1000\n",
      "246/246 [==============================] - 0s 357us/sample - loss: 2.9994e-04\n",
      "Epoch 635/1000\n",
      "246/246 [==============================] - ETA: 0s - loss: 2.1435e-0 - 0s 397us/sample - loss: 2.2387e-04\n",
      "Epoch 636/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 1.6780e-04\n",
      "Epoch 637/1000\n",
      "246/246 [==============================] - 0s 337us/sample - loss: 1.3251e-04\n",
      "Epoch 638/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 1.2104e-04\n",
      "Epoch 639/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 1.3449e-04\n",
      "Epoch 640/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 1.2330e-04\n",
      "Epoch 641/1000\n",
      "246/246 [==============================] - 0s 276us/sample - loss: 1.2209e-04\n",
      "Epoch 642/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 1.2143e-04\n",
      "Epoch 643/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 1.0981e-04\n",
      "Epoch 644/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 1.2005e-04\n",
      "Epoch 645/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 1.4319e-04\n",
      "Epoch 646/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 1.3115e-04\n",
      "Epoch 647/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 1.3916e-04\n",
      "Epoch 648/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 1.6989e-04\n",
      "Epoch 649/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 1.5849e-04\n",
      "Epoch 650/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 1.4600e-04\n",
      "Epoch 651/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 1.3816e-04\n",
      "Epoch 652/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 1.5903e-04\n",
      "Epoch 653/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 1.4548e-04\n",
      "Epoch 654/1000\n",
      "246/246 [==============================] - 0s 300us/sample - loss: 1.3757e-04\n",
      "Epoch 655/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 1.4817e-04\n",
      "Epoch 656/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 1.2660e-04\n",
      "Epoch 657/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 1.2459e-04\n",
      "Epoch 658/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 1.6116e-04\n",
      "Epoch 659/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 1.9679e-04\n",
      "Epoch 660/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 1.6161e-04\n",
      "Epoch 661/1000\n",
      "246/246 [==============================] - 0s 288us/sample - loss: 1.7614e-04\n",
      "Epoch 662/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 2.3423e-04\n",
      "Epoch 663/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 1.5343e-04\n",
      "Epoch 664/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.7407e-04\n",
      "Epoch 665/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 2.0054e-04\n",
      "Epoch 666/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 1.6933e-04\n",
      "Epoch 667/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 1.1903e-04\n",
      "Epoch 668/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.2518e-04\n",
      "Epoch 669/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 1.1659e-04\n",
      "Epoch 670/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 1.1806e-04\n",
      "Epoch 671/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 1.1234e-04\n",
      "Epoch 672/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 1.1747e-04\n",
      "Epoch 673/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 1.0188e-04\n",
      "Epoch 674/1000\n",
      "246/246 [==============================] - 0s 292us/sample - loss: 1.3312e-04\n",
      "Epoch 675/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 1.0665e-04\n",
      "Epoch 676/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 1.0457e-04\n",
      "Epoch 677/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 9.1633e-05\n",
      "Epoch 678/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 9.4997e-05\n",
      "Epoch 679/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 8.6364e-05\n",
      "Epoch 680/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 1.0005e-04\n",
      "Epoch 681/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 9.1409e-05\n",
      "Epoch 682/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 1.1619e-04\n",
      "Epoch 683/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 1.1609e-04\n",
      "Epoch 684/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 1.0313e-04\n",
      "Epoch 685/1000\n",
      "246/246 [==============================] - 0s 332us/sample - loss: 1.1488e-04\n",
      "Epoch 686/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 1.2600e-04\n",
      "Epoch 687/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 1.1398e-04\n",
      "Epoch 688/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 1.0440e-04\n",
      "Epoch 689/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 9.7502e-05\n",
      "Epoch 690/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 9.4604e-05\n",
      "Epoch 691/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.0619e-04\n",
      "Epoch 692/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 1.1210e-04\n",
      "Epoch 693/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 1.6465e-04\n",
      "Epoch 694/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 1.5238e-04\n",
      "Epoch 695/1000\n",
      "246/246 [==============================] - 0s 292us/sample - loss: 1.6617e-04\n",
      "Epoch 696/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 1.4512e-04\n",
      "Epoch 697/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 191us/sample - loss: 1.2403e-04\n",
      "Epoch 698/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 1.2578e-04\n",
      "Epoch 699/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 1.2760e-04\n",
      "Epoch 700/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 1.3155e-04\n",
      "Epoch 701/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 1.0972e-04\n",
      "Epoch 702/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 1.1061e-04\n",
      "Epoch 703/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 1.0738e-04\n",
      "Epoch 704/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 9.8269e-05\n",
      "Epoch 705/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 9.1999e-05\n",
      "Epoch 706/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 9.3199e-05\n",
      "Epoch 707/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 9.7147e-05\n",
      "Epoch 708/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 1.0311e-04\n",
      "Epoch 709/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 1.2672e-04\n",
      "Epoch 710/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 1.2164e-04\n",
      "Epoch 711/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 1.2114e-04\n",
      "Epoch 712/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.5031e-04\n",
      "Epoch 713/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 1.2839e-04\n",
      "Epoch 714/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 1.1849e-04\n",
      "Epoch 715/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 1.2911e-04\n",
      "Epoch 716/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 1.1481e-04\n",
      "Epoch 717/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 1.2374e-04\n",
      "Epoch 718/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 9.2708e-05\n",
      "Epoch 719/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 9.5549e-05\n",
      "Epoch 720/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 8.7053e-05\n",
      "Epoch 721/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 8.5602e-05\n",
      "Epoch 722/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.0402e-04\n",
      "Epoch 723/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 7.9834e-05\n",
      "Epoch 724/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 7.9114e-05\n",
      "Epoch 725/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 7.3893e-05\n",
      "Epoch 726/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 1.0063e-04\n",
      "Epoch 727/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 1.0467e-04\n",
      "Epoch 728/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 8.9814e-05\n",
      "Epoch 729/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 9.4726e-05\n",
      "Epoch 730/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 1.3178e-04\n",
      "Epoch 731/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 8.9180e-05\n",
      "Epoch 732/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 9.0186e-05\n",
      "Epoch 733/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 1.3039e-04\n",
      "Epoch 734/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 1.0048e-04\n",
      "Epoch 735/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 9.9299e-05\n",
      "Epoch 736/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 1.2299e-04\n",
      "Epoch 737/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.8204e-04\n",
      "Epoch 738/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 1.4917e-04\n",
      "Epoch 739/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 1.1280e-04\n",
      "Epoch 740/1000\n",
      "246/246 [==============================] - 0s 190us/sample - loss: 1.5117e-04\n",
      "Epoch 741/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 1.1936e-04\n",
      "Epoch 742/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.1809e-04\n",
      "Epoch 743/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 1.1678e-04\n",
      "Epoch 744/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 1.3994e-04\n",
      "Epoch 745/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 1.1239e-04\n",
      "Epoch 746/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.0027e-04\n",
      "Epoch 747/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 9.5040e-05\n",
      "Epoch 748/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 1.0106e-04\n",
      "Epoch 749/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 9.9156e-05\n",
      "Epoch 750/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 1.7895e-04\n",
      "Epoch 751/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 1.5786e-04\n",
      "Epoch 752/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 1.0988e-04\n",
      "Epoch 753/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 9.4012e-05\n",
      "Epoch 754/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 8.7548e-05\n",
      "Epoch 755/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 8.6750e-05\n",
      "Epoch 756/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 8.6491e-05\n",
      "Epoch 757/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 9.5202e-05\n",
      "Epoch 758/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 1.0696e-04\n",
      "Epoch 759/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 9.1975e-05\n",
      "Epoch 760/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 7.8302e-05\n",
      "Epoch 761/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 9.5606e-05\n",
      "Epoch 762/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 6.9777e-05\n",
      "Epoch 763/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 7.5138e-05\n",
      "Epoch 764/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 7.1459e-05\n",
      "Epoch 765/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 8.9446e-05\n",
      "Epoch 766/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 7.4132e-05\n",
      "Epoch 767/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 7.9851e-05\n",
      "Epoch 768/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 8.2698e-05\n",
      "Epoch 769/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 7.5149e-05\n",
      "Epoch 770/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 8.9228e-05\n",
      "Epoch 771/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 7.6143e-05\n",
      "Epoch 772/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 7.8994e-05\n",
      "Epoch 773/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 7.7811e-05\n",
      "Epoch 774/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 7.9932e-05\n",
      "Epoch 775/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 7.5390e-05\n",
      "Epoch 776/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 8.3048e-05\n",
      "Epoch 777/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 9.8571e-05\n",
      "Epoch 778/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 7.4203e-05\n",
      "Epoch 779/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 9.7038e-05\n",
      "Epoch 780/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 1.1926e-04\n",
      "Epoch 781/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 1.5751e-04\n",
      "Epoch 782/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 1.6959e-04\n",
      "Epoch 783/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 2.5374e-04\n",
      "Epoch 784/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 1.8421e-04\n",
      "Epoch 785/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 182us/sample - loss: 1.3332e-04\n",
      "Epoch 786/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.9016e-04\n",
      "Epoch 787/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.3198e-04\n",
      "Epoch 788/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 1.7452e-04\n",
      "Epoch 789/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 1.8963e-04\n",
      "Epoch 790/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.9916e-04\n",
      "Epoch 791/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 1.8300e-04\n",
      "Epoch 792/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 1.8726e-04\n",
      "Epoch 793/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 1.7859e-04\n",
      "Epoch 794/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 1.5382e-04\n",
      "Epoch 795/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 2.0374e-04\n",
      "Epoch 796/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 1.4128e-04\n",
      "Epoch 797/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 1.1005e-04\n",
      "Epoch 798/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.3779e-04\n",
      "Epoch 799/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 1.4986e-04\n",
      "Epoch 800/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 1.1204e-04\n",
      "Epoch 801/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 8.8937e-05\n",
      "Epoch 802/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.0111e-04\n",
      "Epoch 803/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 8.4786e-05\n",
      "Epoch 804/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 6.9815e-05\n",
      "Epoch 805/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 7.3541e-05\n",
      "Epoch 806/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 6.3135e-05\n",
      "Epoch 807/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 6.1083e-05\n",
      "Epoch 808/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 8.6704e-05\n",
      "Epoch 809/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 6.9654e-05\n",
      "Epoch 810/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 8.1291e-05\n",
      "Epoch 811/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 6.6798e-05\n",
      "Epoch 812/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 6.0202e-05\n",
      "Epoch 813/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 5.8732e-05\n",
      "Epoch 814/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 6.3896e-05\n",
      "Epoch 815/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 7.2887e-05\n",
      "Epoch 816/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 7.3644e-05\n",
      "Epoch 817/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 7.3541e-05\n",
      "Epoch 818/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 7.1316e-05\n",
      "Epoch 819/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 6.5710e-05\n",
      "Epoch 820/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 7.8924e-05\n",
      "Epoch 821/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 6.9742e-05\n",
      "Epoch 822/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 5.2065e-05\n",
      "Epoch 823/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 6.0971e-05\n",
      "Epoch 824/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 7.0452e-05\n",
      "Epoch 825/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 6.2122e-05\n",
      "Epoch 826/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 6.3326e-05\n",
      "Epoch 827/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 7.3152e-05\n",
      "Epoch 828/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 8.1934e-05\n",
      "Epoch 829/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 7.6258e-05\n",
      "Epoch 830/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 7.4708e-05\n",
      "Epoch 831/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 7.3507e-05\n",
      "Epoch 832/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 7.5268e-05\n",
      "Epoch 833/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 7.1192e-05\n",
      "Epoch 834/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 8.2756e-05\n",
      "Epoch 835/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 7.4755e-05\n",
      "Epoch 836/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 8.0675e-05\n",
      "Epoch 837/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 8.3937e-05\n",
      "Epoch 838/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 8.1894e-05\n",
      "Epoch 839/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 5.8441e-05\n",
      "Epoch 840/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 6.2698e-05\n",
      "Epoch 841/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 6.0380e-05\n",
      "Epoch 842/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 5.3747e-05\n",
      "Epoch 843/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 6.0919e-05\n",
      "Epoch 844/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 5.5958e-05\n",
      "Epoch 845/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 6.3871e-05\n",
      "Epoch 846/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 5.8517e-05\n",
      "Epoch 847/1000\n",
      "246/246 [==============================] - 0s 150us/sample - loss: 7.2362e-05\n",
      "Epoch 848/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 7.2529e-05\n",
      "Epoch 849/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 8.8079e-05\n",
      "Epoch 850/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.2004e-04\n",
      "Epoch 851/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.2501e-04\n",
      "Epoch 852/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 1.2440e-04\n",
      "Epoch 853/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 1.3801e-04\n",
      "Epoch 854/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 1.2041e-04\n",
      "Epoch 855/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.1559e-04\n",
      "Epoch 856/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 1.3528e-04\n",
      "Epoch 857/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 1.1679e-04\n",
      "Epoch 858/1000\n",
      "246/246 [==============================] - 0s 158us/sample - loss: 1.2275e-04\n",
      "Epoch 859/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 9.9790e-05\n",
      "Epoch 860/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 9.6698e-05\n",
      "Epoch 861/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.5676e-04\n",
      "Epoch 862/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 1.3664e-04\n",
      "Epoch 863/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 1.0411e-04\n",
      "Epoch 864/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 1.0931e-04\n",
      "Epoch 865/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 9.4566e-05\n",
      "Epoch 866/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 8.5993e-05\n",
      "Epoch 867/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 6.3316e-05\n",
      "Epoch 868/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 7.2091e-05\n",
      "Epoch 869/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 9.7480e-05\n",
      "Epoch 870/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 1.0303e-04\n",
      "Epoch 871/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 1.0729e-04\n",
      "Epoch 872/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 8.5115e-05\n",
      "Epoch 873/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 178us/sample - loss: 9.7543e-05\n",
      "Epoch 874/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 8.3324e-05\n",
      "Epoch 875/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 7.4255e-05\n",
      "Epoch 876/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 7.5373e-05\n",
      "Epoch 877/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 6.1139e-05\n",
      "Epoch 878/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 6.3792e-05\n",
      "Epoch 879/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 5.6832e-05\n",
      "Epoch 880/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 6.1332e-05\n",
      "Epoch 881/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 5.8282e-05\n",
      "Epoch 882/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 7.8561e-05\n",
      "Epoch 883/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 8.2797e-05\n",
      "Epoch 884/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 6.7279e-05\n",
      "Epoch 885/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 7.2786e-05\n",
      "Epoch 886/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 5.7501e-05\n",
      "Epoch 887/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 5.5573e-05\n",
      "Epoch 888/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 5.0472e-05\n",
      "Epoch 889/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 5.3213e-05\n",
      "Epoch 890/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 4.6042e-05\n",
      "Epoch 891/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 4.8282e-05\n",
      "Epoch 892/1000\n",
      "246/246 [==============================] - 0s 162us/sample - loss: 6.1581e-05\n",
      "Epoch 893/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 5.6670e-05\n",
      "Epoch 894/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 5.1609e-05\n",
      "Epoch 895/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 5.7868e-05\n",
      "Epoch 896/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 7.9995e-05\n",
      "Epoch 897/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 5.3632e-05\n",
      "Epoch 898/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 4.8599e-05\n",
      "Epoch 899/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 4.5997e-05\n",
      "Epoch 900/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 6.2140e-05\n",
      "Epoch 901/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 6.7853e-05\n",
      "Epoch 902/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 6.8479e-05\n",
      "Epoch 903/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 7.5358e-05\n",
      "Epoch 904/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 4.8789e-05\n",
      "Epoch 905/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 6.3839e-05\n",
      "Epoch 906/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 7.7394e-05\n",
      "Epoch 907/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 8.8141e-05\n",
      "Epoch 908/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 6.6591e-05\n",
      "Epoch 909/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 7.7093e-05\n",
      "Epoch 910/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 7.1202e-05\n",
      "Epoch 911/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 7.9472e-05\n",
      "Epoch 912/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 8.4235e-05\n",
      "Epoch 913/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 7.5272e-05\n",
      "Epoch 914/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 7.0016e-05\n",
      "Epoch 915/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 7.6662e-05\n",
      "Epoch 916/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 6.0503e-05\n",
      "Epoch 917/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 7.0241e-05\n",
      "Epoch 918/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 5.5531e-05\n",
      "Epoch 919/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 6.3596e-05\n",
      "Epoch 920/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 6.2545e-05\n",
      "Epoch 921/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 5.5104e-05\n",
      "Epoch 922/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 5.6523e-05\n",
      "Epoch 923/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 4.2290e-05\n",
      "Epoch 924/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 4.5324e-05\n",
      "Epoch 925/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 4.8984e-05\n",
      "Epoch 926/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 5.3142e-05\n",
      "Epoch 927/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 4.9371e-05\n",
      "Epoch 928/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 4.3386e-05\n",
      "Epoch 929/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 4.1090e-05\n",
      "Epoch 930/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 3.5875e-05\n",
      "Epoch 931/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 3.1386e-05\n",
      "Epoch 932/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 2.7364e-05\n",
      "Epoch 933/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 2.7505e-05\n",
      "Epoch 934/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 3.1649e-05\n",
      "Epoch 935/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 3.2353e-05\n",
      "Epoch 936/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 3.4009e-05\n",
      "Epoch 937/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 3.2561e-05\n",
      "Epoch 938/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 3.6035e-05\n",
      "Epoch 939/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 4.3744e-05\n",
      "Epoch 940/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 3.9399e-05\n",
      "Epoch 941/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 4.1976e-05\n",
      "Epoch 942/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 3.6859e-05\n",
      "Epoch 943/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 4.5545e-05\n",
      "Epoch 944/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 5.0772e-05\n",
      "Epoch 945/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 6.0959e-05\n",
      "Epoch 946/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 4.9962e-05\n",
      "Epoch 947/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 4.7447e-05\n",
      "Epoch 948/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 4.7249e-05\n",
      "Epoch 949/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 5.5305e-05\n",
      "Epoch 950/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 5.5639e-05\n",
      "Epoch 951/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 5.3229e-05\n",
      "Epoch 952/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 4.1755e-05\n",
      "Epoch 953/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 5.1372e-05\n",
      "Epoch 954/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 6.1718e-05\n",
      "Epoch 955/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 5.1114e-05\n",
      "Epoch 956/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 9.1135e-05\n",
      "Epoch 957/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 1.1375e-04\n",
      "Epoch 958/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 1.4659e-04\n",
      "Epoch 959/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 2.2111e-04\n",
      "Epoch 960/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 2.4673e-04\n",
      "Epoch 961/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 166us/sample - loss: 1.8407e-04\n",
      "Epoch 962/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 1.3373e-04\n",
      "Epoch 963/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 1.4913e-04\n",
      "Epoch 964/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 1.0703e-04\n",
      "Epoch 965/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 1.0680e-04\n",
      "Epoch 966/1000\n",
      "246/246 [==============================] - 0s 154us/sample - loss: 9.6575e-05\n",
      "Epoch 967/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 8.1677e-05\n",
      "Epoch 968/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 1.2296e-04\n",
      "Epoch 969/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 1.5397e-04\n",
      "Epoch 970/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 1.5258e-04\n",
      "Epoch 971/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 1.4379e-04\n",
      "Epoch 972/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 1.3311e-04\n",
      "Epoch 973/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 9.9891e-05\n",
      "Epoch 974/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 1.2359e-04\n",
      "Epoch 975/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 1.1730e-04\n",
      "Epoch 976/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 9.2965e-05\n",
      "Epoch 977/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 9.7888e-05\n",
      "Epoch 978/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 1.1748e-04\n",
      "Epoch 979/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 8.8604e-05\n",
      "Epoch 980/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 1.1139e-04\n",
      "Epoch 981/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 8.9697e-05\n",
      "Epoch 982/1000\n",
      "246/246 [==============================] - 0s 170us/sample - loss: 8.1267e-05\n",
      "Epoch 983/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 8.0695e-05\n",
      "Epoch 984/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 7.4358e-05\n",
      "Epoch 985/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 7.5661e-05\n",
      "Epoch 986/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 1.0836e-04\n",
      "Epoch 987/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 7.9899e-05\n",
      "Epoch 988/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 7.2066e-05\n",
      "Epoch 989/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 6.6278e-05\n",
      "Epoch 990/1000\n",
      "246/246 [==============================] - 0s 174us/sample - loss: 5.6376e-05\n",
      "Epoch 991/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 6.1927e-05\n",
      "Epoch 992/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 6.5517e-05\n",
      "Epoch 993/1000\n",
      "246/246 [==============================] - 0s 182us/sample - loss: 6.9676e-05\n",
      "Epoch 994/1000\n",
      "246/246 [==============================] - 0s 178us/sample - loss: 4.0143e-05\n",
      "Epoch 995/1000\n",
      "246/246 [==============================] - 0s 166us/sample - loss: 5.0936e-05\n",
      "Epoch 996/1000\n",
      "246/246 [==============================] - 0s 195us/sample - loss: 6.9443e-05\n",
      "Epoch 997/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 6.9584e-05\n",
      "Epoch 998/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 6.3962e-05\n",
      "Epoch 999/1000\n",
      "246/246 [==============================] - 0s 186us/sample - loss: 6.7994e-05\n",
      "Epoch 1000/1000\n",
      "246/246 [==============================] - 0s 191us/sample - loss: 5.4414e-05\n",
      "\n",
      "==========================\n",
      "MAE= 15.617961305560488\n",
      "RMSE =  17.008116661469263\n",
      "NMSE =  0.4566719294525683\n",
      "MAPE= 0.07345964558507231\n",
      "IA= 0.4847566707349964\n",
      "U1= 0.04205709824003896\n",
      "U2= 0.9664550746395542\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 55 seconds\n"
     ]
    }
   ],
   "source": [
    "#LSTM-SED\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import merge\n",
    "from tensorflow.python.keras.layers.merge import Multiply\n",
    "from tensorflow.python.keras.layers.core import *\n",
    "from tensorflow.python.keras.models import *\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python.keras.layers import Input,Dense,Reshape,Dropout, Embedding, LSTM, Bidirectional,Permute\n",
    "from tensorflow.python.keras.layers import RepeatVector, TimeDistributed\n",
    "from tensorflow.python.keras.layers.recurrent import GRU\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import concat, read_csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sklearn.metrics as skm\n",
    "import math\n",
    "from math import sqrt\n",
    "import time\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "#1. load dataset\n",
    "dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_NFP.csv', engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "data = dataset.reshape(-1,6)\n",
    "\n",
    "\n",
    "timestep = 6\n",
    "dim = 6\n",
    "\n",
    "#数据缩放 拆分输入X（7维）&输出Y（1维）\n",
    "X = dataset[:,0:6]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "for i in range(dim):\n",
    "    Xdata = X[:,i]\n",
    "    Xdata = Xdata.reshape(-1,1)\n",
    "    Xdata = scaler.fit_transform(Xdata)\n",
    "    Xdata = Xdata.flatten()\n",
    "    X[:,i] = Xdata\n",
    "X_scaler = X.reshape(324,-1)\n",
    "print(X_scaler.shape)\n",
    "\n",
    "Y = dataset[:,0]\n",
    "Y_scaler= (Y-np.min(Y))/(np.max(Y)-np.min(Y))\n",
    "Y_scaler = Y_scaler.reshape(-1)\n",
    "print(Y_scaler.shape)\n",
    "\n",
    "\n",
    "#重构数据集\n",
    "##timestep为时间步长\n",
    "def create_X(seq, timestep):\n",
    "    dataX = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[i:(i+timestep)]\n",
    "        # X按照顺序取值 每次在后面增加一个数据\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "\n",
    "def create_Y(seq, timestep):\n",
    "    dataY = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        # Y向后移动一位取值\n",
    "        dataY.append(seq[i+timestep])\n",
    "    return np.array(dataY)\n",
    "\n",
    "\n",
    "\n",
    "def get_lstm_model():\n",
    "    K.clear_session() #清除之前的模型，省得压满内存\n",
    "    inputs = Input(shape=(timestep, dim,))\n",
    "    lstm_units1 = 60\n",
    "    lstm_units2 = 12\n",
    "    lstm_out1 = LSTM(lstm_units1, return_sequences=True)(inputs)\n",
    "    lstm_out2 = LSTM(lstm_units2, return_sequences=True)(lstm_out1)\n",
    "    lstm_out = Flatten()(lstm_out2)\n",
    "    output = Dense(1, activation='relu')(lstm_out)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#预测\n",
    "pres=[]\n",
    "# 将数据拆分成训练和测试，7/9作为训练数据\n",
    "train_size = int(len(dataset) * 0.78)\n",
    "test_size = len(dataset) - train_size\n",
    "trainX, testX = X_scaler[0:train_size], X_scaler[train_size:len(dataset)]\n",
    "trainY, testY = Y_scaler[0:train_size], Y_scaler[train_size:len(dataset)]\n",
    "print(\"原始训练集的长度：\",train_size)\n",
    "print(\"原始测试集的长度：\",test_size)\n",
    "#print(trainX,trainX.shape)\n",
    "#print(trainY,trainY.shape)\n",
    "#print(testX,testX.shape)\n",
    "#print(testY,testY.shape)\n",
    "\n",
    "train_X=create_X(trainX,timestep)#(246,6)\n",
    "#print(train_X,train_X.shape)\n",
    "train_Y=create_Y(trainY,timestep)#(246,)\n",
    "#print(train_Y,train_Y.shape)\n",
    "test_X=create_X(testX,timestep)#(66,6,6)\n",
    "#print(test_X,test_X.shape)\n",
    "test_Y=create_Y(testY,timestep)#(66,)\n",
    "#print(test_Y,test_Y.shape)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        \n",
    "    model = get_lstm_model()\n",
    "    optimizer = Adam(0.01)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(train_X, train_Y, epochs=1000, batch_size=64)\n",
    "\n",
    "    \n",
    "    # 开始预测\n",
    "    trainPredict = model.predict(train_X)\n",
    "    testPredict = model.predict(test_X)\n",
    "\n",
    "\n",
    "    # 逆缩放预测值\n",
    "    dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_NFP.csv', engine='python')\n",
    "    dataset = dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "    Y = dataset[:,0]\n",
    "    #trainPredict = trainPredict*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori = trainY*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori=trainY_re.reshape((246,1))\n",
    "    testPre = testPredict*((np.max(Y)-np.min(Y)))+np.min(Y)\n",
    "    testPre = testPre.reshape(-1)\n",
    "    Y = dataset[:,0]\n",
    "    Y = Y[train_size+timestep:len(dataset)]\n",
    "    #testY_ori = testY*((np.max(dataset)-np.min(dataset)))+np.min(dataset)\n",
    "    #testY_ori = testY_ori.reshape((-1,1))\n",
    "    testY_ori = Y.reshape(-1)\n",
    "\n",
    "    \n",
    "\n",
    "    # 计算误差\n",
    "    #trainScore = math.sqrt(mean_squared_error(trainY_ori[:,0], trainPredict[:,0]))\n",
    "    #print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    #testScore = math.sqrt(mean_squared_error(testY_ori[:,0], testPredict[:,0]))\n",
    "    #print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "\n",
    "    error = []#Y-Y'\n",
    "    error1= []#abs((Y-Y')/Y)\n",
    "    error2= []#Y*Y'\n",
    "    squared1 =[]#Y*Y\n",
    "    squared2 =[]#Y'*Y'\n",
    "    for i in range(len(testY_ori)):\n",
    "        error.append(testY_ori[i] - testPre[i])\n",
    "        error1.append(abs((testY_ori[i] - testPre[i])/testY_ori[i]))\n",
    "        error2.append(testY_ori[i]*testPre[i])\n",
    "        squared1.append(testY_ori[i]*testY_ori[i])\n",
    "        squared2.append(testPre[i]*testPre[i])\n",
    "        \n",
    "        \n",
    "    squaredError = []#(Y-Y')^2\n",
    "    absError = []#abs(Y-Y')\n",
    "    for val in error:    \n",
    "        squaredError.append(val * val)#target-prediction之差平方     \n",
    "        absError.append(abs(val))#误差绝对值\n",
    "        \n",
    "    MSE=sum(squaredError) / len(squaredError)\n",
    "    meannn=np.mean(testY_ori)\n",
    "    NMSEerror = []\n",
    "    IAerror = []\n",
    "    for i in range(len(testY_ori)):\n",
    "        NMSEerror.append(squaredError[i]/error2[i])\n",
    "        IAerror.append((abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn))*(abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn)))\n",
    "    \n",
    "    U2error1 = []\n",
    "    U2error2 = []\n",
    "    for i in range(len(testY_ori)-1):\n",
    "        U2error1.append(((testY_ori[i+1] - testPre[i+1])/testY_ori[i])*((testY_ori[i+1] - testPre[i+1])/testY_ori[i]))\n",
    "        U2error2.append(((testY_ori[i+1] - testPre[i])/testY_ori[i])*((testY_ori[i+1] - testPre[i])/testY_ori[i]))\n",
    "    print('\\n==========================')\n",
    "    MAE=sum(absError)/len(absError)\n",
    "    print('MAE=',MAE)\n",
    "    from math import sqrt\n",
    "    RMSE=sqrt(MSE)\n",
    "    print(\"RMSE = \", RMSE)#均方根误差RMSE\n",
    "    NMSE=sum(NMSEerror)\n",
    "    print(\"NMSE = \", NMSE)#误差平方的归一化平均值NMSE\n",
    "    MAPE=sum(error1)/len(error1)\n",
    "    print('MAPE=',MAPE)\n",
    "    IA=1-sum(squaredError)/sum(IAerror)\n",
    "    print('IA=',IA)#一致性指数\n",
    "    U1index=RMSE/(sqrt(sum(squared1)/len(squared1))+sqrt(sum(squared2)/len(squared2)))\n",
    "    print('U1=',U1index)\n",
    "    U2index=(sqrt(sum(U2error1)/len(testY_ori)))/(sqrt(sum(U2error2)/len(testY_ori)))\n",
    "    print('U2=',U2index)\n",
    "\n",
    "    #print(testPredict)\n",
    "    testPre=testPre.reshape(-1)\n",
    "    testPre=pd.DataFrame(testPre)\n",
    "    testPre.to_csv('C:/Users/Administrator/Desktop/XI-2/data/pre_result/lstmoutput.csv', header=False)\n",
    "    import csv\n",
    "    Evaluation_index = [MAE,RMSE,NMSE,MAPE,IA,U1index,U2index]\n",
    "    with open(\"C:/Users/Administrator/Desktop/XI-2/data/pre_result/lstmEvaluation.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file ,delimiter=',')\n",
    "        writer.writerow(Evaluation_index)\n",
    "    \n",
    "    plt.figure(facecolor='white')\n",
    "    plt.plot(testPre,color='green', label='Predict')\n",
    "    plt.plot(testY_ori,color='pink', label='Original')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    pres.append(testPre)\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a995d11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(324, 6)\n",
      "(324,)\n",
      "原始训练集的长度： 252\n",
      "原始测试集的长度： 72\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 6, 6)]            0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 6, 120)            32160     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 6, 12)             6384      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6, 12)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 72)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 73        \n",
      "=================================================================\n",
      "Total params: 38,617\n",
      "Trainable params: 38,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 246 samples\n",
      "Epoch 1/1000\n",
      "246/246 [==============================] - 7s 28ms/sample - loss: 0.1011\n",
      "Epoch 2/1000\n",
      "246/246 [==============================] - 0s 288us/sample - loss: 0.0364\n",
      "Epoch 3/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0158\n",
      "Epoch 4/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 0.0228\n",
      "Epoch 5/1000\n",
      "246/246 [==============================] - 0s 353us/sample - loss: 0.0185\n",
      "Epoch 6/1000\n",
      "246/246 [==============================] - 0s 276us/sample - loss: 0.0161\n",
      "Epoch 7/1000\n",
      "246/246 [==============================] - 0s 292us/sample - loss: 0.0153\n",
      "Epoch 8/1000\n",
      "246/246 [==============================] - 0s 308us/sample - loss: 0.0137\n",
      "Epoch 9/1000\n",
      "246/246 [==============================] - 0s 292us/sample - loss: 0.0125\n",
      "Epoch 10/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 0.0137\n",
      "Epoch 11/1000\n",
      "246/246 [==============================] - 0s 304us/sample - loss: 0.0093\n",
      "Epoch 12/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 0.0112\n",
      "Epoch 13/1000\n",
      "246/246 [==============================] - 0s 381us/sample - loss: 0.0100\n",
      "Epoch 14/1000\n",
      "246/246 [==============================] - 0s 300us/sample - loss: 0.0076\n",
      "Epoch 15/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 0.0089\n",
      "Epoch 16/1000\n",
      "246/246 [==============================] - 0s 308us/sample - loss: 0.0091\n",
      "Epoch 17/1000\n",
      "246/246 [==============================] - 0s 308us/sample - loss: 0.0065\n",
      "Epoch 18/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 0.0076\n",
      "Epoch 19/1000\n",
      "246/246 [==============================] - 0s 320us/sample - loss: 0.0069\n",
      "Epoch 20/1000\n",
      "246/246 [==============================] - 0s 312us/sample - loss: 0.0072\n",
      "Epoch 21/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 0.0052\n",
      "Epoch 22/1000\n",
      "246/246 [==============================] - 0s 328us/sample - loss: 0.0059\n",
      "Epoch 23/1000\n",
      "246/246 [==============================] - 0s 345us/sample - loss: 0.0061\n",
      "Epoch 24/1000\n",
      "246/246 [==============================] - 0s 345us/sample - loss: 0.0054\n",
      "Epoch 25/1000\n",
      "246/246 [==============================] - 0s 328us/sample - loss: 0.0055\n",
      "Epoch 26/1000\n",
      "246/246 [==============================] - 0s 539us/sample - loss: 0.0058\n",
      "Epoch 27/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 0.0060\n",
      "Epoch 28/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0060\n",
      "Epoch 29/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0053\n",
      "Epoch 30/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 0.0066\n",
      "Epoch 31/1000\n",
      "246/246 [==============================] - 0s 263us/sample - loss: 0.0057\n",
      "Epoch 32/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0050\n",
      "Epoch 33/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0044\n",
      "Epoch 34/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0049\n",
      "Epoch 35/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0059\n",
      "Epoch 36/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0045\n",
      "Epoch 37/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0051\n",
      "Epoch 38/1000\n",
      "246/246 [==============================] - 0s 324us/sample - loss: 0.0049\n",
      "Epoch 39/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 0.0058\n",
      "Epoch 40/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0051\n",
      "Epoch 41/1000\n",
      "246/246 [==============================] - 0s 304us/sample - loss: 0.0042\n",
      "Epoch 42/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0050\n",
      "Epoch 43/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0044\n",
      "Epoch 44/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0044\n",
      "Epoch 45/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 0.0041\n",
      "Epoch 46/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0039\n",
      "Epoch 47/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0039\n",
      "Epoch 48/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0042\n",
      "Epoch 49/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0041\n",
      "Epoch 50/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0040\n",
      "Epoch 51/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 0.0040\n",
      "Epoch 52/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0042\n",
      "Epoch 53/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 0.0039\n",
      "Epoch 54/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0038\n",
      "Epoch 55/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0038\n",
      "Epoch 56/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0039\n",
      "Epoch 57/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0038\n",
      "Epoch 58/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0047\n",
      "Epoch 59/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0041\n",
      "Epoch 60/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0037\n",
      "Epoch 61/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0035\n",
      "Epoch 62/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0037\n",
      "Epoch 63/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0036\n",
      "Epoch 64/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0033\n",
      "Epoch 65/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0034\n",
      "Epoch 66/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0040\n",
      "Epoch 67/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0039\n",
      "Epoch 68/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0036\n",
      "Epoch 69/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0032\n",
      "Epoch 70/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0033\n",
      "Epoch 71/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0032\n",
      "Epoch 72/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0038\n",
      "Epoch 73/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0036\n",
      "Epoch 74/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0035\n",
      "Epoch 75/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0035\n",
      "Epoch 76/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 0.0040\n",
      "Epoch 77/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0031\n",
      "Epoch 78/1000\n",
      "246/246 [==============================] - 0s 292us/sample - loss: 0.0036\n",
      "Epoch 79/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0037\n",
      "Epoch 80/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0032\n",
      "Epoch 81/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0037\n",
      "Epoch 82/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0037\n",
      "Epoch 83/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 0.0032\n",
      "Epoch 84/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0030\n",
      "Epoch 85/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0031\n",
      "Epoch 86/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0031\n",
      "Epoch 87/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0030\n",
      "Epoch 88/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0028\n",
      "Epoch 89/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0032\n",
      "Epoch 90/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0031\n",
      "Epoch 91/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0032\n",
      "Epoch 92/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0032\n",
      "Epoch 93/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0028\n",
      "Epoch 94/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0029\n",
      "Epoch 95/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0024\n",
      "Epoch 96/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0027\n",
      "Epoch 97/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0025\n",
      "Epoch 98/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0032\n",
      "Epoch 99/1000\n",
      "246/246 [==============================] - 0s 377us/sample - loss: 0.0031\n",
      "Epoch 100/1000\n",
      "246/246 [==============================] - 0s 324us/sample - loss: 0.0041\n",
      "Epoch 101/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0031\n",
      "Epoch 102/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0032\n",
      "Epoch 103/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0030\n",
      "Epoch 104/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0026\n",
      "Epoch 105/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0026\n",
      "Epoch 106/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0027\n",
      "Epoch 107/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0031\n",
      "Epoch 108/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0026\n",
      "Epoch 109/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0023\n",
      "Epoch 110/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0024\n",
      "Epoch 111/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0023\n",
      "Epoch 112/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0026\n",
      "Epoch 113/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0026\n",
      "Epoch 114/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0023\n",
      "Epoch 115/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0029\n",
      "Epoch 116/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0043\n",
      "Epoch 117/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0045\n",
      "Epoch 118/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0035\n",
      "Epoch 119/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0033\n",
      "Epoch 120/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0035\n",
      "Epoch 121/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0030\n",
      "Epoch 122/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0030\n",
      "Epoch 123/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0029\n",
      "Epoch 124/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0028\n",
      "Epoch 125/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0025\n",
      "Epoch 126/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0027\n",
      "Epoch 127/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0024\n",
      "Epoch 128/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0024\n",
      "Epoch 129/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0027\n",
      "Epoch 130/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0023\n",
      "Epoch 131/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0023\n",
      "Epoch 132/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0025\n",
      "Epoch 133/1000\n",
      "246/246 [==============================] - 0s 288us/sample - loss: 0.0025\n",
      "Epoch 134/1000\n",
      "246/246 [==============================] - 0s 478us/sample - loss: 0.0022\n",
      "Epoch 135/1000\n",
      "246/246 [==============================] - 0s 316us/sample - loss: 0.0022\n",
      "Epoch 136/1000\n",
      "246/246 [==============================] - 0s 434us/sample - loss: 0.0024\n",
      "Epoch 137/1000\n",
      "246/246 [==============================] - 0s 288us/sample - loss: 0.0032\n",
      "Epoch 138/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0027\n",
      "Epoch 139/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 0.0024\n",
      "Epoch 140/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0029\n",
      "Epoch 141/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0028\n",
      "Epoch 142/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0026\n",
      "Epoch 143/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0026\n",
      "Epoch 144/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0025\n",
      "Epoch 145/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0025\n",
      "Epoch 146/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0022\n",
      "Epoch 147/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0024\n",
      "Epoch 148/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 0.0024\n",
      "Epoch 149/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0024\n",
      "Epoch 150/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0021\n",
      "Epoch 151/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0024\n",
      "Epoch 152/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0021\n",
      "Epoch 153/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0021\n",
      "Epoch 154/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 0.0023\n",
      "Epoch 155/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0025\n",
      "Epoch 156/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0021\n",
      "Epoch 157/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0022\n",
      "Epoch 158/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0030\n",
      "Epoch 159/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0031\n",
      "Epoch 160/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0028\n",
      "Epoch 161/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0029\n",
      "Epoch 162/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0028\n",
      "Epoch 163/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0020\n",
      "Epoch 164/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0024\n",
      "Epoch 165/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0024\n",
      "Epoch 166/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0021\n",
      "Epoch 167/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0022\n",
      "Epoch 168/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0017\n",
      "Epoch 169/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0021\n",
      "Epoch 170/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0022\n",
      "Epoch 171/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 0.0020\n",
      "Epoch 172/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0022\n",
      "Epoch 173/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0021\n",
      "Epoch 174/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0020\n",
      "Epoch 175/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0020\n",
      "Epoch 176/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0019\n",
      "Epoch 177/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0026\n",
      "Epoch 178/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 0.0021\n",
      "Epoch 179/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0023\n",
      "Epoch 180/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0024\n",
      "Epoch 181/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 0.0026\n",
      "Epoch 182/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0028\n",
      "Epoch 183/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0020\n",
      "Epoch 184/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0020\n",
      "Epoch 185/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0025\n",
      "Epoch 186/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0021\n",
      "Epoch 187/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 0.0023\n",
      "Epoch 188/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0027\n",
      "Epoch 189/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0022\n",
      "Epoch 190/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0032\n",
      "Epoch 191/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0026\n",
      "Epoch 192/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0023\n",
      "Epoch 193/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0026\n",
      "Epoch 194/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0021\n",
      "Epoch 195/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0024\n",
      "Epoch 196/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0025\n",
      "Epoch 197/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0024\n",
      "Epoch 198/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0020\n",
      "Epoch 199/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0020\n",
      "Epoch 200/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0023\n",
      "Epoch 201/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0019\n",
      "Epoch 202/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0024\n",
      "Epoch 203/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0021\n",
      "Epoch 204/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0024\n",
      "Epoch 205/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0024\n",
      "Epoch 206/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0020\n",
      "Epoch 207/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0021\n",
      "Epoch 208/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0024\n",
      "Epoch 209/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0017\n",
      "Epoch 210/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0021\n",
      "Epoch 211/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0021\n",
      "Epoch 212/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0018\n",
      "Epoch 213/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0021\n",
      "Epoch 214/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0018\n",
      "Epoch 215/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0020\n",
      "Epoch 216/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0021\n",
      "Epoch 217/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0017\n",
      "Epoch 218/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0020\n",
      "Epoch 219/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0018\n",
      "Epoch 220/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0018\n",
      "Epoch 221/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0019\n",
      "Epoch 222/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0018\n",
      "Epoch 223/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0016\n",
      "Epoch 224/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0014\n",
      "Epoch 225/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0017\n",
      "Epoch 226/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0020\n",
      "Epoch 227/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0017\n",
      "Epoch 228/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0015\n",
      "Epoch 229/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0018\n",
      "Epoch 230/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0017\n",
      "Epoch 231/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0015\n",
      "Epoch 232/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0023\n",
      "Epoch 233/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0027\n",
      "Epoch 234/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0022\n",
      "Epoch 235/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0016\n",
      "Epoch 236/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0028\n",
      "Epoch 237/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0022\n",
      "Epoch 238/1000\n",
      "246/246 [==============================] - 0s 316us/sample - loss: 0.0021\n",
      "Epoch 239/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0018\n",
      "Epoch 240/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 0.0017\n",
      "Epoch 241/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0020\n",
      "Epoch 242/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0018\n",
      "Epoch 243/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0019\n",
      "Epoch 244/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0019\n",
      "Epoch 245/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0021\n",
      "Epoch 246/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0017\n",
      "Epoch 247/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0019\n",
      "Epoch 248/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0018\n",
      "Epoch 249/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0021\n",
      "Epoch 250/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0016\n",
      "Epoch 251/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0021\n",
      "Epoch 252/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0021\n",
      "Epoch 253/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0020\n",
      "Epoch 254/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0018\n",
      "Epoch 255/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0020\n",
      "Epoch 256/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0020\n",
      "Epoch 257/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0020\n",
      "Epoch 258/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 0.0015\n",
      "Epoch 259/1000\n",
      "246/246 [==============================] - 0s 337us/sample - loss: 0.0016\n",
      "Epoch 260/1000\n",
      "246/246 [==============================] - 0s 288us/sample - loss: 0.0020\n",
      "Epoch 261/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0015\n",
      "Epoch 262/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0015\n",
      "Epoch 263/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 0.0019\n",
      "Epoch 264/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0019\n",
      "Epoch 265/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0016\n",
      "Epoch 266/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 0.0020\n",
      "Epoch 267/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0018\n",
      "Epoch 268/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0019\n",
      "Epoch 269/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0016\n",
      "Epoch 270/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0017\n",
      "Epoch 271/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0017\n",
      "Epoch 272/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0016\n",
      "Epoch 273/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0016\n",
      "Epoch 274/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0015\n",
      "Epoch 275/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0015\n",
      "Epoch 276/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 0.0014\n",
      "Epoch 277/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0014\n",
      "Epoch 278/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0013\n",
      "Epoch 279/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0015\n",
      "Epoch 280/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0017\n",
      "Epoch 281/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0017\n",
      "Epoch 282/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0020\n",
      "Epoch 283/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0015\n",
      "Epoch 284/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0024\n",
      "Epoch 285/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0023\n",
      "Epoch 286/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0020\n",
      "Epoch 287/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0017\n",
      "Epoch 288/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0018\n",
      "Epoch 289/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0013\n",
      "Epoch 290/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0020\n",
      "Epoch 291/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0016\n",
      "Epoch 292/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0021\n",
      "Epoch 293/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0016\n",
      "Epoch 294/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0021\n",
      "Epoch 295/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0022\n",
      "Epoch 296/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0024\n",
      "Epoch 297/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0020\n",
      "Epoch 298/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0019\n",
      "Epoch 299/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0018\n",
      "Epoch 300/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0018\n",
      "Epoch 301/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0018\n",
      "Epoch 302/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0023\n",
      "Epoch 303/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0018\n",
      "Epoch 304/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0019\n",
      "Epoch 305/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0018\n",
      "Epoch 306/1000\n",
      "246/246 [==============================] - 0s 345us/sample - loss: 0.0017\n",
      "Epoch 307/1000\n",
      "246/246 [==============================] - 0s 491us/sample - loss: 0.0018\n",
      "Epoch 308/1000\n",
      "246/246 [==============================] - 0s 349us/sample - loss: 0.0017\n",
      "Epoch 309/1000\n",
      "246/246 [==============================] - 0s 365us/sample - loss: 0.0018\n",
      "Epoch 310/1000\n",
      "246/246 [==============================] - 0s 377us/sample - loss: 0.0018\n",
      "Epoch 311/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 0.0016\n",
      "Epoch 312/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0016\n",
      "Epoch 313/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0017\n",
      "Epoch 314/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0017\n",
      "Epoch 315/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0017\n",
      "Epoch 316/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0015\n",
      "Epoch 317/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0019\n",
      "Epoch 318/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0018\n",
      "Epoch 319/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0016\n",
      "Epoch 320/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0014\n",
      "Epoch 321/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0014\n",
      "Epoch 322/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 0.0017\n",
      "Epoch 323/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0012\n",
      "Epoch 324/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0017\n",
      "Epoch 325/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0016\n",
      "Epoch 326/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0015\n",
      "Epoch 327/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0013\n",
      "Epoch 328/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0016\n",
      "Epoch 329/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0018\n",
      "Epoch 330/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0016\n",
      "Epoch 331/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 0.0016\n",
      "Epoch 332/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0022\n",
      "Epoch 333/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0021\n",
      "Epoch 334/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0019\n",
      "Epoch 335/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0020\n",
      "Epoch 336/1000\n",
      "246/246 [==============================] - 0s 199us/sample - loss: 0.0016\n",
      "Epoch 337/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0019\n",
      "Epoch 338/1000\n",
      "246/246 [==============================] - 0s 203us/sample - loss: 0.0016\n",
      "Epoch 339/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0015\n",
      "Epoch 340/1000\n",
      "246/246 [==============================] - 0s 207us/sample - loss: 0.0018\n",
      "Epoch 341/1000\n",
      "246/246 [==============================] - 0s 345us/sample - loss: 0.0014\n",
      "Epoch 342/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 0.0015\n",
      "Epoch 343/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0017\n",
      "Epoch 344/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0018\n",
      "Epoch 345/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 0.0015\n",
      "Epoch 346/1000\n",
      "246/246 [==============================] - 0s 337us/sample - loss: 0.0014\n",
      "Epoch 347/1000\n",
      "246/246 [==============================] - 0s 369us/sample - loss: 0.0015\n",
      "Epoch 348/1000\n",
      "246/246 [==============================] - 0s 434us/sample - loss: 0.0015\n",
      "Epoch 349/1000\n",
      "246/246 [==============================] - 0s 316us/sample - loss: 0.0013\n",
      "Epoch 350/1000\n",
      "246/246 [==============================] - 0s 296us/sample - loss: 0.0016\n",
      "Epoch 351/1000\n",
      "246/246 [==============================] - 0s 296us/sample - loss: 0.0012\n",
      "Epoch 352/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0016\n",
      "Epoch 353/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0016\n",
      "Epoch 354/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0017\n",
      "Epoch 355/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0016\n",
      "Epoch 356/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0014\n",
      "Epoch 357/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0015\n",
      "Epoch 358/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0018\n",
      "Epoch 359/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0015\n",
      "Epoch 360/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0015\n",
      "Epoch 361/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0015\n",
      "Epoch 362/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 0.0020\n",
      "Epoch 363/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 0.0021\n",
      "Epoch 364/1000\n",
      "246/246 [==============================] - 0s 401us/sample - loss: 0.0016\n",
      "Epoch 365/1000\n",
      "246/246 [==============================] - 0s 373us/sample - loss: 0.0017\n",
      "Epoch 366/1000\n",
      "246/246 [==============================] - 0s 628us/sample - loss: 0.0014\n",
      "Epoch 367/1000\n",
      "246/246 [==============================] - 0s 628us/sample - loss: 0.0014\n",
      "Epoch 368/1000\n",
      "246/246 [==============================] - 0s 701us/sample - loss: 0.0014\n",
      "Epoch 369/1000\n",
      "246/246 [==============================] - 0s 507us/sample - loss: 0.0016\n",
      "Epoch 370/1000\n",
      "246/246 [==============================] - 0s 454us/sample - loss: 0.0015\n",
      "Epoch 371/1000\n",
      "246/246 [==============================] - 0s 503us/sample - loss: 0.0014\n",
      "Epoch 372/1000\n",
      "246/246 [==============================] - 0s 320us/sample - loss: 0.0014\n",
      "Epoch 373/1000\n",
      "246/246 [==============================] - 0s 332us/sample - loss: 9.9180e-04\n",
      "Epoch 374/1000\n",
      "246/246 [==============================] - 0s 304us/sample - loss: 0.0015\n",
      "Epoch 375/1000\n",
      "246/246 [==============================] - 0s 300us/sample - loss: 0.0014\n",
      "Epoch 376/1000\n",
      "246/246 [==============================] - 0s 357us/sample - loss: 0.0013\n",
      "Epoch 377/1000\n",
      "246/246 [==============================] - 0s 381us/sample - loss: 0.0012\n",
      "Epoch 378/1000\n",
      "246/246 [==============================] - 0s 328us/sample - loss: 0.0016\n",
      "Epoch 379/1000\n",
      "246/246 [==============================] - 0s 401us/sample - loss: 0.0014\n",
      "Epoch 380/1000\n",
      "246/246 [==============================] - 0s 365us/sample - loss: 0.0014\n",
      "Epoch 381/1000\n",
      "246/246 [==============================] - 0s 389us/sample - loss: 0.0016\n",
      "Epoch 382/1000\n",
      "246/246 [==============================] - 0s 414us/sample - loss: 0.0016\n",
      "Epoch 383/1000\n",
      "246/246 [==============================] - 0s 328us/sample - loss: 0.0014\n",
      "Epoch 384/1000\n",
      "246/246 [==============================] - 0s 345us/sample - loss: 0.0017\n",
      "Epoch 385/1000\n",
      "246/246 [==============================] - 0s 300us/sample - loss: 0.0015\n",
      "Epoch 386/1000\n",
      "246/246 [==============================] - 0s 304us/sample - loss: 0.0017\n",
      "Epoch 387/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 0.0014\n",
      "Epoch 388/1000\n",
      "246/246 [==============================] - 0s 308us/sample - loss: 0.0016\n",
      "Epoch 389/1000\n",
      "246/246 [==============================] - 0s 341us/sample - loss: 0.0014\n",
      "Epoch 390/1000\n",
      "246/246 [==============================] - 0s 300us/sample - loss: 0.0014\n",
      "Epoch 391/1000\n",
      "246/246 [==============================] - 0s 276us/sample - loss: 0.0017\n",
      "Epoch 392/1000\n",
      "246/246 [==============================] - 0s 308us/sample - loss: 0.0015\n",
      "Epoch 393/1000\n",
      "246/246 [==============================] - 0s 507us/sample - loss: 0.0015\n",
      "Epoch 394/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 0.0017\n",
      "Epoch 395/1000\n",
      "246/246 [==============================] - 0s 276us/sample - loss: 0.0015\n",
      "Epoch 396/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 0.0017\n",
      "Epoch 397/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 0.0017\n",
      "Epoch 398/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 0.0014\n",
      "Epoch 399/1000\n",
      "246/246 [==============================] - 0s 320us/sample - loss: 0.0017\n",
      "Epoch 400/1000\n",
      "246/246 [==============================] - 0s 304us/sample - loss: 0.0014\n",
      "Epoch 401/1000\n",
      "246/246 [==============================] - 0s 276us/sample - loss: 0.0011\n",
      "Epoch 402/1000\n",
      "246/246 [==============================] - 0s 324us/sample - loss: 0.0016\n",
      "Epoch 403/1000\n",
      "246/246 [==============================] - 0s 296us/sample - loss: 0.0017\n",
      "Epoch 404/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 0.0014\n",
      "Epoch 405/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 0.0014\n",
      "Epoch 406/1000\n",
      "246/246 [==============================] - 0s 312us/sample - loss: 0.0014\n",
      "Epoch 407/1000\n",
      "246/246 [==============================] - 0s 288us/sample - loss: 0.0015\n",
      "Epoch 408/1000\n",
      "246/246 [==============================] - 0s 304us/sample - loss: 0.0015\n",
      "Epoch 409/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 0.0016\n",
      "Epoch 410/1000\n",
      "246/246 [==============================] - 0s 304us/sample - loss: 0.0014\n",
      "Epoch 411/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 0.0015\n",
      "Epoch 412/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 0.0014\n",
      "Epoch 413/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 0.0014\n",
      "Epoch 414/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0016\n",
      "Epoch 415/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0015\n",
      "Epoch 416/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 0.0015\n",
      "Epoch 417/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0016\n",
      "Epoch 418/1000\n",
      "246/246 [==============================] - 0s 389us/sample - loss: 0.0018\n",
      "Epoch 419/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 0.0016\n",
      "Epoch 420/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0018\n",
      "Epoch 421/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 0.0016\n",
      "Epoch 422/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0014\n",
      "Epoch 423/1000\n",
      "246/246 [==============================] - 0s 300us/sample - loss: 0.0013\n",
      "Epoch 424/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 0.0012\n",
      "Epoch 425/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 0.0016\n",
      "Epoch 426/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0012\n",
      "Epoch 427/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 0.0014\n",
      "Epoch 428/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0014\n",
      "Epoch 429/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0016\n",
      "Epoch 430/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 0.0013\n",
      "Epoch 431/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 0.0013\n",
      "Epoch 432/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0014\n",
      "Epoch 433/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 0.0011\n",
      "Epoch 434/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 0.0015\n",
      "Epoch 435/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0016\n",
      "Epoch 436/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0013\n",
      "Epoch 437/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0016\n",
      "Epoch 438/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0013\n",
      "Epoch 439/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0016\n",
      "Epoch 440/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0016\n",
      "Epoch 441/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0014\n",
      "Epoch 442/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 0.0016\n",
      "Epoch 443/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 0.0014\n",
      "Epoch 444/1000\n",
      "246/246 [==============================] - 0s 316us/sample - loss: 0.0015\n",
      "Epoch 445/1000\n",
      "246/246 [==============================] - 0s 292us/sample - loss: 0.0015\n",
      "Epoch 446/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0014\n",
      "Epoch 447/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0014\n",
      "Epoch 448/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 280us/sample - loss: 0.0015\n",
      "Epoch 449/1000\n",
      "246/246 [==============================] - 0s 418us/sample - loss: 0.0013\n",
      "Epoch 450/1000\n",
      "246/246 [==============================] - 0s 296us/sample - loss: 0.0013\n",
      "Epoch 451/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 0.0013\n",
      "Epoch 452/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 0.0015\n",
      "Epoch 453/1000\n",
      "246/246 [==============================] - 0s 369us/sample - loss: 0.0012\n",
      "Epoch 454/1000\n",
      "246/246 [==============================] - 0s 300us/sample - loss: 0.0013\n",
      "Epoch 455/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0014\n",
      "Epoch 456/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0014\n",
      "Epoch 457/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0014\n",
      "Epoch 458/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 0.0014\n",
      "Epoch 459/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0013\n",
      "Epoch 460/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0013\n",
      "Epoch 461/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0014\n",
      "Epoch 462/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0015\n",
      "Epoch 463/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0013\n",
      "Epoch 464/1000\n",
      "246/246 [==============================] - 0s 357us/sample - loss: 0.0013\n",
      "Epoch 465/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0012\n",
      "Epoch 466/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 0.0012\n",
      "Epoch 467/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0015\n",
      "Epoch 468/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0013\n",
      "Epoch 469/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0011\n",
      "Epoch 470/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0013\n",
      "Epoch 471/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0014\n",
      "Epoch 472/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0014\n",
      "Epoch 473/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0016\n",
      "Epoch 474/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0019\n",
      "Epoch 475/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0021\n",
      "Epoch 476/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0017\n",
      "Epoch 477/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0019\n",
      "Epoch 478/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0019\n",
      "Epoch 479/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0015\n",
      "Epoch 480/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0016\n",
      "Epoch 481/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0018\n",
      "Epoch 482/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0017\n",
      "Epoch 483/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0015\n",
      "Epoch 484/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0014\n",
      "Epoch 485/1000\n",
      "246/246 [==============================] - 0s 482us/sample - loss: 0.0012\n",
      "Epoch 486/1000\n",
      "246/246 [==============================] - 0s 320us/sample - loss: 0.0011\n",
      "Epoch 487/1000\n",
      "246/246 [==============================] - 0s 349us/sample - loss: 0.0015\n",
      "Epoch 488/1000\n",
      "246/246 [==============================] - 0s 336us/sample - loss: 0.0013\n",
      "Epoch 489/1000\n",
      "246/246 [==============================] - 0s 474us/sample - loss: 0.0017\n",
      "Epoch 490/1000\n",
      "246/246 [==============================] - 0s 316us/sample - loss: 0.0014\n",
      "Epoch 491/1000\n",
      "246/246 [==============================] - 0s 276us/sample - loss: 0.0014\n",
      "Epoch 492/1000\n",
      "246/246 [==============================] - 0s 430us/sample - loss: 0.0016\n",
      "Epoch 493/1000\n",
      "246/246 [==============================] - 0s 385us/sample - loss: 0.0014\n",
      "Epoch 494/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 0.0012\n",
      "Epoch 495/1000\n",
      "246/246 [==============================] - 0s 401us/sample - loss: 0.0013\n",
      "Epoch 496/1000\n",
      "246/246 [==============================] - 0s 389us/sample - loss: 0.0011\n",
      "Epoch 497/1000\n",
      "246/246 [==============================] - 0s 328us/sample - loss: 0.0014\n",
      "Epoch 498/1000\n",
      "246/246 [==============================] - 0s 365us/sample - loss: 0.0013\n",
      "Epoch 499/1000\n",
      "246/246 [==============================] - 0s 337us/sample - loss: 0.0013\n",
      "Epoch 500/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 0.0013\n",
      "Epoch 501/1000\n",
      "246/246 [==============================] - 0s 292us/sample - loss: 0.0015\n",
      "Epoch 502/1000\n",
      "246/246 [==============================] - 0s 300us/sample - loss: 0.0011\n",
      "Epoch 503/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0012\n",
      "Epoch 504/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0011\n",
      "Epoch 505/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0014\n",
      "Epoch 506/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0012\n",
      "Epoch 507/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0014\n",
      "Epoch 508/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0014\n",
      "Epoch 509/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0015\n",
      "Epoch 510/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0012\n",
      "Epoch 511/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0013\n",
      "Epoch 512/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0012\n",
      "Epoch 513/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0011\n",
      "Epoch 514/1000\n",
      "246/246 [==============================] - 0s 288us/sample - loss: 0.0013\n",
      "Epoch 515/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0014\n",
      "Epoch 516/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0017\n",
      "Epoch 517/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 0.0015\n",
      "Epoch 518/1000\n",
      "246/246 [==============================] - 0s 523us/sample - loss: 0.0012\n",
      "Epoch 519/1000\n",
      "246/246 [==============================] - 0s 357us/sample - loss: 0.0016\n",
      "Epoch 520/1000\n",
      "246/246 [==============================] - 0s 300us/sample - loss: 0.0014\n",
      "Epoch 521/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 0.0010\n",
      "Epoch 522/1000\n",
      "246/246 [==============================] - 0s 454us/sample - loss: 0.0012\n",
      "Epoch 523/1000\n",
      "246/246 [==============================] - 0s 308us/sample - loss: 0.0014\n",
      "Epoch 524/1000\n",
      "246/246 [==============================] - 0s 292us/sample - loss: 0.0012\n",
      "Epoch 525/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 0.0013\n",
      "Epoch 526/1000\n",
      "246/246 [==============================] - 0s 341us/sample - loss: 0.0014\n",
      "Epoch 527/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 0.0015\n",
      "Epoch 528/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0012\n",
      "Epoch 529/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0015\n",
      "Epoch 530/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0012\n",
      "Epoch 531/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 532/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0011\n",
      "Epoch 533/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0010\n",
      "Epoch 534/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0014\n",
      "Epoch 535/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 9.2858e-04\n",
      "Epoch 536/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0011\n",
      "Epoch 537/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0011\n",
      "Epoch 538/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0011\n",
      "Epoch 539/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 9.7122e-04\n",
      "Epoch 540/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 284us/sample - loss: 0.0011\n",
      "Epoch 541/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0011\n",
      "Epoch 542/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 0.0011\n",
      "Epoch 543/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0014\n",
      "Epoch 544/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0011\n",
      "Epoch 545/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0012\n",
      "Epoch 546/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0013\n",
      "Epoch 547/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0013\n",
      "Epoch 548/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0013\n",
      "Epoch 549/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0014\n",
      "Epoch 550/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0013\n",
      "Epoch 551/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0016\n",
      "Epoch 552/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0015\n",
      "Epoch 553/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0014\n",
      "Epoch 554/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0015\n",
      "Epoch 555/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 556/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0010\n",
      "Epoch 557/1000\n",
      "246/246 [==============================] - 0s 296us/sample - loss: 0.0013\n",
      "Epoch 558/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0014\n",
      "Epoch 559/1000\n",
      "246/246 [==============================] - 0s 300us/sample - loss: 0.0010\n",
      "Epoch 560/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0013\n",
      "Epoch 561/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0014\n",
      "Epoch 562/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0012\n",
      "Epoch 563/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0011\n",
      "Epoch 564/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0013\n",
      "Epoch 565/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0014\n",
      "Epoch 566/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0013\n",
      "Epoch 567/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0013\n",
      "Epoch 568/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0013\n",
      "Epoch 569/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 570/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0014\n",
      "Epoch 571/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0015\n",
      "Epoch 572/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0014\n",
      "Epoch 573/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0016\n",
      "Epoch 574/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 575/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0012\n",
      "Epoch 576/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 577/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0013\n",
      "Epoch 578/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 579/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0013\n",
      "Epoch 580/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0014\n",
      "Epoch 581/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0014\n",
      "Epoch 582/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0012\n",
      "Epoch 583/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 584/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0012\n",
      "Epoch 585/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0015\n",
      "Epoch 586/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0015\n",
      "Epoch 587/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0013\n",
      "Epoch 588/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0012\n",
      "Epoch 589/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0013\n",
      "Epoch 590/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 8.9531e-04\n",
      "Epoch 591/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0012\n",
      "Epoch 592/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 593/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0012\n",
      "Epoch 594/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 595/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 596/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0014\n",
      "Epoch 597/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0013\n",
      "Epoch 598/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0015\n",
      "Epoch 599/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0014\n",
      "Epoch 600/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0013\n",
      "Epoch 601/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0014\n",
      "Epoch 602/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0013\n",
      "Epoch 603/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0016\n",
      "Epoch 604/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0015\n",
      "Epoch 605/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0013\n",
      "Epoch 606/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 607/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0013\n",
      "Epoch 608/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0014\n",
      "Epoch 609/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0014\n",
      "Epoch 610/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0012\n",
      "Epoch 611/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0016\n",
      "Epoch 612/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0014\n",
      "Epoch 613/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0016\n",
      "Epoch 614/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0015\n",
      "Epoch 615/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0017\n",
      "Epoch 616/1000\n",
      "246/246 [==============================] - 0s 531us/sample - loss: 0.0013\n",
      "Epoch 617/1000\n",
      "246/246 [==============================] - 0s 474us/sample - loss: 0.0013\n",
      "Epoch 618/1000\n",
      "246/246 [==============================] - 0s 438us/sample - loss: 0.0013\n",
      "Epoch 619/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0015\n",
      "Epoch 620/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 621/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0011\n",
      "Epoch 622/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0014\n",
      "Epoch 623/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0014\n",
      "Epoch 624/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0014\n",
      "Epoch 625/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0014\n",
      "Epoch 626/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0013\n",
      "Epoch 627/1000\n",
      "246/246 [==============================] - 0s 296us/sample - loss: 0.0011\n",
      "Epoch 628/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0014\n",
      "Epoch 629/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0012\n",
      "Epoch 630/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0012\n",
      "Epoch 631/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0012\n",
      "Epoch 632/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0014\n",
      "Epoch 633/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0014\n",
      "Epoch 634/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0011\n",
      "Epoch 635/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0010\n",
      "Epoch 636/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0010\n",
      "Epoch 637/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 638/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 8.5995e-04\n",
      "Epoch 639/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 0.0012\n",
      "Epoch 640/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0012\n",
      "Epoch 641/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 642/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0014\n",
      "Epoch 643/1000\n",
      "246/246 [==============================] - 0s 345us/sample - loss: 0.0014\n",
      "Epoch 644/1000\n",
      "246/246 [==============================] - 0s 353us/sample - loss: 0.0013\n",
      "Epoch 645/1000\n",
      "246/246 [==============================] - 0s 288us/sample - loss: 0.0011\n",
      "Epoch 646/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0014\n",
      "Epoch 647/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0011\n",
      "Epoch 648/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0012\n",
      "Epoch 649/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0012\n",
      "Epoch 650/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 0.0013\n",
      "Epoch 651/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0012\n",
      "Epoch 652/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0012\n",
      "Epoch 653/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0011\n",
      "Epoch 654/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0011\n",
      "Epoch 655/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0013\n",
      "Epoch 656/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0014\n",
      "Epoch 657/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0015\n",
      "Epoch 658/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0015\n",
      "Epoch 659/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0010\n",
      "Epoch 660/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 661/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0012\n",
      "Epoch 662/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0016\n",
      "Epoch 663/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0011\n",
      "Epoch 664/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0012\n",
      "Epoch 665/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0012\n",
      "Epoch 666/1000\n",
      "246/246 [==============================] - 0s 312us/sample - loss: 0.0012\n",
      "Epoch 667/1000\n",
      "246/246 [==============================] - 0s 405us/sample - loss: 0.0011\n",
      "Epoch 668/1000\n",
      "246/246 [==============================] - 0s 300us/sample - loss: 0.0014\n",
      "Epoch 669/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0011\n",
      "Epoch 670/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0012\n",
      "Epoch 671/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0014\n",
      "Epoch 672/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0012\n",
      "Epoch 673/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 674/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0018\n",
      "Epoch 675/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0016\n",
      "Epoch 676/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0016\n",
      "Epoch 677/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0015\n",
      "Epoch 678/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0019\n",
      "Epoch 679/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0015\n",
      "Epoch 680/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0014\n",
      "Epoch 681/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0015\n",
      "Epoch 682/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0010\n",
      "Epoch 683/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 0.0012\n",
      "Epoch 684/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0011\n",
      "Epoch 685/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0011\n",
      "Epoch 686/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0014\n",
      "Epoch 687/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0017\n",
      "Epoch 688/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0012\n",
      "Epoch 689/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0013\n",
      "Epoch 690/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0012\n",
      "Epoch 691/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0010\n",
      "Epoch 692/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 693/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0010\n",
      "Epoch 694/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 695/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0010\n",
      "Epoch 696/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0010\n",
      "Epoch 697/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0013\n",
      "Epoch 698/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 699/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 9.8173e-04\n",
      "Epoch 700/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 8.4343e-04\n",
      "Epoch 701/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0011\n",
      "Epoch 702/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0011\n",
      "Epoch 703/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0012\n",
      "Epoch 704/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0013\n",
      "Epoch 705/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 9.9844e-04\n",
      "Epoch 706/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0012\n",
      "Epoch 707/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 9.8106e-04\n",
      "Epoch 708/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0010\n",
      "Epoch 709/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 710/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0014\n",
      "Epoch 711/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 0.0011\n",
      "Epoch 712/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0013\n",
      "Epoch 713/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0014\n",
      "Epoch 714/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0013\n",
      "Epoch 715/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0013\n",
      "Epoch 716/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0013\n",
      "Epoch 717/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0011\n",
      "Epoch 718/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0010\n",
      "Epoch 719/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0013\n",
      "Epoch 720/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0011\n",
      "Epoch 721/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0011\n",
      "Epoch 722/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0011\n",
      "Epoch 723/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 724/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0010\n",
      "Epoch 725/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0012\n",
      "Epoch 726/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0014\n",
      "Epoch 727/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 728/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0013\n",
      "Epoch 729/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0014\n",
      "Epoch 730/1000\n",
      "246/246 [==============================] - 0s 328us/sample - loss: 0.0011\n",
      "Epoch 731/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 7.5600e-04\n",
      "Epoch 732/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 733/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 734/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0012\n",
      "Epoch 735/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 736/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0010\n",
      "Epoch 737/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0012\n",
      "Epoch 738/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0011\n",
      "Epoch 739/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0013\n",
      "Epoch 740/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0012\n",
      "Epoch 741/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 742/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 743/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 744/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0015\n",
      "Epoch 745/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 746/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 747/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0010\n",
      "Epoch 748/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0013\n",
      "Epoch 749/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 9.9436e-04\n",
      "Epoch 750/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 9.5806e-04\n",
      "Epoch 751/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0014\n",
      "Epoch 752/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0010\n",
      "Epoch 753/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0010\n",
      "Epoch 754/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0013\n",
      "Epoch 755/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0014\n",
      "Epoch 756/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0014\n",
      "Epoch 757/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0013\n",
      "Epoch 758/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0012\n",
      "Epoch 759/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0013\n",
      "Epoch 760/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0012\n",
      "Epoch 761/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0013\n",
      "Epoch 762/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0012\n",
      "Epoch 763/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 9.9663e-04\n",
      "Epoch 764/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 765/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0013\n",
      "Epoch 766/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0011\n",
      "Epoch 767/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0010\n",
      "Epoch 768/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0013\n",
      "Epoch 769/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 770/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0013\n",
      "Epoch 771/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0014\n",
      "Epoch 772/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 9.4460e-04\n",
      "Epoch 773/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0011\n",
      "Epoch 774/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0010\n",
      "Epoch 775/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0014\n",
      "Epoch 776/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 9.7974e-04\n",
      "Epoch 777/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0014\n",
      "Epoch 778/1000\n",
      "246/246 [==============================] - 0s 288us/sample - loss: 0.0014\n",
      "Epoch 779/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 780/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0010\n",
      "Epoch 781/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0010\n",
      "Epoch 782/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0010\n",
      "Epoch 783/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0012\n",
      "Epoch 784/1000\n",
      "246/246 [==============================] - 0s 316us/sample - loss: 0.0013\n",
      "Epoch 785/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 0.0012\n",
      "Epoch 786/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 787/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0011\n",
      "Epoch 788/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 789/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 790/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0012\n",
      "Epoch 791/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0011\n",
      "Epoch 792/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0011\n",
      "Epoch 793/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0013\n",
      "Epoch 794/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0011\n",
      "Epoch 795/1000\n",
      "246/246 [==============================] - 0s 328us/sample - loss: 0.0013\n",
      "Epoch 796/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 797/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0014\n",
      "Epoch 798/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0012\n",
      "Epoch 799/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0015\n",
      "Epoch 800/1000\n",
      "246/246 [==============================] - 0s 320us/sample - loss: 0.0013\n",
      "Epoch 801/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0011\n",
      "Epoch 802/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0016\n",
      "Epoch 803/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0012\n",
      "Epoch 804/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 0.0012\n",
      "Epoch 805/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 0.0011\n",
      "Epoch 806/1000\n",
      "246/246 [==============================] - 0s 292us/sample - loss: 0.0011\n",
      "Epoch 807/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0012\n",
      "Epoch 808/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 809/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0011\n",
      "Epoch 810/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 811/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0013\n",
      "Epoch 812/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 9.8426e-04\n",
      "Epoch 813/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 0.0011\n",
      "Epoch 814/1000\n",
      "246/246 [==============================] - 0s 296us/sample - loss: 0.0013\n",
      "Epoch 815/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0012\n",
      "Epoch 816/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0013\n",
      "Epoch 817/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 818/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0010\n",
      "Epoch 819/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0013\n",
      "Epoch 820/1000\n",
      "246/246 [==============================] - 0s 312us/sample - loss: 0.0011\n",
      "Epoch 821/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 9.9716e-04\n",
      "Epoch 822/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0011\n",
      "Epoch 823/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 824/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 825/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 9.5659e-04\n",
      "Epoch 826/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0012\n",
      "Epoch 827/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0012\n",
      "Epoch 828/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 0.0011\n",
      "Epoch 829/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 0.0013\n",
      "Epoch 830/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 831/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 832/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 8.4281e-04\n",
      "Epoch 833/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0010\n",
      "Epoch 834/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 9.5145e-04\n",
      "Epoch 835/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0012\n",
      "Epoch 836/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0014\n",
      "Epoch 837/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0012\n",
      "Epoch 838/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0013\n",
      "Epoch 839/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 840/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0012\n",
      "Epoch 841/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0010\n",
      "Epoch 842/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 843/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0011\n",
      "Epoch 844/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0013\n",
      "Epoch 845/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0013\n",
      "Epoch 846/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 847/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 848/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0011\n",
      "Epoch 849/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0014\n",
      "Epoch 850/1000\n",
      "246/246 [==============================] - 0s 357us/sample - loss: 0.0014\n",
      "Epoch 851/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0013\n",
      "Epoch 852/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 853/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 854/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0011\n",
      "Epoch 855/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0013\n",
      "Epoch 856/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0013\n",
      "Epoch 857/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0012\n",
      "Epoch 858/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0010\n",
      "Epoch 859/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0010\n",
      "Epoch 860/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0013\n",
      "Epoch 861/1000\n",
      "246/246 [==============================] - 0s 211us/sample - loss: 9.8979e-04\n",
      "Epoch 862/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0010\n",
      "Epoch 863/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0011\n",
      "Epoch 864/1000\n",
      "246/246 [==============================] - 0s 377us/sample - loss: 0.0011\n",
      "Epoch 865/1000\n",
      "246/246 [==============================] - 0s 296us/sample - loss: 0.0013\n",
      "Epoch 866/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 0.0013\n",
      "Epoch 867/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0016\n",
      "Epoch 868/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0010\n",
      "Epoch 869/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 0.0011\n",
      "Epoch 870/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 9.1344e-04\n",
      "Epoch 871/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 9.9739e-04\n",
      "Epoch 872/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 9.6819e-04\n",
      "Epoch 873/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0012\n",
      "Epoch 874/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0011\n",
      "Epoch 875/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 0.0011\n",
      "Epoch 876/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0011\n",
      "Epoch 877/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 878/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0010\n",
      "Epoch 879/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 0.0012\n",
      "Epoch 880/1000\n",
      "246/246 [==============================] - 0s 300us/sample - loss: 0.0012\n",
      "Epoch 881/1000\n",
      "246/246 [==============================] - 0s 235us/sample - loss: 9.2248e-04\n",
      "Epoch 882/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 9.5651e-04\n",
      "Epoch 883/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 9.0458e-04\n",
      "Epoch 884/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0010\n",
      "Epoch 885/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0012\n",
      "Epoch 886/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 887/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0011\n",
      "Epoch 888/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0012\n",
      "Epoch 889/1000\n",
      "246/246 [==============================] - 0s 215us/sample - loss: 0.0011\n",
      "Epoch 890/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0012\n",
      "Epoch 891/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0013\n",
      "Epoch 892/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 893/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0011\n",
      "Epoch 894/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0010\n",
      "Epoch 895/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0011\n",
      "Epoch 896/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 0.0010\n",
      "Epoch 897/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 898/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0013\n",
      "Epoch 899/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0011\n",
      "Epoch 900/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0013\n",
      "Epoch 901/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0011\n",
      "Epoch 902/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0011\n",
      "Epoch 903/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 9.8938e-04\n",
      "Epoch 904/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0012\n",
      "Epoch 905/1000\n",
      "246/246 [==============================] - 0s 219us/sample - loss: 0.0012\n",
      "Epoch 906/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0012\n",
      "Epoch 907/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 908/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 909/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0012\n",
      "Epoch 910/1000\n",
      "246/246 [==============================] - 0s 223us/sample - loss: 0.0011\n",
      "Epoch 911/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 0.0015\n",
      "Epoch 912/1000\n",
      "246/246 [==============================] - 0s 341us/sample - loss: 0.0013\n",
      "Epoch 913/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0015\n",
      "Epoch 914/1000\n",
      "246/246 [==============================] - 0s 227us/sample - loss: 0.0014\n",
      "Epoch 915/1000\n",
      "246/246 [==============================] - 0s 231us/sample - loss: 0.0015\n",
      "Epoch 916/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0013\n",
      "Epoch 917/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 0.0014\n",
      "Epoch 918/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0010\n",
      "Epoch 919/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0012\n",
      "Epoch 920/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0011\n",
      "Epoch 921/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 0.0014\n",
      "Epoch 922/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 0.0013\n",
      "Epoch 923/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 0.0015\n",
      "Epoch 924/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 0.0011\n",
      "Epoch 925/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 0.0014\n",
      "Epoch 926/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0015\n",
      "Epoch 927/1000\n",
      "246/246 [==============================] - 0s 365us/sample - loss: 0.0012\n",
      "Epoch 928/1000\n",
      "246/246 [==============================] - 0s 426us/sample - loss: 0.0014\n",
      "Epoch 929/1000\n",
      "246/246 [==============================] - 0s 304us/sample - loss: 0.0013\n",
      "Epoch 930/1000\n",
      "246/246 [==============================] - 0s 288us/sample - loss: 0.0012\n",
      "Epoch 931/1000\n",
      "246/246 [==============================] - 0s 345us/sample - loss: 0.0012\n",
      "Epoch 932/1000\n",
      "246/246 [==============================] - 0s 462us/sample - loss: 9.9338e-04\n",
      "Epoch 933/1000\n",
      "246/246 [==============================] - 0s 462us/sample - loss: 0.0011\n",
      "Epoch 934/1000\n",
      "246/246 [==============================] - 0s 466us/sample - loss: 0.0014s - loss: 0.001\n",
      "Epoch 935/1000\n",
      "246/246 [==============================] - 0s 341us/sample - loss: 0.0015\n",
      "Epoch 936/1000\n",
      "246/246 [==============================] - 0s 349us/sample - loss: 9.9641e-04\n",
      "Epoch 937/1000\n",
      "246/246 [==============================] - 0s 316us/sample - loss: 0.0010\n",
      "Epoch 938/1000\n",
      "246/246 [==============================] - 0s 316us/sample - loss: 0.0011\n",
      "Epoch 939/1000\n",
      "246/246 [==============================] - 0s 292us/sample - loss: 0.0012\n",
      "Epoch 940/1000\n",
      "246/246 [==============================] - 0s 308us/sample - loss: 9.3013e-04\n",
      "Epoch 941/1000\n",
      "246/246 [==============================] - 0s 304us/sample - loss: 0.0011\n",
      "Epoch 942/1000\n",
      "246/246 [==============================] - 0s 304us/sample - loss: 9.3463e-04\n",
      "Epoch 943/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 9.3799e-04\n",
      "Epoch 944/1000\n",
      "246/246 [==============================] - 0s 296us/sample - loss: 9.1406e-04\n",
      "Epoch 945/1000\n",
      "246/246 [==============================] - 0s 300us/sample - loss: 9.6524e-04\n",
      "Epoch 946/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 0.0010\n",
      "Epoch 947/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 0.0011\n",
      "Epoch 948/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0012\n",
      "Epoch 949/1000\n",
      "246/246 [==============================] - 0s 308us/sample - loss: 9.9166e-04\n",
      "Epoch 950/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 0.0011\n",
      "Epoch 951/1000\n",
      "246/246 [==============================] - 0s 247us/sample - loss: 0.0010\n",
      "Epoch 952/1000\n",
      "246/246 [==============================] - 0s 288us/sample - loss: 0.0011\n",
      "Epoch 953/1000\n",
      "246/246 [==============================] - 0s 324us/sample - loss: 9.5013e-04\n",
      "Epoch 954/1000\n",
      "246/246 [==============================] - 0s 397us/sample - loss: 0.0012\n",
      "Epoch 955/1000\n",
      "246/246 [==============================] - 0s 361us/sample - loss: 0.0014\n",
      "Epoch 956/1000\n",
      "246/246 [==============================] - 0s 292us/sample - loss: 0.0014\n",
      "Epoch 957/1000\n",
      "246/246 [==============================] - 0s 288us/sample - loss: 0.0012\n",
      "Epoch 958/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 0.0013\n",
      "Epoch 959/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 9.9834e-04\n",
      "Epoch 960/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 0.0011\n",
      "Epoch 961/1000\n",
      "246/246 [==============================] - 0s 288us/sample - loss: 0.0012\n",
      "Epoch 962/1000\n",
      "246/246 [==============================] - 0s 288us/sample - loss: 0.0010\n",
      "Epoch 963/1000\n",
      "246/246 [==============================] - 0s 276us/sample - loss: 9.5319e-04\n",
      "Epoch 964/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0011\n",
      "Epoch 965/1000\n",
      "246/246 [==============================] - 0s 385us/sample - loss: 9.6704e-04\n",
      "Epoch 966/1000\n",
      "246/246 [==============================] - 0s 454us/sample - loss: 0.0011\n",
      "Epoch 967/1000\n",
      "246/246 [==============================] - 0s 300us/sample - loss: 0.0012\n",
      "Epoch 968/1000\n",
      "246/246 [==============================] - 0s 292us/sample - loss: 0.0013\n",
      "Epoch 969/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0010\n",
      "Epoch 970/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 0.0011\n",
      "Epoch 971/1000\n",
      "246/246 [==============================] - 0s 255us/sample - loss: 0.0010\n",
      "Epoch 972/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 8.8224e-04\n",
      "Epoch 973/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 9.4894e-04\n",
      "Epoch 974/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 0.0010\n",
      "Epoch 975/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0010\n",
      "Epoch 976/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 7.3673e-04\n",
      "Epoch 977/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 9.5901e-04\n",
      "Epoch 978/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 9.7712e-04\n",
      "Epoch 979/1000\n",
      "246/246 [==============================] - 0s 276us/sample - loss: 9.7594e-04\n",
      "Epoch 980/1000\n",
      "246/246 [==============================] - 0s 251us/sample - loss: 0.0011\n",
      "Epoch 981/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 0.0011\n",
      "Epoch 982/1000\n",
      "246/246 [==============================] - 0s 243us/sample - loss: 0.0010\n",
      "Epoch 983/1000\n",
      "246/246 [==============================] - 0s 264us/sample - loss: 0.0010\n",
      "Epoch 984/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 0.0012\n",
      "Epoch 985/1000\n",
      "246/246 [==============================] - 0s 268us/sample - loss: 0.0011\n",
      "Epoch 986/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 9.6570e-04\n",
      "Epoch 987/1000\n",
      "246/246 [==============================] - 0s 324us/sample - loss: 0.0011\n",
      "Epoch 988/1000\n",
      "246/246 [==============================] - 0s 259us/sample - loss: 0.0010\n",
      "Epoch 989/1000\n",
      "246/246 [==============================] - 0s 320us/sample - loss: 0.0011\n",
      "Epoch 990/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 0.0012\n",
      "Epoch 991/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 9.6010e-04\n",
      "Epoch 992/1000\n",
      "246/246 [==============================] - 0s 239us/sample - loss: 9.5785e-04\n",
      "Epoch 993/1000\n",
      "246/246 [==============================] - 0s 296us/sample - loss: 9.6697e-04\n",
      "Epoch 994/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 0.0010\n",
      "Epoch 995/1000\n",
      "246/246 [==============================] - 0s 288us/sample - loss: 9.4338e-04\n",
      "Epoch 996/1000\n",
      "246/246 [==============================] - 0s 284us/sample - loss: 0.0012\n",
      "Epoch 997/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "246/246 [==============================] - 0s 276us/sample - loss: 0.0015\n",
      "Epoch 998/1000\n",
      "246/246 [==============================] - 0s 272us/sample - loss: 9.7560e-04\n",
      "Epoch 999/1000\n",
      "246/246 [==============================] - 0s 292us/sample - loss: 0.0010\n",
      "Epoch 1000/1000\n",
      "246/246 [==============================] - 0s 280us/sample - loss: 0.0010\n",
      "\n",
      "==========================\n",
      "MAE= 9.123882640491832\n",
      "RMSE =  11.025315591286658\n",
      "NMSE =  0.18512857446917508\n",
      "MAPE= 0.04273191499426806\n",
      "IA= 0.594693729046041\n",
      "U1= 0.02675875094611937\n",
      "U2= 0.93313562873907\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 70 seconds\n"
     ]
    }
   ],
   "source": [
    "#Bi-LSTM-SED\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import merge\n",
    "from tensorflow.python.keras.layers.merge import Multiply\n",
    "from tensorflow.python.keras.layers.core import *\n",
    "from tensorflow.python.keras.models import *\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python.keras.layers import Input,Dense,Reshape,Dropout, Embedding, LSTM, Bidirectional,Permute\n",
    "from tensorflow.python.keras.layers import RepeatVector, TimeDistributed\n",
    "from tensorflow.python.keras.layers.recurrent import GRU\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import concat, read_csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sklearn.metrics as skm\n",
    "import math\n",
    "from math import sqrt\n",
    "import time\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "#1. load dataset\n",
    "dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_NFP.csv', engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "data = dataset.reshape(-1,6)\n",
    "\n",
    "\n",
    "timestep = 6\n",
    "dim = 6\n",
    "\n",
    "#数据缩放 拆分输入X（7维）&输出Y（1维）\n",
    "X = dataset[:,0:6]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "for i in range(dim):\n",
    "    Xdata = X[:,i]\n",
    "    Xdata = Xdata.reshape(-1,1)\n",
    "    Xdata = scaler.fit_transform(Xdata)\n",
    "    Xdata = Xdata.flatten()\n",
    "    X[:,i] = Xdata\n",
    "X_scaler = X.reshape(324,-1)\n",
    "print(X_scaler.shape)\n",
    "\n",
    "Y = dataset[:,0]\n",
    "Y_scaler= (Y-np.min(Y))/(np.max(Y)-np.min(Y))\n",
    "Y_scaler = Y_scaler.reshape(-1)\n",
    "print(Y_scaler.shape)\n",
    "\n",
    "\n",
    "#重构数据集\n",
    "##timestep为时间步长\n",
    "def create_X(seq, timestep):\n",
    "    dataX = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[i:(i+timestep)]\n",
    "        # X按照顺序取值 每次在后面增加一个数据\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "\n",
    "def create_Y(seq, timestep):\n",
    "    dataY = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        # Y向后移动一位取值\n",
    "        dataY.append(seq[i+timestep])\n",
    "    return np.array(dataY)\n",
    "\n",
    "\n",
    "\n",
    "def get_bilstm_model():\n",
    "    K.clear_session() #清除之前的模型，省得压满内存\n",
    "    inputs = Input(shape=(timestep, dim,))\n",
    "    lstm_units1 = 60\n",
    "    lstm_units2 = 12\n",
    "    bilstm_out1 = Bidirectional(LSTM(lstm_units1,return_sequences=True),merge_mode='concat')(inputs)\n",
    "    bilstm_out2 = LSTM(lstm_units2, return_sequences=True)(bilstm_out1)\n",
    "    dropout_out = Dropout(0.5)(bilstm_out2)\n",
    "    bilstm_out = Flatten()(dropout_out)\n",
    "    output = Dense(1, activation='relu')(bilstm_out)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#预测\n",
    "pres=[]\n",
    "# 将数据拆分成训练和测试，7/9作为训练数据\n",
    "train_size = int(len(dataset) * 0.78)\n",
    "test_size = len(dataset) - train_size\n",
    "trainX, testX = X_scaler[0:train_size], X_scaler[train_size:len(dataset)]\n",
    "trainY, testY = Y_scaler[0:train_size], Y_scaler[train_size:len(dataset)]\n",
    "print(\"原始训练集的长度：\",train_size)\n",
    "print(\"原始测试集的长度：\",test_size)\n",
    "#print(trainX,trainX.shape)\n",
    "#print(trainY,trainY.shape)\n",
    "#print(testX,testX.shape)\n",
    "#print(testY,testY.shape)\n",
    "\n",
    "train_X=create_X(trainX,timestep)#(246,6)\n",
    "#print(train_X,train_X.shape)\n",
    "train_Y=create_Y(trainY,timestep)#(246,)\n",
    "#print(train_Y,train_Y.shape)\n",
    "test_X=create_X(testX,timestep)#(66,6,6)\n",
    "#print(test_X,test_X.shape)\n",
    "test_Y=create_Y(testY,timestep)#(66,)\n",
    "#print(test_Y,test_Y.shape)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        \n",
    "    model = get_bilstm_model()\n",
    "    optimizer = Adam(0.01)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(train_X, train_Y, epochs=1000, batch_size=64)\n",
    "\n",
    "    # 开始预测\n",
    "    trainPredict = model.predict(train_X)\n",
    "    testPredict = model.predict(test_X)\n",
    "\n",
    "\n",
    "    # 逆缩放预测值\n",
    "    dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_NFP.csv', engine='python')\n",
    "    dataset = dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "    Y = dataset[:,0]\n",
    "    #trainPredict = trainPredict*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori = trainY*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori=trainY_re.reshape((246,1))\n",
    "    testPre = testPredict*((np.max(Y)-np.min(Y)))+np.min(Y)\n",
    "    testPre = testPre.reshape(-1)\n",
    "    Y = dataset[:,0]\n",
    "    Y = Y[train_size+timestep:len(dataset)]\n",
    "    #testY_ori = testY*((np.max(dataset)-np.min(dataset)))+np.min(dataset)\n",
    "    #testY_ori = testY_ori.reshape((-1,1))\n",
    "    testY_ori = Y.reshape(-1)\n",
    "\n",
    "    \n",
    "\n",
    "    # 计算误差\n",
    "    #trainScore = math.sqrt(mean_squared_error(trainY_ori[:,0], trainPredict[:,0]))\n",
    "    #print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    #testScore = math.sqrt(mean_squared_error(testY_ori[:,0], testPredict[:,0]))\n",
    "    #print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "\n",
    "    error = []#Y-Y'\n",
    "    error1= []#abs((Y-Y')/Y)\n",
    "    error2= []#Y*Y'\n",
    "    squared1 =[]#Y*Y\n",
    "    squared2 =[]#Y'*Y'\n",
    "    for i in range(len(testY_ori)):\n",
    "        error.append(testY_ori[i] - testPre[i])\n",
    "        error1.append(abs((testY_ori[i] - testPre[i])/testY_ori[i]))\n",
    "        error2.append(testY_ori[i]*testPre[i])\n",
    "        squared1.append(testY_ori[i]*testY_ori[i])\n",
    "        squared2.append(testPre[i]*testPre[i])\n",
    "        \n",
    "        \n",
    "    squaredError = []#(Y-Y')^2\n",
    "    absError = []#abs(Y-Y')\n",
    "    for val in error:    \n",
    "        squaredError.append(val * val)#target-prediction之差平方     \n",
    "        absError.append(abs(val))#误差绝对值\n",
    "        \n",
    "    MSE=sum(squaredError) / len(squaredError)\n",
    "    meannn=np.mean(testY_ori)\n",
    "    NMSEerror = []\n",
    "    IAerror = []\n",
    "    for i in range(len(testY_ori)):\n",
    "        NMSEerror.append(squaredError[i]/error2[i])\n",
    "        IAerror.append((abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn))*(abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn)))\n",
    "    \n",
    "    U2error1 = []\n",
    "    U2error2 = []\n",
    "    for i in range(len(testY_ori)-1):\n",
    "        U2error1.append(((testY_ori[i+1] - testPre[i+1])/testY_ori[i])*((testY_ori[i+1] - testPre[i+1])/testY_ori[i]))\n",
    "        U2error2.append(((testY_ori[i+1] - testPre[i])/testY_ori[i])*((testY_ori[i+1] - testPre[i])/testY_ori[i]))\n",
    "    print('\\n==========================')\n",
    "    MAE=sum(absError)/len(absError)\n",
    "    print('MAE=',MAE)\n",
    "    from math import sqrt\n",
    "    RMSE=sqrt(MSE)\n",
    "    print(\"RMSE = \", RMSE)#均方根误差RMSE\n",
    "    NMSE=sum(NMSEerror)\n",
    "    print(\"NMSE = \", NMSE)#误差平方的归一化平均值NMSE\n",
    "    MAPE=sum(error1)/len(error1)\n",
    "    print('MAPE=',MAPE)\n",
    "    IA=1-sum(squaredError)/sum(IAerror)\n",
    "    print('IA=',IA)#一致性指数\n",
    "    U1index=RMSE/(sqrt(sum(squared1)/len(squared1))+sqrt(sum(squared2)/len(squared2)))\n",
    "    print('U1=',U1index)\n",
    "    U2index=(sqrt(sum(U2error1)/len(testY_ori)))/(sqrt(sum(U2error2)/len(testY_ori)))\n",
    "    print('U2=',U2index)\n",
    "\n",
    "    #print(testPredict)\n",
    "    testPre=testPre.reshape(-1)\n",
    "    testPre=pd.DataFrame(testPre)\n",
    "    testPre.to_csv('C:/Users/Administrator/Desktop/XI-2/data/pre_result/lstmoutput.csv', header=False)\n",
    "    import csv\n",
    "    Evaluation_index = [MAE,RMSE,NMSE,MAPE,IA,U1index,U2index]\n",
    "    with open(\"C:/Users/Administrator/Desktop/XI-2/data/pre_result/lstmEvaluation.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file ,delimiter=',')\n",
    "        writer.writerow(Evaluation_index)\n",
    "    \n",
    "    plt.figure(facecolor='white')\n",
    "    plt.plot(testPre,color='green', label='Predict')\n",
    "    plt.plot(testY_ori,color='pink', label='Original')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    pres.append(testPre)\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095ac58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
