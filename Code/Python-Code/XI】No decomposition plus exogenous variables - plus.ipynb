{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b0c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConvLSTM-SED   multivariable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.python.keras.layers import ConvLSTM2D\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#matplotlib inline\n",
    "import time\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "#dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/PD-TVFEMD-IMF1.csv', usecols=[6], engine='python')\n",
    "dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_DJH.csv', engine='python')\n",
    "#print(dataframe)\n",
    "print(\"数据集的长度：\",len(dataframe))\n",
    "dataset = dataframe.values\n",
    "# 将整型变为float\n",
    "dataset = dataset.astype('float32')\n",
    "#print(dataset)\n",
    "\n",
    "timestep = 6\n",
    "dim = 2\n",
    "#数据缩放 拆分输入X（7维）&输出Y（1维）\n",
    "X = dataset[:,0:2]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "for i in range(dim):\n",
    "    Xdata = X[:,i]\n",
    "    Xdata = Xdata.reshape(-1,1)\n",
    "    Xdata = scaler.fit_transform(Xdata)\n",
    "    Xdata = Xdata.flatten()\n",
    "    X[:,i] = Xdata\n",
    "X_scaler = X.reshape(324,-1)\n",
    "\n",
    "Y = dataset[:,0]\n",
    "Y_scaler= (Y-np.min(Y))/(np.max(Y)-np.min(Y))\n",
    "Y_scaler = Y_scaler.reshape(-1)\n",
    "\n",
    "#print(X_scaler)\n",
    "#print(Y_scaler)\n",
    "#print(Y)\n",
    "\n",
    "\n",
    "# 将数据拆分成训练和测试，8/9作为训练数据\n",
    "train_size = int(len(dataset) * 0.89)\n",
    "test_size = len(dataset) - train_size\n",
    "trainX, testX = X_scaler[0:train_size,:], X_scaler[train_size:len(dataset),:]\n",
    "trainY, testY = Y_scaler[0:train_size], Y_scaler[train_size:len(dataset)]\n",
    "print(\"原始训练集的长度：\",train_size)\n",
    "print(\"原始测试集的长度：\",test_size)\n",
    "#print(trainX)\n",
    "#print(trainY)\n",
    "#print(testX)\n",
    "#print(testY)\n",
    "\n",
    "\n",
    "\n",
    "#重构数据集\n",
    "##timestep为时间步长\n",
    "def create_trainX(seq, timestep):\n",
    "    dataX = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[i:(i+timestep)]\n",
    "        # X按照顺序取值 每次在后面增加一个数据\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "\n",
    "def create_trainY(seq, timestep):\n",
    "    dataY = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        # Y向后移动一位取值\n",
    "        dataY.append(seq[i + timestep])\n",
    "    return np.array(dataY)\n",
    "\n",
    "def create_testX(seq, timestep):\n",
    "    dataX = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[(i):(i+timestep)]\n",
    "        # X按照顺序取值 每次在后面增加一个数据\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "\n",
    "\n",
    "\n",
    "#----------forecasting-----------\n",
    "\n",
    "pres=[]\n",
    "trainX = create_trainX(trainX, timestep)\n",
    "trainY = create_trainY(trainY, timestep)\n",
    "testX = create_testX(testX, timestep) \n",
    "testY = testY[timestep:len(testY)]\n",
    "print(\"转为监督学习，训练集数据长度：\", len(trainX))\n",
    "#print(trainX,trainY)\n",
    "print(\"转为监督学习，测试集数据长度：\",len(testX))\n",
    "#print(testX, testY )\n",
    "\n",
    "\n",
    "# 数据重构为5D [samples, timesteps, rows, columns, features]\n",
    "trainX_input5D = np.reshape(trainX, (trainX.shape[0], 2,1,timestep//2, dim))\n",
    "testX_input5D = np.reshape(testX, (testX.shape[0],2,1,timestep//2, dim))\n",
    "#trainX_input5D = np.reshape(trainX, (trainX.shape[0], timestep,1,1, dim))\n",
    "#testX_input5D = np.reshape(testX, (testX.shape[0],timestep,1,1, dim))\n",
    "print('构造得到模型的输入数据(训练数据已有标签trainY): ',trainX_input5D.shape,testX_input5D.shape)\n",
    "\n",
    "\n",
    "# create and fit the convlstm network\n",
    "if __name__ == '__main__':\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=12, kernel_size=(2,2), activation='relu', \n",
    "                         input_shape=(2,1,timestep//2, dim),\n",
    "#                         input_shape=(timestep,1,1, dim),\n",
    "                         padding='same', return_sequences=True))\n",
    "    model.add(ConvLSTM2D(filters=12, kernel_size=(1,1), activation='relu',\n",
    "                         padding='same', return_sequences=True))\n",
    "    \n",
    "    #model.add(Dropout(0.1))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.fit(trainX_input5D, trainY, epochs=700)\n",
    "\n",
    "    # 打印模型\n",
    "    model.summary()\n",
    "\n",
    "    # 开始预测\n",
    "    trainPredict = model.predict(trainX_input5D)\n",
    "    testPredict = model.predict(testX_input5D)\n",
    "\n",
    "    # 逆缩放预测值\n",
    "    dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD.csv', engine='python')\n",
    "    dataset = dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "    Y = dataset[:,0]\n",
    "    #trainPredict = trainPredict*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori = trainY*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori=trainY_re.reshape((246,1))\n",
    "    testPredict = testPredict*((np.max(Y)-np.min(Y)))+np.min(Y)\n",
    "    testPre = testPredict.reshape(-1)\n",
    "    Y = dataset[:,0]\n",
    "    Y = Y[train_size+timestep:len(dataset)]\n",
    "    #testY_ori = testY*((np.max(dataset)-np.min(dataset)))+np.min(dataset)\n",
    "    #testY_ori = testY_ori.reshape((-1,1))\n",
    "    testY_ori = Y.reshape(-1)\n",
    "\n",
    "    \n",
    "\n",
    "    # 计算误差\n",
    "    #trainScore = math.sqrt(mean_squared_error(trainY_ori[:,0], trainPredict[:,0]))\n",
    "    #print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    #testScore = math.sqrt(mean_squared_error(testY_ori[:,0], testPredict[:,0]))\n",
    "    #print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "\n",
    "    error = []#Y-Y'\n",
    "    error1= []#abs((Y-Y')/Y)\n",
    "    error2= []#Y*Y'\n",
    "    squared1 =[]#Y*Y\n",
    "    squared2 =[]#Y'*Y'\n",
    "    for i in range(len(testY_ori)):\n",
    "        error.append(testY_ori[i] - testPre[i])\n",
    "        error1.append(abs((testY_ori[i] - testPre[i])/testY_ori[i]))\n",
    "        error2.append(testY_ori[i]*testPre[i])\n",
    "        squared1.append(testY_ori[i]*testY_ori[i])\n",
    "        squared2.append(testPre[i]*testPre[i])\n",
    "        \n",
    "        \n",
    "    squaredError = []#(Y-Y')^2\n",
    "    absError = []#abs(Y-Y')\n",
    "    for val in error:    \n",
    "        squaredError.append(val * val)#target-prediction之差平方     \n",
    "        absError.append(abs(val))#误差绝对值\n",
    "        \n",
    "    MSE=sum(squaredError) / len(squaredError)\n",
    "    meannn=np.mean(testY_ori)\n",
    "    NMSEerror = []\n",
    "    IAerror = []\n",
    "    for i in range(len(testY_ori)):\n",
    "        NMSEerror.append(squaredError[i]/error2[i])\n",
    "        IAerror.append((abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn))*(abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn)))\n",
    "    \n",
    "    U2error1 = []\n",
    "    U2error2 = []\n",
    "    for i in range(len(testY_ori)-1):\n",
    "        U2error1.append(((testY_ori[i+1] - testPre[i+1])/testY_ori[i])*((testY_ori[i+1] - testPre[i+1])/testY_ori[i]))\n",
    "        U2error2.append(((testY_ori[i+1] - testPre[i])/testY_ori[i])*((testY_ori[i+1] - testPre[i])/testY_ori[i]))\n",
    "    print('\\n==========================')\n",
    "    MAE=sum(absError)/len(absError)\n",
    "    print('MAE=',MAE)\n",
    "    from math import sqrt\n",
    "    RMSE=sqrt(MSE)\n",
    "    print(\"RMSE = \", RMSE)#均方根误差RMSE\n",
    "    NMSE=sum(NMSEerror)\n",
    "    print(\"NMSE = \", NMSE)#误差平方的归一化平均值NMSE\n",
    "    MAPE=sum(error1)/len(error1)\n",
    "    print('MAPE=',MAPE)\n",
    "    IA=1-sum(squaredError)/sum(IAerror)\n",
    "    print('IA=',IA)#一致性指数\n",
    "    U1index=RMSE/(sqrt(sum(squared1)/len(squared1))+sqrt(sum(squared2)/len(squared2)))\n",
    "    print('U1=',U1index)\n",
    "    U2index=(sqrt(sum(U2error1)/len(testY_ori)))/(sqrt(sum(U2error2)/len(testY_ori)))\n",
    "    print('U2=',U2index)\n",
    "\n",
    "    #print(testPredict)\n",
    "    testPre=testPre.reshape(-1)\n",
    "    testPre=pd.DataFrame(testPre)\n",
    "    testPre.to_csv('C:/Users/Administrator/Desktop/XI-2/data/pre_result/convlstmoutput.csv', header=False)\n",
    "    import csv\n",
    "    Evaluation_index = [MAE,RMSE,NMSE,MAPE,IA,U1index,U2index]\n",
    "    with open(\"C:/Users/Administrator/Desktop/XI-2/data/pre_result/convlstmEvaluation.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file ,delimiter=',')\n",
    "        writer.writerow(Evaluation_index)\n",
    "    \n",
    "    plt.figure(facecolor='white')\n",
    "    plt.plot(testPre,color='green', label='Predict')\n",
    "    plt.plot(testY_ori,color='pink', label='Original')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    pres.append(testPre)\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))\n",
    "\n",
    "#ConvLSTM-SED   multivariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaf298d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#TSRConvLSTM-SED-(2,2)\n",
    "import numpy\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.python.keras.layers import ConvLSTM2D\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#matplotlib inline\n",
    "import time\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# load the dataset\n",
    "#dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/PD-IMF1.csv', usecols=[6], engine='python')\n",
    "dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_DJH.csv', engine='python')\n",
    "#print(dataframe)\n",
    "print(\"数据集的长度：\",len(dataframe))\n",
    "dataset = dataframe.values\n",
    "# 将整型变为float\n",
    "dataset = dataset.astype('float32')\n",
    "#print(dataset)\n",
    "Y = dataset[:,0]#Y\n",
    "Y_scaler= (Y-numpy.min(Y))/(numpy.max(Y)-numpy.min(Y))\n",
    "Y_scaler = Y_scaler.reshape(-1)\n",
    "\n",
    "X = dataset[:,1]\n",
    "X_scaler= (X-numpy.min(X))/(numpy.max(X)-numpy.min(X))\n",
    "X_scaler = X_scaler.reshape(-1)\n",
    "\n",
    "XX = numpy.zeros([324,2,2])\n",
    "XX[:,0,0] = Y_scaler\n",
    "XX[:,0,1] = X_scaler\n",
    "XX[:,1,0] = X_scaler\n",
    "XX[:,1,1] = Y_scaler\n",
    "\n",
    "# 将数据拆分成训练和测试，7/9作为训练数据\n",
    "train_size = int(len(dataset) * 0.78)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = XX[0:train_size,:,:], XX[train_size:len(dataset),:,:]\n",
    "print(\"原始训练集的长度：\",train_size)\n",
    "print(\"原始测试集的长度：\",test_size)\n",
    "\n",
    "\n",
    "# 切片\n",
    "def create_train(seq, timestep):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[i:(i + timestep),:,:]\n",
    "        # X按照顺序取值\n",
    "        dataX.append(a)\n",
    "        # Y向后移动一位取值\n",
    "        dataY.append(seq[i + timestep,:,:])\n",
    "    return numpy.array(dataX), numpy.array(dataY)\n",
    "\n",
    "def create_testX(seq, timestep):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[i:(i + timestep),:,:]\n",
    "        # X按照顺序取值\n",
    "        dataX.append(a)\n",
    "    return numpy.array(dataX)\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "\n",
    "\n",
    "\n",
    "#----------forecasting-----------\n",
    "timestep = 6\n",
    "pres=[]\n",
    "trainX,trainY = create_train(train, timestep)\n",
    "testX = create_testX(test, timestep) \n",
    "testY = test[timestep:len(X),0,0]\n",
    "print(\"转为监督学习，训练集数据长度：\", len(trainX))\n",
    "#print(trainX,trainY)\n",
    "print(\"转为监督学习，测试集数据长度：\",len(testX))\n",
    "#print(testX, testY )\n",
    "\n",
    "\n",
    "# 数据重构为5D [samples, timesteps, rows, columns, features]\n",
    "trainX_input5D = numpy.reshape(trainX, (trainX.shape[0], timestep,2,2, 1))\n",
    "testX_input5D = numpy.reshape(testX, (testX.shape[0],timestep, 2,2, 1))\n",
    "print('构造得到模型的输入数据(训练数据已有标签trainY): ',trainX_input5D.shape,testX_input5D.shape)\n",
    "\n",
    "\n",
    "# create and fit the convlstm network\n",
    "from tensorflow.python.keras.layers.convolutional import Conv3D ,Conv2D\n",
    "if __name__ == '__main__':\n",
    "    model = Sequential()\n",
    "    model.add(ConvLSTM2D(filters=12, kernel_size=(2, 2), activation='relu', input_shape=(timestep, 2,2,1),\n",
    "                         padding='same',return_sequences=True))\n",
    "    model.add(ConvLSTM2D(filters=12, kernel_size=(2, 2),activation='relu',\n",
    "                         padding='same',return_sequences=False))\n",
    "    model.add(Conv2D(filters=1, kernel_size=(1, 1),\n",
    "               activation='sigmoid',\n",
    "               padding='same', data_format='channels_last'))\n",
    "    #model.add(Flatten())\n",
    "    #model.add(Dense(1))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.fit(trainX_input5D, trainY, epochs=1000)\n",
    "\n",
    "    # 打印模型\n",
    "    model.summary()\n",
    "\n",
    "    # 开始预测\n",
    "    trainPredict = model.predict(trainX_input5D)\n",
    "    testPredict = model.predict(testX_input5D)\n",
    "    \n",
    "    # 逆缩放预测值\n",
    "    testPredict1 = testPredict[:,0,0,:]\n",
    "    testPredict1 = testPredict1.reshape(-1)\n",
    "    dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD.csv', engine='python')\n",
    "    dataset = dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "    Y = dataset[:,0]\n",
    "    #trainPredict = trainPredict*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori = trainY*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori=trainY_re.reshape((246,1))\n",
    "    testPredict = testPredict1*((np.max(Y)-np.min(Y)))+np.min(Y)\n",
    "    testPre = testPredict.reshape(-1)\n",
    "    Y = dataset[:,0]\n",
    "    Y = Y[train_size+timestep:len(dataset)]\n",
    "    #testY_ori = testY*((np.max(dataset)-np.min(dataset)))+np.min(dataset)\n",
    "    #testY_ori = testY_ori.reshape((-1,1))\n",
    "    testY_ori = Y.reshape(-1)\n",
    "\n",
    "    \n",
    "\n",
    "    # 计算误差\n",
    "    #trainScore = math.sqrt(mean_squared_error(trainY_ori[:,0], trainPredict[:,0]))\n",
    "    #print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    #testScore = math.sqrt(mean_squared_error(testY_ori[:,0], testPredict[:,0]))\n",
    "    #print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "\n",
    "    error = []#Y-Y'\n",
    "    error1= []#abs((Y-Y')/Y)\n",
    "    error2= []#Y*Y'\n",
    "    squared1 =[]#Y*Y\n",
    "    squared2 =[]#Y'*Y'\n",
    "    for i in range(len(testY_ori)):\n",
    "        error.append(testY_ori[i] - testPre[i])\n",
    "        error1.append(abs((testY_ori[i] - testPre[i])/testY_ori[i]))\n",
    "        error2.append(testY_ori[i]*testPre[i])\n",
    "        squared1.append(testY_ori[i]*testY_ori[i])\n",
    "        squared2.append(testPre[i]*testPre[i])\n",
    "        \n",
    "        \n",
    "    squaredError = []#(Y-Y')^2\n",
    "    absError = []#abs(Y-Y')\n",
    "    for val in error:    \n",
    "        squaredError.append(val * val)#target-prediction之差平方     \n",
    "        absError.append(abs(val))#误差绝对值\n",
    "        \n",
    "    MSE=sum(squaredError) / len(squaredError)\n",
    "    meannn=np.mean(testY_ori)\n",
    "    NMSEerror = []\n",
    "    IAerror = []\n",
    "    for i in range(len(testY_ori)):\n",
    "        NMSEerror.append(squaredError[i]/error2[i])\n",
    "        IAerror.append((abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn))*(abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn)))\n",
    "    \n",
    "    U2error1 = []\n",
    "    U2error2 = []\n",
    "    for i in range(len(testY_ori)-1):\n",
    "        U2error1.append(((testY_ori[i+1] - testPre[i+1])/testY_ori[i])*((testY_ori[i+1] - testPre[i+1])/testY_ori[i]))\n",
    "        U2error2.append(((testY_ori[i+1] - testPre[i])/testY_ori[i])*((testY_ori[i+1] - testPre[i])/testY_ori[i]))\n",
    "    print('\\n==========================')\n",
    "    MAE=sum(absError)/len(absError)\n",
    "    print('MAE=',MAE)\n",
    "    from math import sqrt\n",
    "    RMSE=sqrt(MSE)\n",
    "    print(\"RMSE = \", RMSE)#均方根误差RMSE\n",
    "    NMSE=sum(NMSEerror)\n",
    "    print(\"NMSE = \", NMSE)#误差平方的归一化平均值NMSE\n",
    "    MAPE=sum(error1)/len(error1)\n",
    "    print('MAPE=',MAPE)\n",
    "    IA=1-sum(squaredError)/sum(IAerror)\n",
    "    print('IA=',IA)#一致性指数\n",
    "    U1index=RMSE/(sqrt(sum(squared1)/len(squared1))+sqrt(sum(squared2)/len(squared2)))\n",
    "    print('U1=',U1index)\n",
    "    U2index=(sqrt(sum(U2error1)/len(testY_ori)))/(sqrt(sum(U2error2)/len(testY_ori)))\n",
    "    print('U2=',U2index)\n",
    "\n",
    "    #print(testPredict)\n",
    "    testPre=testPre.reshape(-1)\n",
    "    testPre=pd.DataFrame(testPre)\n",
    "    testPre.to_csv('C:/Users/Administrator/Desktop/XI-2/data/pre_result/convlstmoutput.csv', header=False)\n",
    "    import csv\n",
    "    Evaluation_index = [MAE,RMSE,NMSE,MAPE,IA,U1index,U2index]\n",
    "    with open(\"C:/Users/Administrator/Desktop/XI-2/data/pre_result/convlstmEvaluation.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file ,delimiter=',')\n",
    "        writer.writerow(Evaluation_index)\n",
    "    \n",
    "    plt.figure(facecolor='white')\n",
    "    plt.plot(testPre,color='green', label='Predict')\n",
    "    plt.plot(testY_ori,color='pink', label='Original')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    pres.append(testPre)\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b30f0bc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集的长度： 324\n",
      "原始训练集的长度： 288\n",
      "原始测试集的长度： 36\n",
      "转为监督学习，训练集数据长度： 282\n",
      "转为监督学习，测试集数据长度： 30\n",
      "构造得到模型的输入数据(训练数据已有标签trainY):  (282, 1, 6, 2) (30, 1, 6, 2)\n",
      "Train on 282 samples\n",
      "Epoch 1/1000\n",
      "282/282 [==============================] - 1s 2ms/sample - loss: 0.1689\n",
      "Epoch 2/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.1037\n",
      "Epoch 3/1000\n",
      "282/282 [==============================] - 0s 78us/sample - loss: 0.0577\n",
      "Epoch 4/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0318\n",
      "Epoch 5/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0204\n",
      "Epoch 6/1000\n",
      "282/282 [==============================] - 0s 99us/sample - loss: 0.0175\n",
      "Epoch 7/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0171\n",
      "Epoch 8/1000\n",
      "282/282 [==============================] - 0s 110us/sample - loss: 0.0168\n",
      "Epoch 9/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0164\n",
      "Epoch 10/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0159\n",
      "Epoch 11/1000\n",
      "282/282 [==============================] - 0s 149us/sample - loss: 0.0155\n",
      "Epoch 12/1000\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.017 - 0s 85us/sample - loss: 0.0152\n",
      "Epoch 13/1000\n",
      "282/282 [==============================] - 0s 103us/sample - loss: 0.0148\n",
      "Epoch 14/1000\n",
      "282/282 [==============================] - 0s 92us/sample - loss: 0.0144\n",
      "Epoch 15/1000\n",
      "282/282 [==============================] - 0s 78us/sample - loss: 0.0141\n",
      "Epoch 16/1000\n",
      "282/282 [==============================] - 0s 88us/sample - loss: 0.0137\n",
      "Epoch 17/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0134\n",
      "Epoch 18/1000\n",
      "282/282 [==============================] - 0s 81us/sample - loss: 0.0130\n",
      "Epoch 19/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0127\n",
      "Epoch 20/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0123\n",
      "Epoch 21/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0120\n",
      "Epoch 22/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0117\n",
      "Epoch 23/1000\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.011 - 0s 78us/sample - loss: 0.0114\n",
      "Epoch 24/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0111\n",
      "Epoch 25/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0107\n",
      "Epoch 26/1000\n",
      "282/282 [==============================] - 0s 88us/sample - loss: 0.0104\n",
      "Epoch 27/1000\n",
      "282/282 [==============================] - 0s 78us/sample - loss: 0.0102\n",
      "Epoch 28/1000\n",
      "282/282 [==============================] - 0s 85us/sample - loss: 0.0099\n",
      "Epoch 29/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0097\n",
      "Epoch 30/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0094\n",
      "Epoch 31/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0091\n",
      "Epoch 32/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0089\n",
      "Epoch 33/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0087\n",
      "Epoch 34/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0085\n",
      "Epoch 35/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0083\n",
      "Epoch 36/1000\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.009 - 0s 64us/sample - loss: 0.0081\n",
      "Epoch 37/1000\n",
      "282/282 [==============================] - 0s 81us/sample - loss: 0.0079\n",
      "Epoch 38/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0077\n",
      "Epoch 39/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0076\n",
      "Epoch 40/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0074\n",
      "Epoch 41/1000\n",
      "282/282 [==============================] - 0s 81us/sample - loss: 0.0073\n",
      "Epoch 42/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0071\n",
      "Epoch 43/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0070\n",
      "Epoch 44/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0069\n",
      "Epoch 45/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0067\n",
      "Epoch 46/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0066\n",
      "Epoch 47/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0065\n",
      "Epoch 48/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0064\n",
      "Epoch 49/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0063\n",
      "Epoch 50/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0062\n",
      "Epoch 51/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0061\n",
      "Epoch 52/1000\n",
      "282/282 [==============================] - 0s 81us/sample - loss: 0.0060\n",
      "Epoch 53/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0059\n",
      "Epoch 54/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0058\n",
      "Epoch 55/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0058\n",
      "Epoch 56/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0057\n",
      "Epoch 57/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0056\n",
      "Epoch 58/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0055\n",
      "Epoch 59/1000\n",
      "282/282 [==============================] - 0s 78us/sample - loss: 0.0055\n",
      "Epoch 60/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0054\n",
      "Epoch 61/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0053\n",
      "Epoch 62/1000\n",
      "282/282 [==============================] - 0s 78us/sample - loss: 0.0053\n",
      "Epoch 63/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0052\n",
      "Epoch 64/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0051\n",
      "Epoch 65/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0051\n",
      "Epoch 66/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0051\n",
      "Epoch 67/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0051\n",
      "Epoch 68/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0050\n",
      "Epoch 69/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0049\n",
      "Epoch 70/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0049\n",
      "Epoch 71/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0048\n",
      "Epoch 72/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0048\n",
      "Epoch 73/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0047\n",
      "Epoch 74/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0047\n",
      "Epoch 75/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0046\n",
      "Epoch 76/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0046\n",
      "Epoch 77/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0045\n",
      "Epoch 78/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0045\n",
      "Epoch 79/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0045\n",
      "Epoch 80/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0044\n",
      "Epoch 81/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0044\n",
      "Epoch 82/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0043\n",
      "Epoch 83/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0043\n",
      "Epoch 84/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0042\n",
      "Epoch 85/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0042\n",
      "Epoch 86/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0042\n",
      "Epoch 87/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0041\n",
      "Epoch 88/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0041\n",
      "Epoch 89/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0041\n",
      "Epoch 90/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0040\n",
      "Epoch 91/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0040\n",
      "Epoch 92/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0040\n",
      "Epoch 93/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0040\n",
      "Epoch 94/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0039\n",
      "Epoch 95/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0039\n",
      "Epoch 96/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0039\n",
      "Epoch 97/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0039\n",
      "Epoch 98/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0039\n",
      "Epoch 99/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0038\n",
      "Epoch 100/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0038\n",
      "Epoch 101/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0038\n",
      "Epoch 102/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0038\n",
      "Epoch 103/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0038\n",
      "Epoch 104/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0038\n",
      "Epoch 105/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0038\n",
      "Epoch 106/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0037\n",
      "Epoch 107/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0037\n",
      "Epoch 108/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0037\n",
      "Epoch 109/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0037\n",
      "Epoch 110/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0037\n",
      "Epoch 111/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0037\n",
      "Epoch 112/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0037\n",
      "Epoch 113/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0036\n",
      "Epoch 114/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0036\n",
      "Epoch 115/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0036\n",
      "Epoch 116/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0036\n",
      "Epoch 117/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0036\n",
      "Epoch 118/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0036\n",
      "Epoch 119/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0036\n",
      "Epoch 120/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0036\n",
      "Epoch 121/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0036\n",
      "Epoch 122/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0035\n",
      "Epoch 123/1000\n",
      "282/282 [==============================] - ETA: 0s - loss: 0.004 - 0s 60us/sample - loss: 0.0035\n",
      "Epoch 124/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0035\n",
      "Epoch 125/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0035\n",
      "Epoch 126/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0035\n",
      "Epoch 127/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0035\n",
      "Epoch 128/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0035\n",
      "Epoch 129/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0034\n",
      "Epoch 130/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0034\n",
      "Epoch 131/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0034\n",
      "Epoch 132/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0034\n",
      "Epoch 133/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0034\n",
      "Epoch 134/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0034\n",
      "Epoch 135/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0034\n",
      "Epoch 136/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0034\n",
      "Epoch 137/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0033\n",
      "Epoch 138/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0033\n",
      "Epoch 139/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0033\n",
      "Epoch 140/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0033\n",
      "Epoch 141/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0033\n",
      "Epoch 142/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0033\n",
      "Epoch 143/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0033\n",
      "Epoch 144/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0032\n",
      "Epoch 145/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0033\n",
      "Epoch 146/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0032\n",
      "Epoch 147/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0032\n",
      "Epoch 148/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0032\n",
      "Epoch 149/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0032\n",
      "Epoch 150/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0032\n",
      "Epoch 151/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0032\n",
      "Epoch 152/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0031\n",
      "Epoch 153/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0031\n",
      "Epoch 154/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0031\n",
      "Epoch 155/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0031\n",
      "Epoch 156/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0031\n",
      "Epoch 157/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0031\n",
      "Epoch 158/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0031\n",
      "Epoch 159/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0031\n",
      "Epoch 160/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0030\n",
      "Epoch 161/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0030\n",
      "Epoch 162/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0030\n",
      "Epoch 163/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0030\n",
      "Epoch 164/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0030\n",
      "Epoch 165/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0030\n",
      "Epoch 166/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0030\n",
      "Epoch 167/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0030\n",
      "Epoch 168/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0030\n",
      "Epoch 169/1000\n",
      "282/282 [==============================] - 0s 145us/sample - loss: 0.0029\n",
      "Epoch 170/1000\n",
      "282/282 [==============================] - 0s 50us/sample - loss: 0.0029\n",
      "Epoch 171/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0029\n",
      "Epoch 172/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0029\n",
      "Epoch 173/1000\n",
      "282/282 [==============================] - 0s 53us/sample - loss: 0.0029\n",
      "Epoch 174/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0029\n",
      "Epoch 175/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0029\n",
      "Epoch 176/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0029\n",
      "Epoch 177/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0028\n",
      "Epoch 178/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0028\n",
      "Epoch 179/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0028\n",
      "Epoch 180/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0028\n",
      "Epoch 181/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0028\n",
      "Epoch 182/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0028\n",
      "Epoch 183/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0028\n",
      "Epoch 184/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0028\n",
      "Epoch 185/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0028\n",
      "Epoch 186/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0027\n",
      "Epoch 187/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0027\n",
      "Epoch 188/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0027\n",
      "Epoch 189/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0027\n",
      "Epoch 190/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0027\n",
      "Epoch 191/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0027\n",
      "Epoch 192/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0027\n",
      "Epoch 193/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0027\n",
      "Epoch 194/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0027\n",
      "Epoch 195/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0026\n",
      "Epoch 196/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0026\n",
      "Epoch 197/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0026\n",
      "Epoch 198/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0026\n",
      "Epoch 199/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0026\n",
      "Epoch 200/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0026\n",
      "Epoch 201/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0026\n",
      "Epoch 202/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0026\n",
      "Epoch 203/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0025\n",
      "Epoch 204/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0025\n",
      "Epoch 205/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0025\n",
      "Epoch 206/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0026\n",
      "Epoch 207/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0025\n",
      "Epoch 208/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0025\n",
      "Epoch 209/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0025\n",
      "Epoch 210/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0025\n",
      "Epoch 211/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0025\n",
      "Epoch 212/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0025\n",
      "Epoch 213/1000\n",
      "282/282 [==============================] - 0s 85us/sample - loss: 0.0025\n",
      "Epoch 214/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0024\n",
      "Epoch 215/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0024\n",
      "Epoch 216/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0024\n",
      "Epoch 217/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0024\n",
      "Epoch 218/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0024\n",
      "Epoch 219/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0024\n",
      "Epoch 220/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0024\n",
      "Epoch 221/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0024\n",
      "Epoch 222/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0024\n",
      "Epoch 223/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0023\n",
      "Epoch 224/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0023\n",
      "Epoch 225/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0023\n",
      "Epoch 226/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0023\n",
      "Epoch 227/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0023\n",
      "Epoch 228/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0023\n",
      "Epoch 229/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0023\n",
      "Epoch 230/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0023\n",
      "Epoch 231/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0023\n",
      "Epoch 232/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0022\n",
      "Epoch 233/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0022\n",
      "Epoch 234/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0022\n",
      "Epoch 235/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0022\n",
      "Epoch 236/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0022\n",
      "Epoch 237/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0022\n",
      "Epoch 238/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0022\n",
      "Epoch 239/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0022\n",
      "Epoch 240/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0022\n",
      "Epoch 241/1000\n",
      "282/282 [==============================] - 0s 92us/sample - loss: 0.0022\n",
      "Epoch 242/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0021\n",
      "Epoch 243/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0022\n",
      "Epoch 244/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0021\n",
      "Epoch 245/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0021\n",
      "Epoch 246/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0021\n",
      "Epoch 247/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0021\n",
      "Epoch 248/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0021\n",
      "Epoch 249/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0021\n",
      "Epoch 250/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0021\n",
      "Epoch 251/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0021\n",
      "Epoch 252/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0021\n",
      "Epoch 253/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0020\n",
      "Epoch 254/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0020\n",
      "Epoch 255/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0020\n",
      "Epoch 256/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0021\n",
      "Epoch 257/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0020\n",
      "Epoch 258/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0020\n",
      "Epoch 259/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0020\n",
      "Epoch 260/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0020\n",
      "Epoch 261/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0020\n",
      "Epoch 262/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0019\n",
      "Epoch 263/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0019\n",
      "Epoch 264/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0019\n",
      "Epoch 265/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0020\n",
      "Epoch 266/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0019\n",
      "Epoch 267/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0019\n",
      "Epoch 268/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0019\n",
      "Epoch 269/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0019\n",
      "Epoch 270/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0019\n",
      "Epoch 271/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0019\n",
      "Epoch 272/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0019\n",
      "Epoch 273/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0019\n",
      "Epoch 274/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0019\n",
      "Epoch 275/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0019\n",
      "Epoch 276/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0018\n",
      "Epoch 277/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0018\n",
      "Epoch 278/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0018\n",
      "Epoch 279/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0018\n",
      "Epoch 280/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0018\n",
      "Epoch 281/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0018\n",
      "Epoch 282/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0018\n",
      "Epoch 283/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0018\n",
      "Epoch 284/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0018\n",
      "Epoch 285/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0017\n",
      "Epoch 286/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0018\n",
      "Epoch 287/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0017\n",
      "Epoch 288/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0017\n",
      "Epoch 289/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0018\n",
      "Epoch 290/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0017\n",
      "Epoch 291/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0017\n",
      "Epoch 292/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0017\n",
      "Epoch 293/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0017\n",
      "Epoch 294/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0017\n",
      "Epoch 295/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0017\n",
      "Epoch 296/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0017\n",
      "Epoch 297/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0017\n",
      "Epoch 298/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0017\n",
      "Epoch 299/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0017\n",
      "Epoch 300/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0017\n",
      "Epoch 301/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0016\n",
      "Epoch 302/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0016\n",
      "Epoch 303/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0016\n",
      "Epoch 304/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0016\n",
      "Epoch 305/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0016\n",
      "Epoch 306/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0016\n",
      "Epoch 307/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0016\n",
      "Epoch 308/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0016\n",
      "Epoch 309/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0016\n",
      "Epoch 310/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0016\n",
      "Epoch 311/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0015\n",
      "Epoch 312/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0015\n",
      "Epoch 313/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0015\n",
      "Epoch 314/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0015\n",
      "Epoch 315/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0015\n",
      "Epoch 316/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0015\n",
      "Epoch 317/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0015\n",
      "Epoch 318/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0015\n",
      "Epoch 319/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0015\n",
      "Epoch 320/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0015\n",
      "Epoch 321/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0015\n",
      "Epoch 322/1000\n",
      "282/282 [==============================] - 0s 81us/sample - loss: 0.0015\n",
      "Epoch 323/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0015\n",
      "Epoch 324/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0015\n",
      "Epoch 325/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0015\n",
      "Epoch 326/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0015\n",
      "Epoch 327/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0014\n",
      "Epoch 328/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0014\n",
      "Epoch 329/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0014\n",
      "Epoch 330/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0014\n",
      "Epoch 331/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0014\n",
      "Epoch 332/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0014\n",
      "Epoch 333/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0014\n",
      "Epoch 334/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0014\n",
      "Epoch 335/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0014\n",
      "Epoch 336/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0014\n",
      "Epoch 337/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0014\n",
      "Epoch 338/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0014\n",
      "Epoch 339/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0014\n",
      "Epoch 340/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0014\n",
      "Epoch 341/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0014\n",
      "Epoch 342/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0014\n",
      "Epoch 343/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0014\n",
      "Epoch 344/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0013\n",
      "Epoch 345/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0013\n",
      "Epoch 346/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0013\n",
      "Epoch 347/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0013\n",
      "Epoch 348/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0013\n",
      "Epoch 349/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0013\n",
      "Epoch 350/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0013\n",
      "Epoch 351/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0013\n",
      "Epoch 352/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0013\n",
      "Epoch 353/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0013\n",
      "Epoch 354/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0013\n",
      "Epoch 355/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0013\n",
      "Epoch 356/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0013\n",
      "Epoch 357/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0013\n",
      "Epoch 358/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0013\n",
      "Epoch 359/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0013\n",
      "Epoch 360/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0013\n",
      "Epoch 361/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0013\n",
      "Epoch 362/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0013\n",
      "Epoch 363/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0013\n",
      "Epoch 364/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0013\n",
      "Epoch 365/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0012\n",
      "Epoch 366/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0012\n",
      "Epoch 367/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0012\n",
      "Epoch 368/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0012\n",
      "Epoch 369/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0012\n",
      "Epoch 370/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0013\n",
      "Epoch 371/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0012\n",
      "Epoch 372/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0012\n",
      "Epoch 373/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0012\n",
      "Epoch 374/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0012\n",
      "Epoch 375/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0012\n",
      "Epoch 376/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0012\n",
      "Epoch 377/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0012\n",
      "Epoch 378/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0012\n",
      "Epoch 379/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0012\n",
      "Epoch 380/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0012\n",
      "Epoch 381/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0012\n",
      "Epoch 382/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0012\n",
      "Epoch 383/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0012\n",
      "Epoch 384/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 0.0011\n",
      "Epoch 385/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0012\n",
      "Epoch 386/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0012\n",
      "Epoch 387/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0011\n",
      "Epoch 388/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0011\n",
      "Epoch 389/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0011\n",
      "Epoch 390/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0011\n",
      "Epoch 391/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0012\n",
      "Epoch 392/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0012\n",
      "Epoch 393/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0011\n",
      "Epoch 394/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0011\n",
      "Epoch 395/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0011\n",
      "Epoch 396/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0011\n",
      "Epoch 397/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0011\n",
      "Epoch 398/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0011\n",
      "Epoch 399/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0011\n",
      "Epoch 400/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0011\n",
      "Epoch 401/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0011\n",
      "Epoch 402/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0011\n",
      "Epoch 403/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0011\n",
      "Epoch 404/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0011\n",
      "Epoch 405/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0011\n",
      "Epoch 406/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0011\n",
      "Epoch 407/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0011\n",
      "Epoch 408/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0011\n",
      "Epoch 409/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0011\n",
      "Epoch 410/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0011\n",
      "Epoch 411/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0011\n",
      "Epoch 412/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0011\n",
      "Epoch 413/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0011\n",
      "Epoch 414/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0010\n",
      "Epoch 415/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0010\n",
      "Epoch 416/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0011\n",
      "Epoch 417/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0011\n",
      "Epoch 418/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0011\n",
      "Epoch 419/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0011\n",
      "Epoch 420/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0011\n",
      "Epoch 421/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0010\n",
      "Epoch 422/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0010\n",
      "Epoch 423/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0010\n",
      "Epoch 424/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0010\n",
      "Epoch 425/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 0.0010\n",
      "Epoch 426/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0010\n",
      "Epoch 427/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0010\n",
      "Epoch 428/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 0.0010\n",
      "Epoch 429/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0010\n",
      "Epoch 430/1000\n",
      "282/282 [==============================] - 0s 85us/sample - loss: 0.0010\n",
      "Epoch 431/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0010\n",
      "Epoch 432/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0010\n",
      "Epoch 433/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 9.8983e-04\n",
      "Epoch 434/1000\n",
      "282/282 [==============================] - 0s 81us/sample - loss: 9.9374e-04\n",
      "Epoch 435/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 0.0010\n",
      "Epoch 436/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0010\n",
      "Epoch 437/1000\n",
      "282/282 [==============================] - 0s 78us/sample - loss: 0.0010\n",
      "Epoch 438/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 9.7827e-04\n",
      "Epoch 439/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 9.8704e-04\n",
      "Epoch 440/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 9.9244e-04\n",
      "Epoch 441/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 0.0010\n",
      "Epoch 442/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 9.8267e-04\n",
      "Epoch 443/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 9.6844e-04\n",
      "Epoch 444/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 9.7418e-04\n",
      "Epoch 445/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 9.7423e-04\n",
      "Epoch 446/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 9.6398e-04\n",
      "Epoch 447/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 9.6059e-04\n",
      "Epoch 448/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 9.6544e-04\n",
      "Epoch 449/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 9.5673e-04\n",
      "Epoch 450/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 9.5201e-04\n",
      "Epoch 451/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 9.4641e-04\n",
      "Epoch 452/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 9.4876e-04\n",
      "Epoch 453/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 9.9382e-04\n",
      "Epoch 454/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 0.0010\n",
      "Epoch 455/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 9.8012e-04\n",
      "Epoch 456/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 9.4837e-04\n",
      "Epoch 457/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 9.3660e-04\n",
      "Epoch 458/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 9.3771e-04\n",
      "Epoch 459/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 9.3785e-04\n",
      "Epoch 460/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 9.2783e-04\n",
      "Epoch 461/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 9.5186e-04\n",
      "Epoch 462/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 9.3953e-04\n",
      "Epoch 463/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 60us/sample - loss: 9.3463e-04\n",
      "Epoch 464/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 9.2809e-04\n",
      "Epoch 465/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 9.2087e-04\n",
      "Epoch 466/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 9.2237e-04\n",
      "Epoch 467/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 9.1919e-04\n",
      "Epoch 468/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 9.1539e-04\n",
      "Epoch 469/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 9.2899e-04\n",
      "Epoch 470/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 9.2201e-04\n",
      "Epoch 471/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 9.1602e-04\n",
      "Epoch 472/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 9.2234e-04\n",
      "Epoch 473/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 9.2153e-04\n",
      "Epoch 474/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 9.0621e-04\n",
      "Epoch 475/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 9.0646e-04\n",
      "Epoch 476/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 9.0191e-04\n",
      "Epoch 477/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.9710e-04\n",
      "Epoch 478/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 9.0391e-04\n",
      "Epoch 479/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 9.1402e-04\n",
      "Epoch 480/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 9.3430e-04\n",
      "Epoch 481/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 8.9651e-04\n",
      "Epoch 482/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 9.1267e-04\n",
      "Epoch 483/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 8.9710e-04\n",
      "Epoch 484/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 8.9314e-04\n",
      "Epoch 485/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.8426e-04\n",
      "Epoch 486/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 9.1211e-04\n",
      "Epoch 487/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 8.9009e-04\n",
      "Epoch 488/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.8027e-04\n",
      "Epoch 489/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 8.8445e-04\n",
      "Epoch 490/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.9148e-04\n",
      "Epoch 491/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.7797e-04\n",
      "Epoch 492/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.7819e-04\n",
      "Epoch 493/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.8339e-04\n",
      "Epoch 494/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 8.7645e-04\n",
      "Epoch 495/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.7485e-04\n",
      "Epoch 496/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 8.7131e-04\n",
      "Epoch 497/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.8570e-04\n",
      "Epoch 498/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.8518e-04\n",
      "Epoch 499/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 9.2041e-04\n",
      "Epoch 500/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 8.8888e-04\n",
      "Epoch 501/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.9706e-04\n",
      "Epoch 502/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.7525e-04\n",
      "Epoch 503/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 9.0183e-04\n",
      "Epoch 504/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.6632e-04\n",
      "Epoch 505/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 8.5199e-04\n",
      "Epoch 506/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.5914e-04\n",
      "Epoch 507/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.5414e-04\n",
      "Epoch 508/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.6128e-04\n",
      "Epoch 509/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.5740e-04\n",
      "Epoch 510/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.5436e-04\n",
      "Epoch 511/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 8.4774e-04\n",
      "Epoch 512/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.6135e-04\n",
      "Epoch 513/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.4965e-04\n",
      "Epoch 514/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.4688e-04\n",
      "Epoch 515/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.6040e-04\n",
      "Epoch 516/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.4908e-04\n",
      "Epoch 517/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.4036e-04\n",
      "Epoch 518/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.5241e-04\n",
      "Epoch 519/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.4206e-04\n",
      "Epoch 520/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.4780e-04\n",
      "Epoch 521/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.3997e-04\n",
      "Epoch 522/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.3326e-04\n",
      "Epoch 523/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.3488e-04\n",
      "Epoch 524/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.3155e-04\n",
      "Epoch 525/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 8.5252e-04\n",
      "Epoch 526/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.6746e-04\n",
      "Epoch 527/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.6178e-04\n",
      "Epoch 528/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.2731e-04\n",
      "Epoch 529/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.3203e-04\n",
      "Epoch 530/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.4155e-04\n",
      "Epoch 531/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.3808e-04\n",
      "Epoch 532/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.3616e-04\n",
      "Epoch 533/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.2923e-04\n",
      "Epoch 534/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 8.3135e-04\n",
      "Epoch 535/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 8.2261e-04\n",
      "Epoch 536/1000\n",
      "282/282 [==============================] - 0s 84us/sample - loss: 8.2558e-04\n",
      "Epoch 537/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 8.2600e-04\n",
      "Epoch 538/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.3241e-04\n",
      "Epoch 539/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 8.3104e-04\n",
      "Epoch 540/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.2374e-04\n",
      "Epoch 541/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.4296e-04\n",
      "Epoch 542/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 8.3214e-04\n",
      "Epoch 543/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 8.3341e-04\n",
      "Epoch 544/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.3125e-04\n",
      "Epoch 545/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.2836e-04\n",
      "Epoch 546/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.1849e-04\n",
      "Epoch 547/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.1061e-04\n",
      "Epoch 548/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.2404e-04\n",
      "Epoch 549/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.3429e-04\n",
      "Epoch 550/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 8.2082e-04\n",
      "Epoch 551/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 8.2076e-04\n",
      "Epoch 552/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 120us/sample - loss: 8.1575e-04\n",
      "Epoch 553/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 8.0317e-04\n",
      "Epoch 554/1000\n",
      "282/282 [==============================] - 0s 53us/sample - loss: 8.0638e-04\n",
      "Epoch 555/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.1684e-04\n",
      "Epoch 556/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.0131e-04\n",
      "Epoch 557/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 8.0154e-04\n",
      "Epoch 558/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.9679e-04\n",
      "Epoch 559/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.0504e-04\n",
      "Epoch 560/1000\n",
      "282/282 [==============================] - 0s 78us/sample - loss: 8.0157e-04\n",
      "Epoch 561/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.0484e-04\n",
      "Epoch 562/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.9549e-04\n",
      "Epoch 563/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 8.1711e-04\n",
      "Epoch 564/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 8.1309e-04\n",
      "Epoch 565/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.0845e-04\n",
      "Epoch 566/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.2935e-04\n",
      "Epoch 567/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 8.0330e-04\n",
      "Epoch 568/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.1329e-04\n",
      "Epoch 569/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.0764e-04\n",
      "Epoch 570/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.1436e-04\n",
      "Epoch 571/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 7.8621e-04\n",
      "Epoch 572/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.9743e-04\n",
      "Epoch 573/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.9319e-04\n",
      "Epoch 574/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.9871e-04\n",
      "Epoch 575/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.6366e-04\n",
      "Epoch 576/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 7.9548e-04\n",
      "Epoch 577/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.8485e-04\n",
      "Epoch 578/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.9728e-04\n",
      "Epoch 579/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 7.9368e-04\n",
      "Epoch 580/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.9459e-04\n",
      "Epoch 581/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.8604e-04\n",
      "Epoch 582/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.9569e-04\n",
      "Epoch 583/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 7.8282e-04\n",
      "Epoch 584/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.8232e-04\n",
      "Epoch 585/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.4593e-04\n",
      "Epoch 586/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 9.4879e-04\n",
      "Epoch 587/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.4006e-04\n",
      "Epoch 588/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 8.6138e-04\n",
      "Epoch 589/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.2755e-04\n",
      "Epoch 590/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.7920e-04\n",
      "Epoch 591/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.7483e-04\n",
      "Epoch 592/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.7685e-04\n",
      "Epoch 593/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.7362e-04\n",
      "Epoch 594/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 7.7997e-04\n",
      "Epoch 595/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 8.0189e-04\n",
      "Epoch 596/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.7519e-04\n",
      "Epoch 597/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.7446e-04\n",
      "Epoch 598/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.8890e-04\n",
      "Epoch 599/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 8.1974e-04\n",
      "Epoch 600/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.8214e-04\n",
      "Epoch 601/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.7805e-04\n",
      "Epoch 602/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.9892e-04\n",
      "Epoch 603/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 7.7363e-04\n",
      "Epoch 604/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 7.7129e-04\n",
      "Epoch 605/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.7054e-04\n",
      "Epoch 606/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.6489e-04\n",
      "Epoch 607/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.6362e-04\n",
      "Epoch 608/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.7199e-04\n",
      "Epoch 609/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.7682e-04\n",
      "Epoch 610/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.6211e-04\n",
      "Epoch 611/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.6544e-04\n",
      "Epoch 612/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.6602e-04\n",
      "Epoch 613/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.5963e-04\n",
      "Epoch 614/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.5973e-04\n",
      "Epoch 615/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.8090e-04\n",
      "Epoch 616/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.6751e-04\n",
      "Epoch 617/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.7157e-04\n",
      "Epoch 618/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 8.0025e-04\n",
      "Epoch 619/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 7.6257e-04\n",
      "Epoch 620/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.6707e-04\n",
      "Epoch 621/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.9198e-04\n",
      "Epoch 622/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.8858e-04\n",
      "Epoch 623/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.8192e-04\n",
      "Epoch 624/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.9970e-04\n",
      "Epoch 625/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 8.0743e-04\n",
      "Epoch 626/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.5847e-04\n",
      "Epoch 627/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.5995e-04\n",
      "Epoch 628/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.6652e-04\n",
      "Epoch 629/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.8611e-04\n",
      "Epoch 630/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.6800e-04\n",
      "Epoch 631/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.6354e-04\n",
      "Epoch 632/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.7014e-04\n",
      "Epoch 633/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.5750e-04\n",
      "Epoch 634/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.5509e-04\n",
      "Epoch 635/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.4313e-04\n",
      "Epoch 636/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.5957e-04\n",
      "Epoch 637/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 7.5021e-04\n",
      "Epoch 638/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.5821e-04\n",
      "Epoch 639/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.4254e-04\n",
      "Epoch 640/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.4888e-04\n",
      "Epoch 641/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 57us/sample - loss: 7.4624e-04\n",
      "Epoch 642/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.4493e-04\n",
      "Epoch 643/1000\n",
      "282/282 [==============================] - 0s 81us/sample - loss: 7.4615e-04\n",
      "Epoch 644/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.4370e-04\n",
      "Epoch 645/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.4083e-04\n",
      "Epoch 646/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.4401e-04\n",
      "Epoch 647/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.6130e-04\n",
      "Epoch 648/1000\n",
      "282/282 [==============================] - 0s 88us/sample - loss: 7.4967e-04\n",
      "Epoch 649/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.8920e-04\n",
      "Epoch 650/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.6874e-04\n",
      "Epoch 651/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.5964e-04\n",
      "Epoch 652/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.5005e-04\n",
      "Epoch 653/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.4311e-04\n",
      "Epoch 654/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 7.3829e-04\n",
      "Epoch 655/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.3843e-04\n",
      "Epoch 656/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 7.6028e-04\n",
      "Epoch 657/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 8.4408e-04\n",
      "Epoch 658/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.6694e-04\n",
      "Epoch 659/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 7.4417e-04\n",
      "Epoch 660/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 7.3364e-04\n",
      "Epoch 661/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.4433e-04\n",
      "Epoch 662/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.3141e-04\n",
      "Epoch 663/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.3731e-04\n",
      "Epoch 664/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.3579e-04\n",
      "Epoch 665/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.3244e-04\n",
      "Epoch 666/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.3179e-04\n",
      "Epoch 667/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.4059e-04\n",
      "Epoch 668/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.3803e-04\n",
      "Epoch 669/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.3651e-04\n",
      "Epoch 670/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.3791e-04\n",
      "Epoch 671/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 7.2929e-04\n",
      "Epoch 672/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.2943e-04\n",
      "Epoch 673/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.3854e-04\n",
      "Epoch 674/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.3661e-04\n",
      "Epoch 675/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.3291e-04\n",
      "Epoch 676/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.6329e-04\n",
      "Epoch 677/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.3790e-04\n",
      "Epoch 678/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.3853e-04\n",
      "Epoch 679/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.6362e-04\n",
      "Epoch 680/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.3289e-04\n",
      "Epoch 681/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.3187e-04\n",
      "Epoch 682/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.3319e-04\n",
      "Epoch 683/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.8933e-04\n",
      "Epoch 684/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.4553e-04\n",
      "Epoch 685/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.2450e-04\n",
      "Epoch 686/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.2982e-04\n",
      "Epoch 687/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.3510e-04\n",
      "Epoch 688/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.3014e-04\n",
      "Epoch 689/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.2765e-04\n",
      "Epoch 690/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.5022e-04\n",
      "Epoch 691/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.2194e-04\n",
      "Epoch 692/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.2567e-04\n",
      "Epoch 693/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.2404e-04\n",
      "Epoch 694/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.3909e-04\n",
      "Epoch 695/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.4646e-04\n",
      "Epoch 696/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.2314e-04\n",
      "Epoch 697/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.3430e-04\n",
      "Epoch 698/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.1686e-04\n",
      "Epoch 699/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 7.2043e-04\n",
      "Epoch 700/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 7.1972e-04\n",
      "Epoch 701/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.3305e-04\n",
      "Epoch 702/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.1688e-04\n",
      "Epoch 703/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.1807e-04\n",
      "Epoch 704/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.2845e-04\n",
      "Epoch 705/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.2651e-04\n",
      "Epoch 706/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.2182e-04\n",
      "Epoch 707/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.1390e-04\n",
      "Epoch 708/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.2459e-04\n",
      "Epoch 709/1000\n",
      "282/282 [==============================] - ETA: 0s - loss: 6.1827e-0 - 0s 64us/sample - loss: 7.3309e-04\n",
      "Epoch 710/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.3102e-04\n",
      "Epoch 711/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.3129e-04\n",
      "Epoch 712/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.1813e-04\n",
      "Epoch 713/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.1077e-04\n",
      "Epoch 714/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.3366e-04\n",
      "Epoch 715/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.3479e-04\n",
      "Epoch 716/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.3718e-04\n",
      "Epoch 717/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.6651e-04\n",
      "Epoch 718/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 7.2970e-04\n",
      "Epoch 719/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.8732e-04\n",
      "Epoch 720/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.1394e-04\n",
      "Epoch 721/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.0437e-04\n",
      "Epoch 722/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.1527e-04\n",
      "Epoch 723/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.2472e-04\n",
      "Epoch 724/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.3024e-04\n",
      "Epoch 725/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0570e-04\n",
      "Epoch 726/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 7.1621e-04\n",
      "Epoch 727/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.3614e-04\n",
      "Epoch 728/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.3753e-04\n",
      "Epoch 729/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.2447e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 730/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.1731e-04\n",
      "Epoch 731/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.2159e-04\n",
      "Epoch 732/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0906e-04\n",
      "Epoch 733/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.2581e-04\n",
      "Epoch 734/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.1431e-04\n",
      "Epoch 735/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0536e-04\n",
      "Epoch 736/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.2036e-04\n",
      "Epoch 737/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.3239e-04\n",
      "Epoch 738/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.0576e-04\n",
      "Epoch 739/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0513e-04\n",
      "Epoch 740/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.1451e-04\n",
      "Epoch 741/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0432e-04\n",
      "Epoch 742/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.5247e-04\n",
      "Epoch 743/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.1200e-04\n",
      "Epoch 744/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.1181e-04\n",
      "Epoch 745/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 7.1708e-04\n",
      "Epoch 746/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 7.1053e-04\n",
      "Epoch 747/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.1635e-04\n",
      "Epoch 748/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.1679e-04\n",
      "Epoch 749/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0172e-04\n",
      "Epoch 750/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.0157e-04\n",
      "Epoch 751/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.1248e-04\n",
      "Epoch 752/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 7.0863e-04\n",
      "Epoch 753/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 7.4771e-04\n",
      "Epoch 754/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0502e-04\n",
      "Epoch 755/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.0946e-04\n",
      "Epoch 756/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.9810e-04\n",
      "Epoch 757/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.9543e-04\n",
      "Epoch 758/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.0325e-04\n",
      "Epoch 759/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.1846e-04\n",
      "Epoch 760/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.1578e-04\n",
      "Epoch 761/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.3063e-04\n",
      "Epoch 762/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.3192e-04\n",
      "Epoch 763/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.4700e-04\n",
      "Epoch 764/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.2576e-04\n",
      "Epoch 765/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0476e-04\n",
      "Epoch 766/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.9542e-04\n",
      "Epoch 767/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.0076e-04\n",
      "Epoch 768/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 7.1956e-04\n",
      "Epoch 769/1000\n",
      "282/282 [==============================] - 0s 78us/sample - loss: 7.0407e-04\n",
      "Epoch 770/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 7.1524e-04\n",
      "Epoch 771/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0862e-04\n",
      "Epoch 772/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0092e-04\n",
      "Epoch 773/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0267e-04\n",
      "Epoch 774/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.9836e-04\n",
      "Epoch 775/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0218e-04\n",
      "Epoch 776/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.9013e-04\n",
      "Epoch 777/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.9113e-04\n",
      "Epoch 778/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8957e-04\n",
      "Epoch 779/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.0870e-04\n",
      "Epoch 780/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 6.9033e-04\n",
      "Epoch 781/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 6.9554e-04\n",
      "Epoch 782/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.9188e-04\n",
      "Epoch 783/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.0725e-04\n",
      "Epoch 784/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8847e-04\n",
      "Epoch 785/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 6.9146e-04\n",
      "Epoch 786/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.9485e-04\n",
      "Epoch 787/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.8913e-04\n",
      "Epoch 788/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 7.1361e-04\n",
      "Epoch 789/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0323e-04\n",
      "Epoch 790/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0053e-04\n",
      "Epoch 791/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0091e-04\n",
      "Epoch 792/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.9184e-04\n",
      "Epoch 793/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8437e-04\n",
      "Epoch 794/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.9648e-04\n",
      "Epoch 795/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.3361e-04\n",
      "Epoch 796/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0005e-04\n",
      "Epoch 797/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8232e-04\n",
      "Epoch 798/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8592e-04\n",
      "Epoch 799/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.8833e-04\n",
      "Epoch 800/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.9740e-04\n",
      "Epoch 801/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8924e-04\n",
      "Epoch 802/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.9553e-04\n",
      "Epoch 803/1000\n",
      "282/282 [==============================] - ETA: 0s - loss: 5.7008e-0 - 0s 64us/sample - loss: 6.9084e-04\n",
      "Epoch 804/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.2223e-04\n",
      "Epoch 805/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 7.4315e-04\n",
      "Epoch 806/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 7.0813e-04\n",
      "Epoch 807/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8405e-04\n",
      "Epoch 808/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.9656e-04\n",
      "Epoch 809/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0442e-04\n",
      "Epoch 810/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 7.2006e-04\n",
      "Epoch 811/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.9604e-04\n",
      "Epoch 812/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 6.8081e-04\n",
      "Epoch 813/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 6.8148e-04\n",
      "Epoch 814/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8091e-04\n",
      "Epoch 815/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.8337e-04\n",
      "Epoch 816/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.0947e-04\n",
      "Epoch 817/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8562e-04\n",
      "Epoch 818/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8948e-04\n",
      "Epoch 819/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0181e-04\n",
      "Epoch 820/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 7.0178e-04\n",
      "Epoch 821/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0529e-04\n",
      "Epoch 822/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 7.0715e-04\n",
      "Epoch 823/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.7801e-04\n",
      "Epoch 824/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.8200e-04\n",
      "Epoch 825/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 6.8284e-04\n",
      "Epoch 826/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.8001e-04\n",
      "Epoch 827/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.9557e-04\n",
      "Epoch 828/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0100e-04\n",
      "Epoch 829/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.9353e-04\n",
      "Epoch 830/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.9028e-04\n",
      "Epoch 831/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8670e-04\n",
      "Epoch 832/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.0287e-04\n",
      "Epoch 833/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8744e-04\n",
      "Epoch 834/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.7983e-04\n",
      "Epoch 835/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0622e-04\n",
      "Epoch 836/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.8904e-04\n",
      "Epoch 837/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 6.9003e-04\n",
      "Epoch 838/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.7903e-04\n",
      "Epoch 839/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 6.8047e-04\n",
      "Epoch 840/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8538e-04\n",
      "Epoch 841/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.8473e-04\n",
      "Epoch 842/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8960e-04\n",
      "Epoch 843/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.9112e-04\n",
      "Epoch 844/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.8968e-04\n",
      "Epoch 845/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.8215e-04\n",
      "Epoch 846/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0073e-04\n",
      "Epoch 847/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.0608e-04\n",
      "Epoch 848/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.1440e-04\n",
      "Epoch 849/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0898e-04\n",
      "Epoch 850/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.8475e-04\n",
      "Epoch 851/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.7634e-04\n",
      "Epoch 852/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.8697e-04\n",
      "Epoch 853/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8692e-04\n",
      "Epoch 854/1000\n",
      "282/282 [==============================] - 0s 81us/sample - loss: 6.7851e-04\n",
      "Epoch 855/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8851e-04\n",
      "Epoch 856/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.7963e-04\n",
      "Epoch 857/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 6.7961e-04\n",
      "Epoch 858/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.9210e-04\n",
      "Epoch 859/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.2332e-04\n",
      "Epoch 860/1000\n",
      "282/282 [==============================] - 0s 68us/sample - loss: 7.0482e-04\n",
      "Epoch 861/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.7418e-04\n",
      "Epoch 862/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.7978e-04\n",
      "Epoch 863/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.6730e-04\n",
      "Epoch 864/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.7446e-04\n",
      "Epoch 865/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.9338e-04\n",
      "Epoch 866/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.4229e-04\n",
      "Epoch 867/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.9000e-04\n",
      "Epoch 868/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.9330e-04\n",
      "Epoch 869/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.7317e-04\n",
      "Epoch 870/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.7669e-04\n",
      "Epoch 871/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.7776e-04\n",
      "Epoch 872/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.7088e-04\n",
      "Epoch 873/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8453e-04\n",
      "Epoch 874/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.7269e-04\n",
      "Epoch 875/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.6603e-04\n",
      "Epoch 876/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.7577e-04\n",
      "Epoch 877/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.6556e-04\n",
      "Epoch 878/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 6.7177e-04\n",
      "Epoch 879/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.8872e-04\n",
      "Epoch 880/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0570e-04\n",
      "Epoch 881/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.1482e-04\n",
      "Epoch 882/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 6.6968e-04\n",
      "Epoch 883/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.8355e-04\n",
      "Epoch 884/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.9366e-04\n",
      "Epoch 885/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8826e-04\n",
      "Epoch 886/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.1469e-04\n",
      "Epoch 887/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.8441e-04\n",
      "Epoch 888/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 6.7646e-04\n",
      "Epoch 889/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8890e-04\n",
      "Epoch 890/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.0358e-04\n",
      "Epoch 891/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.1444e-04\n",
      "Epoch 892/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.5831e-04\n",
      "Epoch 893/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.7928e-04\n",
      "Epoch 894/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.7454e-04\n",
      "Epoch 895/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.9465e-04\n",
      "Epoch 896/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.7300e-04\n",
      "Epoch 897/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.6918e-04\n",
      "Epoch 898/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.7925e-04\n",
      "Epoch 899/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.7952e-04\n",
      "Epoch 900/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.6827e-04\n",
      "Epoch 901/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8020e-04\n",
      "Epoch 902/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.6011e-04\n",
      "Epoch 903/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8122e-04\n",
      "Epoch 904/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8162e-04\n",
      "Epoch 905/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.7160e-04\n",
      "Epoch 906/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.6965e-04\n",
      "Epoch 907/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 67us/sample - loss: 6.8501e-04\n",
      "Epoch 908/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 6.7833e-04\n",
      "Epoch 909/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.6801e-04\n",
      "Epoch 910/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.8172e-04\n",
      "Epoch 911/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.7178e-04\n",
      "Epoch 912/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.9612e-04\n",
      "Epoch 913/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.7218e-04\n",
      "Epoch 914/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.6999e-04\n",
      "Epoch 915/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.8286e-04\n",
      "Epoch 916/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.9378e-04\n",
      "Epoch 917/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.9001e-04\n",
      "Epoch 918/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0139e-04\n",
      "Epoch 919/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.6584e-04\n",
      "Epoch 920/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.7523e-04\n",
      "Epoch 921/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8624e-04\n",
      "Epoch 922/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.7083e-04\n",
      "Epoch 923/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.7565e-04\n",
      "Epoch 924/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.8009e-04\n",
      "Epoch 925/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.6785e-04\n",
      "Epoch 926/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.6311e-04\n",
      "Epoch 927/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.6602e-04\n",
      "Epoch 928/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.6688e-04\n",
      "Epoch 929/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.6144e-04\n",
      "Epoch 930/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.7364e-04\n",
      "Epoch 931/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.9342e-04\n",
      "Epoch 932/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 6.6927e-04\n",
      "Epoch 933/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 6.7576e-04\n",
      "Epoch 934/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.6894e-04\n",
      "Epoch 935/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.0360e-04\n",
      "Epoch 936/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.2308e-04\n",
      "Epoch 937/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.7713e-04\n",
      "Epoch 938/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.3997e-04\n",
      "Epoch 939/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.7916e-04\n",
      "Epoch 940/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.8042e-04\n",
      "Epoch 941/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.6107e-04\n",
      "Epoch 942/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.6636e-04\n",
      "Epoch 943/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.6553e-04\n",
      "Epoch 944/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.7332e-04\n",
      "Epoch 945/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 6.7318e-04\n",
      "Epoch 946/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 6.5788e-04\n",
      "Epoch 947/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.8880e-04\n",
      "Epoch 948/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.7552e-04\n",
      "Epoch 949/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.6962e-04\n",
      "Epoch 950/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.6321e-04\n",
      "Epoch 951/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.5561e-04\n",
      "Epoch 952/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.5653e-04\n",
      "Epoch 953/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.5849e-04\n",
      "Epoch 954/1000\n",
      "282/282 [==============================] - 0s 152us/sample - loss: 6.8363e-04\n",
      "Epoch 955/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.6150e-04\n",
      "Epoch 956/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.6224e-04\n",
      "Epoch 957/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.5925e-04\n",
      "Epoch 958/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.6640e-04\n",
      "Epoch 959/1000\n",
      "282/282 [==============================] - 0s 57us/sample - loss: 6.5421e-04\n",
      "Epoch 960/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.5833e-04\n",
      "Epoch 961/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.7275e-04\n",
      "Epoch 962/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.5868e-04\n",
      "Epoch 963/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.0222e-04\n",
      "Epoch 964/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 7.1181e-04\n",
      "Epoch 965/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.9243e-04\n",
      "Epoch 966/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8918e-04\n",
      "Epoch 967/1000\n",
      "282/282 [==============================] - 0s 85us/sample - loss: 6.6160e-04\n",
      "Epoch 968/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 7.0170e-04\n",
      "Epoch 969/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8772e-04\n",
      "Epoch 970/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.6736e-04\n",
      "Epoch 971/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.5904e-04\n",
      "Epoch 972/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.7033e-04\n",
      "Epoch 973/1000\n",
      "282/282 [==============================] - 0s 85us/sample - loss: 6.8521e-04\n",
      "Epoch 974/1000\n",
      "282/282 [==============================] - 0s 81us/sample - loss: 6.8379e-04\n",
      "Epoch 975/1000\n",
      "282/282 [==============================] - 0s 74us/sample - loss: 6.5431e-04\n",
      "Epoch 976/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.7344e-04\n",
      "Epoch 977/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.4944e-04\n",
      "Epoch 978/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.6490e-04\n",
      "Epoch 979/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.5295e-04\n",
      "Epoch 980/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.5006e-04\n",
      "Epoch 981/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.7659e-04\n",
      "Epoch 982/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.4331e-04\n",
      "Epoch 983/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 6.6969e-04\n",
      "Epoch 984/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.8406e-04\n",
      "Epoch 985/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 7.0506e-04\n",
      "Epoch 986/1000\n",
      "282/282 [==============================] - 0s 71us/sample - loss: 6.6019e-04\n",
      "Epoch 987/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.6578e-04\n",
      "Epoch 988/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.6335e-04\n",
      "Epoch 989/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.6749e-04\n",
      "Epoch 990/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 7.0398e-04\n",
      "Epoch 991/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.8076e-04\n",
      "Epoch 992/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.5781e-04\n",
      "Epoch 993/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.8451e-04\n",
      "Epoch 994/1000\n",
      "282/282 [==============================] - 0s 64us/sample - loss: 6.5184e-04\n",
      "Epoch 995/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.7437e-04\n",
      "Epoch 996/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282/282 [==============================] - 0s 64us/sample - loss: 6.5993e-04\n",
      "Epoch 997/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.4804e-04\n",
      "Epoch 998/1000\n",
      "282/282 [==============================] - 0s 67us/sample - loss: 6.5086e-04\n",
      "Epoch 999/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.5369e-04\n",
      "Epoch 1000/1000\n",
      "282/282 [==============================] - 0s 60us/sample - loss: 6.6777e-04\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "time_distributed_33 (TimeDis multiple                  84        \n",
      "_________________________________________________________________\n",
      "time_distributed_34 (TimeDis multiple                  0         \n",
      "_________________________________________________________________\n",
      "time_distributed_35 (TimeDis multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             multiple                  49        \n",
      "=================================================================\n",
      "Total params: 133\n",
      "Trainable params: 133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "\n",
      "==========================\n",
      "MAE= 2.256359354654948\n",
      "RMSE =  2.8167740864154927\n",
      "NMSE =  0.005223891173017137\n",
      "MAPE= 0.010577384588638475\n",
      "IA= 0.9768124217578495\n",
      "U1= 0.006642466743664433\n",
      "U2= 0.49324114883587944\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running time: 19 seconds\n"
     ]
    }
   ],
   "source": [
    "#CNN-SED\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.python.keras.layers.convolutional import Conv1D\n",
    "from tensorflow.python.keras.layers.convolutional import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#matplotlib inline\n",
    "import time\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_DJH.csv', engine='python')\n",
    "#print(dataframe)\n",
    "print(\"数据集的长度：\",len(dataframe))\n",
    "dataset = dataframe.values\n",
    "# 将整型变为float\n",
    "dataset = dataset.astype('float32')\n",
    "#print(dataset)\n",
    "\n",
    "timestep = 6\n",
    "dim = 2\n",
    "#数据缩放 拆分输入X（7维）&输出Y（1维）\n",
    "X = dataset[:,0:2]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "for i in range(dim):\n",
    "    Xdata = X[:,i]\n",
    "    Xdata = Xdata.reshape(-1,1)\n",
    "    Xdata = scaler.fit_transform(Xdata)\n",
    "    Xdata = Xdata.flatten()\n",
    "    X[:,i] = Xdata\n",
    "X_scaler = X.reshape(324,-1)\n",
    "\n",
    "Y = dataset[:,0]\n",
    "Y_scaler= (Y-np.min(Y))/(np.max(Y)-np.min(Y))\n",
    "Y_scaler = Y_scaler.reshape(-1)\n",
    "\n",
    "\n",
    "# 将数据拆分成训练和测试，7/9作为训练数据\n",
    "train_size = int(len(dataset) * 0.89)\n",
    "test_size = len(dataset) - train_size\n",
    "trainX, testX = X_scaler[0:train_size,:], X_scaler[train_size:len(dataset),:]\n",
    "trainY, testY = Y_scaler[0:train_size], Y_scaler[train_size:len(dataset)]\n",
    "print(\"原始训练集的长度：\",train_size)\n",
    "print(\"原始测试集的长度：\",test_size)\n",
    "#print(trainX)\n",
    "#print(trainY)\n",
    "#print(testX)\n",
    "#print(testY)\n",
    "\n",
    "\n",
    "\n",
    "#重构数据集\n",
    "##timestep为时间步长\n",
    "def create_trainX(seq, timestep):\n",
    "    dataX = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[i:(i+timestep)]\n",
    "        # X按照顺序取值 每次在后面增加一个数据\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "\n",
    "def create_trainY(seq, timestep):\n",
    "    dataY = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        # Y向后移动一位取值\n",
    "        dataY.append(seq[i + timestep])\n",
    "    return np.array(dataY)\n",
    "\n",
    "def create_testX(seq, timestep):\n",
    "    dataX = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[(i):(i+timestep)]\n",
    "        # X按照顺序取值 每次在后面增加一个数据\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "\n",
    "\n",
    "\n",
    "#----------forecasting-----------\n",
    "pres=[]\n",
    "trainX = create_trainX(trainX, timestep)\n",
    "trainY = create_trainY(trainY, timestep)\n",
    "testX = create_testX(testX, timestep) \n",
    "testY = testY[timestep:len(testY)]\n",
    "print(\"转为监督学习，训练集数据长度：\", len(trainX))\n",
    "#print(trainX,trainY)\n",
    "print(\"转为监督学习，测试集数据长度：\",len(testX))\n",
    "#print(testX, testY )\n",
    "\n",
    "\n",
    "# 数据重构为4D [samples, subsequences, timesteps, features]\n",
    "trainX_input4D = np.reshape(trainX, (trainX.shape[0],1,timestep,dim))\n",
    "testX_input4D = np.reshape(testX, (testX.shape[0],1,timestep,dim))\n",
    "print('构造得到模型的输入数据(训练数据已有标签trainY): ',trainX_input4D.shape,testX_input4D.shape)\n",
    "\n",
    "\n",
    "# create and fit the convlstm network\n",
    "if __name__ == '__main__':\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=12, kernel_size=3, activation='relu', input_shape=(None,1, testX.shape[1]))))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=1)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "    #model.add(LSTM(4,activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.fit(trainX_input4D, trainY, epochs=1000)\n",
    "    \n",
    "\n",
    "\n",
    "    # 打印模型\n",
    "    model.summary()\n",
    "\n",
    "    # 开始预测\n",
    "    trainPredict = model.predict(trainX_input4D)\n",
    "    testPredict = model.predict(testX_input4D)\n",
    "\n",
    "    # 逆缩放预测值\n",
    "    dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD.csv', engine='python')\n",
    "    dataset = dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "    Y = dataset[:,0]\n",
    "    #trainPredict = trainPredict*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori = trainY*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori=trainY_re.reshape((246,1))\n",
    "    testPredict = testPredict*((np.max(Y)-np.min(Y)))+np.min(Y)\n",
    "    testPre = testPredict.reshape(-1)\n",
    "    Y = dataset[:,0]\n",
    "    Y = Y[train_size+timestep:len(dataset)]\n",
    "    #testY_ori = testY*((np.max(dataset)-np.min(dataset)))+np.min(dataset)\n",
    "    #testY_ori = testY_ori.reshape((-1,1))\n",
    "    testY_ori = Y.reshape(-1)\n",
    "\n",
    "    \n",
    "\n",
    "    # 计算误差\n",
    "    #trainScore = math.sqrt(mean_squared_error(trainY_ori[:,0], trainPredict[:,0]))\n",
    "    #print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    #testScore = math.sqrt(mean_squared_error(testY_ori[:,0], testPredict[:,0]))\n",
    "    #print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "\n",
    "    error = []#Y-Y'\n",
    "    error1= []#abs((Y-Y')/Y)\n",
    "    error2= []#Y*Y'\n",
    "    squared1 =[]#Y*Y\n",
    "    squared2 =[]#Y'*Y'\n",
    "    for i in range(len(testY_ori)):\n",
    "        error.append(testY_ori[i] - testPre[i])\n",
    "        error1.append(abs((testY_ori[i] - testPre[i])/testY_ori[i]))\n",
    "        error2.append(testY_ori[i]*testPre[i])\n",
    "        squared1.append(testY_ori[i]*testY_ori[i])\n",
    "        squared2.append(testPre[i]*testPre[i])\n",
    "        \n",
    "        \n",
    "    squaredError = []#(Y-Y')^2\n",
    "    absError = []#abs(Y-Y')\n",
    "    for val in error:    \n",
    "        squaredError.append(val * val)#target-prediction之差平方     \n",
    "        absError.append(abs(val))#误差绝对值\n",
    "        \n",
    "    MSE=sum(squaredError) / len(squaredError)\n",
    "    meannn=np.mean(testY_ori)\n",
    "    NMSEerror = []\n",
    "    IAerror = []\n",
    "    for i in range(len(testY_ori)):\n",
    "        NMSEerror.append(squaredError[i]/error2[i])\n",
    "        IAerror.append((abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn))*(abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn)))\n",
    "    \n",
    "    U2error1 = []\n",
    "    U2error2 = []\n",
    "    for i in range(len(testY_ori)-1):\n",
    "        U2error1.append(((testY_ori[i+1] - testPre[i+1])/testY_ori[i])*((testY_ori[i+1] - testPre[i+1])/testY_ori[i]))\n",
    "        U2error2.append(((testY_ori[i+1] - testPre[i])/testY_ori[i])*((testY_ori[i+1] - testPre[i])/testY_ori[i]))\n",
    "    print('\\n==========================')\n",
    "    MAE=sum(absError)/len(absError)\n",
    "    print('MAE=',MAE)\n",
    "    from math import sqrt\n",
    "    RMSE=sqrt(MSE)\n",
    "    print(\"RMSE = \", RMSE)#均方根误差RMSE\n",
    "    NMSE=sum(NMSEerror)\n",
    "    print(\"NMSE = \", NMSE)#误差平方的归一化平均值NMSE\n",
    "    MAPE=sum(error1)/len(error1)\n",
    "    print('MAPE=',MAPE)\n",
    "    IA=1-sum(squaredError)/sum(IAerror)\n",
    "    print('IA=',IA)#一致性指数\n",
    "    U1index=RMSE/(sqrt(sum(squared1)/len(squared1))+sqrt(sum(squared2)/len(squared2)))\n",
    "    print('U1=',U1index)\n",
    "    U2index=(sqrt(sum(U2error1)/len(testY_ori)))/(sqrt(sum(U2error2)/len(testY_ori)))\n",
    "    print('U2=',U2index)\n",
    "\n",
    "    #print(testPredict)\n",
    "    testPre=testPre.reshape(-1)\n",
    "    testPre=pd.DataFrame(testPre)\n",
    "    testPre.to_csv('C:/Users/Administrator/Desktop/XI-2/data/pre_result/cnnoutput.csv', header=False)\n",
    "    import csv\n",
    "    Evaluation_index = [MAE,RMSE,NMSE,MAPE,IA,U1index,U2index]\n",
    "    with open(\"C:/Users/Administrator/Desktop/XI-2/data/pre_result/cnnEvaluation.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file ,delimiter=',')\n",
    "        writer.writerow(Evaluation_index)\n",
    "    \n",
    "    plt.figure(facecolor='white')\n",
    "    plt.plot(testPre,color='green', label='Predict')\n",
    "    plt.plot(testY_ori,color='pink', label='Original')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    pres.append(testPre)\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))\n",
    "\n",
    "#CNN-SED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251e2b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN-LSTM-SED\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import TimeDistributed\n",
    "from tensorflow.python.keras.layers.convolutional import Conv1D\n",
    "from tensorflow.python.keras.layers.convolutional import MaxPooling1D\n",
    "from tensorflow.python.keras.layers import Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#matplotlib inline\n",
    "import time\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "# load the dataset\n",
    "dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_DJH.csv', engine='python')\n",
    "#print(dataframe)\n",
    "print(\"数据集的长度：\",len(dataframe))\n",
    "dataset = dataframe.values\n",
    "# 将整型变为float\n",
    "dataset = dataset.astype('float32')\n",
    "#print(dataset)\n",
    "\n",
    "timestep = 6\n",
    "dim = 2\n",
    "#数据缩放 拆分输入X（7维）&输出Y（1维）\n",
    "X = dataset[:,0:2]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "for i in range(dim):\n",
    "    Xdata = X[:,i]\n",
    "    Xdata = Xdata.reshape(-1,1)\n",
    "    Xdata = scaler.fit_transform(Xdata)\n",
    "    Xdata = Xdata.flatten()\n",
    "    X[:,i] = Xdata\n",
    "X_scaler = X.reshape(324,-1)\n",
    "\n",
    "Y = dataset[:,0]\n",
    "Y_scaler= (Y-np.min(Y))/(np.max(Y)-np.min(Y))\n",
    "Y_scaler = Y_scaler.reshape(-1)\n",
    "\n",
    "\n",
    "# 将数据拆分成训练和测试，7/9作为训练数据\n",
    "train_size = int(len(dataset) * 0.89)\n",
    "test_size = len(dataset) - train_size\n",
    "trainX, testX = X_scaler[0:train_size,:], X_scaler[train_size:len(dataset),:]\n",
    "trainY, testY = Y_scaler[0:train_size], Y_scaler[train_size:len(dataset)]\n",
    "print(\"原始训练集的长度：\",train_size)\n",
    "print(\"原始测试集的长度：\",test_size)\n",
    "#print(trainX)\n",
    "#print(trainY)\n",
    "#print(testX)\n",
    "#print(testY)\n",
    "\n",
    "\n",
    "\n",
    "#重构数据集\n",
    "##timestep为时间步长\n",
    "def create_trainX(seq, timestep):\n",
    "    dataX = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[i:(i+timestep)]\n",
    "        # X按照顺序取值 每次在后面增加一个数据\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "\n",
    "def create_trainY(seq, timestep):\n",
    "    dataY = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        # Y向后移动一位取值\n",
    "        dataY.append(seq[i + timestep])\n",
    "    return np.array(dataY)\n",
    "\n",
    "def create_testX(seq, timestep):\n",
    "    dataX = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[(i):(i+timestep)]\n",
    "        # X按照顺序取值 每次在后面增加一个数据\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "\n",
    "\n",
    "\n",
    "#----------forecasting-----------\n",
    "pres=[]\n",
    "trainX = create_trainX(trainX, timestep)\n",
    "trainY = create_trainY(trainY, timestep)\n",
    "testX = create_testX(testX, timestep) \n",
    "testY = testY[timestep:len(testY)]\n",
    "print(\"转为监督学习，训练集数据长度：\", len(trainX))\n",
    "#print(trainX,trainY)\n",
    "print(\"转为监督学习，测试集数据长度：\",len(testX))\n",
    "#print(testX, testY )\n",
    "\n",
    "\n",
    "# 数据重构为4D [samples, subsequences, timesteps, features]\n",
    "trainX_input4D = np.reshape(trainX, (trainX.shape[0],1,timestep,dim))\n",
    "testX_input4D = np.reshape(testX, (testX.shape[0],1,timestep,dim))\n",
    "print('构造得到模型的输入数据(训练数据已有标签trainY): ',trainX_input4D.shape,testX_input4D.shape)\n",
    "\n",
    "\n",
    "# create and fit the convlstm network\n",
    "if __name__ == '__main__':\n",
    "    model = Sequential()\n",
    "    model.add(TimeDistributed(Conv1D(filters=36, kernel_size=3, activation='relu', input_shape=(None,1, testX.shape[1]))))\n",
    "    model.add(TimeDistributed(MaxPooling1D(pool_size=1)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "    model.add(LSTM(60,activation='relu',return_sequences=True))\n",
    "    model.add(LSTM(12,activation='relu',return_sequences=True))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    model.fit(trainX_input4D, trainY, epochs=1000)\n",
    "    \n",
    "\n",
    "\n",
    "    # 打印模型\n",
    "    model.summary()\n",
    "\n",
    "    # 开始预测\n",
    "    trainPredict = model.predict(trainX_input4D)\n",
    "    testPredict = model.predict(testX_input4D)\n",
    "\n",
    "\n",
    "    # 逆缩放预测值\n",
    "    dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD.csv', engine='python')\n",
    "    dataset = dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "    Y = dataset[:,0]\n",
    "    #trainPredict = trainPredict*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori = trainY*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori=trainY_re.reshape((246,1))\n",
    "    testPredict = testPredict*((np.max(Y)-np.min(Y)))+np.min(Y)\n",
    "    testPre = testPredict.reshape(-1)\n",
    "    Y = dataset[:,0]\n",
    "    Y = Y[train_size+timestep:len(dataset)]\n",
    "    #testY_ori = testY*((np.max(dataset)-np.min(dataset)))+np.min(dataset)\n",
    "    #testY_ori = testY_ori.reshape((-1,1))\n",
    "    testY_ori = Y.reshape(-1)\n",
    "\n",
    "    \n",
    "\n",
    "    # 计算误差\n",
    "    #trainScore = math.sqrt(mean_squared_error(trainY_ori[:,0], trainPredict[:,0]))\n",
    "    #print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    #testScore = math.sqrt(mean_squared_error(testY_ori[:,0], testPredict[:,0]))\n",
    "    #print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "\n",
    "    error = []#Y-Y'\n",
    "    error1= []#abs((Y-Y')/Y)\n",
    "    error2= []#Y*Y'\n",
    "    squared1 =[]#Y*Y\n",
    "    squared2 =[]#Y'*Y'\n",
    "    for i in range(len(testY_ori)):\n",
    "        error.append(testY_ori[i] - testPre[i])\n",
    "        error1.append(abs((testY_ori[i] - testPre[i])/testY_ori[i]))\n",
    "        error2.append(testY_ori[i]*testPre[i])\n",
    "        squared1.append(testY_ori[i]*testY_ori[i])\n",
    "        squared2.append(testPre[i]*testPre[i])\n",
    "        \n",
    "        \n",
    "    squaredError = []#(Y-Y')^2\n",
    "    absError = []#abs(Y-Y')\n",
    "    for val in error:    \n",
    "        squaredError.append(val * val)#target-prediction之差平方     \n",
    "        absError.append(abs(val))#误差绝对值\n",
    "        \n",
    "    MSE=sum(squaredError) / len(squaredError)\n",
    "    meannn=np.mean(testY_ori)\n",
    "    NMSEerror = []\n",
    "    IAerror = []\n",
    "    for i in range(len(testY_ori)):\n",
    "        NMSEerror.append(squaredError[i]/error2[i])\n",
    "        IAerror.append((abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn))*(abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn)))\n",
    "    \n",
    "    U2error1 = []\n",
    "    U2error2 = []\n",
    "    for i in range(len(testY_ori)-1):\n",
    "        U2error1.append(((testY_ori[i+1] - testPre[i+1])/testY_ori[i])*((testY_ori[i+1] - testPre[i+1])/testY_ori[i]))\n",
    "        U2error2.append(((testY_ori[i+1] - testPre[i])/testY_ori[i])*((testY_ori[i+1] - testPre[i])/testY_ori[i]))\n",
    "    print('\\n==========================')\n",
    "    MAE=sum(absError)/len(absError)\n",
    "    print('MAE=',MAE)\n",
    "    from math import sqrt\n",
    "    RMSE=sqrt(MSE)\n",
    "    print(\"RMSE = \", RMSE)#均方根误差RMSE\n",
    "    NMSE=sum(NMSEerror)\n",
    "    print(\"NMSE = \", NMSE)#误差平方的归一化平均值NMSE\n",
    "    MAPE=sum(error1)/len(error1)\n",
    "    print('MAPE=',MAPE)\n",
    "    IA=1-sum(squaredError)/sum(IAerror)\n",
    "    print('IA=',IA)#一致性指数\n",
    "    U1index=RMSE/(sqrt(sum(squared1)/len(squared1))+sqrt(sum(squared2)/len(squared2)))\n",
    "    print('U1=',U1index)\n",
    "    U2index=(sqrt(sum(U2error1)/len(testY_ori)))/(sqrt(sum(U2error2)/len(testY_ori)))\n",
    "    print('U2=',U2index)\n",
    "\n",
    "    #print(testPredict)\n",
    "    testPre=testPre.reshape(-1)\n",
    "    testPre=pd.DataFrame(testPre)\n",
    "    testPre.to_csv('C:/Users/Administrator/Desktop/XI-2/data/pre_result/cnnoutput.csv', header=False)\n",
    "    import csv\n",
    "    Evaluation_index = [MAE,RMSE,NMSE,MAPE,IA,U1index,U2index]\n",
    "    with open(\"C:/Users/Administrator/Desktop/XI-2/data/pre_result/cnnEvaluation.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file ,delimiter=',')\n",
    "        writer.writerow(Evaluation_index)\n",
    "    \n",
    "    plt.figure(facecolor='white')\n",
    "    plt.plot(testPre,color='green', label='Predict')\n",
    "    plt.plot(testY_ori,color='pink', label='Original')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    pres.append(testPre)\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))\n",
    "\n",
    "#CNN-LSTM-SED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbb8fd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#GRU_SED\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import merge\n",
    "from tensorflow.python.keras.layers.merge import Multiply\n",
    "from tensorflow.python.keras.layers.core import *\n",
    "from tensorflow.python.keras.models import *\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python.keras.layers import Input,Dense,Reshape,Dropout, Embedding, LSTM, Bidirectional,Permute\n",
    "from tensorflow.python.keras.layers import RepeatVector, TimeDistributed\n",
    "from tensorflow.python.keras.layers.recurrent import GRU\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import concat, read_csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sklearn.metrics as skm\n",
    "import math\n",
    "from math import sqrt\n",
    "import time\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "#1. load dataset\n",
    "dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_DJH.csv', engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "data = dataset.reshape(-1,6)\n",
    "\n",
    "\n",
    "timestep = 6\n",
    "dim = 2\n",
    "\n",
    "#数据缩放 拆分输入X（7维）&输出Y（1维）\n",
    "X = dataset[:,0:2]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "for i in range(dim):\n",
    "    Xdata = X[:,i]\n",
    "    Xdata = Xdata.reshape(-1,1)\n",
    "    Xdata = scaler.fit_transform(Xdata)\n",
    "    Xdata = Xdata.flatten()\n",
    "    X[:,i] = Xdata\n",
    "X_scaler = X.reshape(324,-1)\n",
    "print(X_scaler.shape)\n",
    "\n",
    "Y = dataset[:,0]\n",
    "Y_scaler= (Y-np.min(Y))/(np.max(Y)-np.min(Y))\n",
    "Y_scaler = Y_scaler.reshape(-1)\n",
    "print(Y_scaler.shape)\n",
    "\n",
    "\n",
    "#重构数据集\n",
    "##timestep为时间步长\n",
    "def create_X(seq, timestep):\n",
    "    dataX = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[i:(i+timestep)]\n",
    "        # X按照顺序取值 每次在后面增加一个数据\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "\n",
    "def create_Y(seq, timestep):\n",
    "    dataY = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        # Y向后移动一位取值\n",
    "        dataY.append(seq[i+timestep])\n",
    "    return np.array(dataY)\n",
    "\n",
    "#-------------------------------------------#\n",
    "#  建立注意力模型\n",
    "#-------------------------------------------#\n",
    "def get_activations(model, inputs, print_shape_only=False, layer_name=None):\n",
    "    # Documentation is available online on Github at the address below.\n",
    "    # From: https://github.com/philipperemy/keras-visualize-activations\n",
    "#    print('----- activations -----')\n",
    "    activations = []\n",
    "    inp = model.input\n",
    "    if layer_name is None:\n",
    "        outputs = [layer.output for layer in model.layers]\n",
    "    else:\n",
    "        outputs = [layer.output for layer in model.layers if layer.name == layer_name]  # all layer outputs\n",
    "    funcs = [K.function([inp] + [K.learning_phase()], [out]) for out in outputs]  # evaluation functions\n",
    "    layer_outputs = [func([inputs, 1])[0] for func in funcs]\n",
    "    for layer_activations in layer_outputs:\n",
    "        activations.append(layer_activations)\n",
    "#        if print_shape_only:\n",
    "#            print(layer_activations.shape)\n",
    "#        else:\n",
    "#            print(layer_activations)\n",
    "    return activations\n",
    "\n",
    "def attention_3d_block(inputs):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Dense(TIME_STEPS, activation='softmax')(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)#(batch_size, time_steps, input_dim)\n",
    "    output_attention_mul = Multiply()([inputs, a_probs])#(batch_size, time_steps, input_dim)\n",
    "    return output_attention_mul\n",
    "\n",
    "\n",
    "def get_gru_model():\n",
    "    K.clear_session() #清除之前的模型，省得压满内存\n",
    "    inputs = Input(shape=(timestep, dim,))\n",
    "    gru_units1 = 12\n",
    "    gru_units2 = 12\n",
    "    # (batch_size, time_steps, INPUT_DIM) -> (batch_size, input_dim, lstm_units)\n",
    "    gur_out1 = GRU(gru_units1,return_sequences=True)(inputs)\n",
    "    gur_out2 = GRU(gru_units2,return_sequences=True)(gur_out1)\n",
    "    # (batch_size, input_dim, lstm_units) -> (batch_size, input_dim*lstm_units)\n",
    "    dropout_out = Dropout(0.5)(gur_out2)\n",
    "    gur_out = Flatten()(dropout_out)\n",
    "    output = Dense(1, activation='sigmoid')(gur_out)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#预测\n",
    "pres=[]\n",
    "# 将数据拆分成训练和测试，7/9作为训练数据\n",
    "train_size = int(len(dataset) * 0.89)\n",
    "test_size = len(dataset) - train_size\n",
    "trainX, testX = X_scaler[0:train_size,:], X_scaler[train_size:len(dataset),:]\n",
    "trainY, testY = Y_scaler[0:train_size], Y_scaler[train_size:len(dataset)]\n",
    "print(\"原始训练集的长度：\",train_size)\n",
    "print(\"原始测试集的长度：\",test_size)\n",
    "#print(trainX,trainX.shape)\n",
    "#print(trainY,trainY.shape)\n",
    "#print(testX,testX.shape)\n",
    "#print(testY,testY.shape)\n",
    "\n",
    "train_X=create_X(trainX,timestep)#(246,6,6)\n",
    "#print(train_X,train_X.shape)\n",
    "train_Y=create_Y(trainY,timestep)#(246,)\n",
    "#print(train_Y,train_Y.shape)\n",
    "test_X=create_X(testX,timestep)#(66,6,6)\n",
    "#print(test_X,test_X.shape)\n",
    "test_Y=create_Y(testY,timestep)#(66,)\n",
    "#print(test_Y,test_Y.shape)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(6)\n",
    "        \n",
    "    model = get_gru_model()\n",
    "    optimizer = Adam(0.01)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(train_X, train_Y, epochs=1000, batch_size=64)\n",
    "\n",
    "    \n",
    "    # 开始预测\n",
    "    trainPredict = model.predict(train_X)\n",
    "    testPredict = model.predict(test_X)\n",
    "\n",
    "\n",
    "    # 逆缩放预测值\n",
    "    dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD.csv', engine='python')\n",
    "    dataset = dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "    Y = dataset[:,0]\n",
    "    #trainPredict = trainPredict*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori = trainY*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori=trainY_re.reshape((246,1))\n",
    "    testPre = testPredict*((np.max(Y)-np.min(Y)))+np.min(Y)\n",
    "    testPre = testPre.reshape(-1)\n",
    "    Y = dataset[:,0]\n",
    "    Y = Y[train_size+timestep:len(dataset)]\n",
    "    #testY_ori = testY*((np.max(dataset)-np.min(dataset)))+np.min(dataset)\n",
    "    #testY_ori = testY_ori.reshape((-1,1))\n",
    "    testY_ori = Y.reshape(-1)\n",
    "\n",
    "    \n",
    "\n",
    "    # 计算误差\n",
    "    #trainScore = math.sqrt(mean_squared_error(trainY_ori[:,0], trainPredict[:,0]))\n",
    "    #print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    #testScore = math.sqrt(mean_squared_error(testY_ori[:,0], testPredict[:,0]))\n",
    "    #print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "\n",
    "    error = []#Y-Y'\n",
    "    error1= []#abs((Y-Y')/Y)\n",
    "    error2= []#Y*Y'\n",
    "    squared1 =[]#Y*Y\n",
    "    squared2 =[]#Y'*Y'\n",
    "    for i in range(len(testY_ori)):\n",
    "        error.append(testY_ori[i] - testPre[i])\n",
    "        error1.append(abs((testY_ori[i] - testPre[i])/testY_ori[i]))\n",
    "        error2.append(testY_ori[i]*testPre[i])\n",
    "        squared1.append(testY_ori[i]*testY_ori[i])\n",
    "        squared2.append(testPre[i]*testPre[i])\n",
    "        \n",
    "        \n",
    "    squaredError = []#(Y-Y')^2\n",
    "    absError = []#abs(Y-Y')\n",
    "    for val in error:    \n",
    "        squaredError.append(val * val)#target-prediction之差平方     \n",
    "        absError.append(abs(val))#误差绝对值\n",
    "        \n",
    "    MSE=sum(squaredError) / len(squaredError)\n",
    "    meannn=np.mean(testY_ori)\n",
    "    NMSEerror = []\n",
    "    IAerror = []\n",
    "    for i in range(len(testY_ori)):\n",
    "        NMSEerror.append(squaredError[i]/error2[i])\n",
    "        IAerror.append((abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn))*(abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn)))\n",
    "    \n",
    "    U2error1 = []\n",
    "    U2error2 = []\n",
    "    for i in range(len(testY_ori)-1):\n",
    "        U2error1.append(((testY_ori[i+1] - testPre[i+1])/testY_ori[i])*((testY_ori[i+1] - testPre[i+1])/testY_ori[i]))\n",
    "        U2error2.append(((testY_ori[i+1] - testPre[i])/testY_ori[i])*((testY_ori[i+1] - testPre[i])/testY_ori[i]))\n",
    "    print('\\n==========================')\n",
    "    MAE=sum(absError)/len(absError)\n",
    "    print('MAE=',MAE)\n",
    "    from math import sqrt\n",
    "    RMSE=sqrt(MSE)\n",
    "    print(\"RMSE = \", RMSE)#均方根误差RMSE\n",
    "    NMSE=sum(NMSEerror)\n",
    "    print(\"NMSE = \", NMSE)#误差平方的归一化平均值NMSE\n",
    "    MAPE=sum(error1)/len(error1)\n",
    "    print('MAPE=',MAPE)\n",
    "    IA=1-sum(squaredError)/sum(IAerror)\n",
    "    print('IA=',IA)#一致性指数\n",
    "    U1index=RMSE/(sqrt(sum(squared1)/len(squared1))+sqrt(sum(squared2)/len(squared2)))\n",
    "    print('U1=',U1index)\n",
    "    U2index=(sqrt(sum(U2error1)/len(testY_ori)))/(sqrt(sum(U2error2)/len(testY_ori)))\n",
    "    print('U2=',U2index)\n",
    "\n",
    "    #print(testPredict)\n",
    "    testPre=testPre.reshape(-1)\n",
    "    testPre=pd.DataFrame(testPre)\n",
    "    testPre.to_csv('C:/Users/Administrator/Desktop/XI-2/data/pre_result/gruoutput.csv', header=False)\n",
    "    import csv\n",
    "    Evaluation_index = [MAE,RMSE,NMSE,MAPE,IA,U1index,U2index]\n",
    "    with open(\"C:/Users/Administrator/Desktop/XI-2/data/pre_result/gruEvaluation.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file ,delimiter=',')\n",
    "        writer.writerow(Evaluation_index)\n",
    "    \n",
    "    plt.figure(facecolor='white')\n",
    "    plt.plot(testPre,color='green', label='Predict')\n",
    "    plt.plot(testY_ori,color='pink', label='Original')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    pres.append(testPre)\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))\n",
    "\n",
    "#GRU-SED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf953b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM-SED\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import merge\n",
    "from tensorflow.python.keras.layers.merge import Multiply\n",
    "from tensorflow.python.keras.layers.core import *\n",
    "from tensorflow.python.keras.models import *\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python.keras.layers import Input,Dense,Reshape,Dropout, Embedding, LSTM, Bidirectional,Permute\n",
    "from tensorflow.python.keras.layers import RepeatVector, TimeDistributed\n",
    "from tensorflow.python.keras.layers.recurrent import GRU\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import concat, read_csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sklearn.metrics as skm\n",
    "import math\n",
    "from math import sqrt\n",
    "import time\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "#1. load dataset\n",
    "dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_DJH.csv', engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "data = dataset.reshape(-1,6)\n",
    "\n",
    "\n",
    "timestep = 6\n",
    "dim = 2\n",
    "\n",
    "#数据缩放 拆分输入X（7维）&输出Y（1维）\n",
    "X = dataset[:,0:2]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "for i in range(dim):\n",
    "    Xdata = X[:,i]\n",
    "    Xdata = Xdata.reshape(-1,1)\n",
    "    Xdata = scaler.fit_transform(Xdata)\n",
    "    Xdata = Xdata.flatten()\n",
    "    X[:,i] = Xdata\n",
    "X_scaler = X.reshape(324,-1)\n",
    "print(X_scaler.shape)\n",
    "\n",
    "Y = dataset[:,0]\n",
    "Y_scaler= (Y-np.min(Y))/(np.max(Y)-np.min(Y))\n",
    "Y_scaler = Y_scaler.reshape(-1)\n",
    "print(Y_scaler.shape)\n",
    "\n",
    "\n",
    "#重构数据集\n",
    "##timestep为时间步长\n",
    "def create_X(seq, timestep):\n",
    "    dataX = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[i:(i+timestep)]\n",
    "        # X按照顺序取值 每次在后面增加一个数据\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "\n",
    "def create_Y(seq, timestep):\n",
    "    dataY = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        # Y向后移动一位取值\n",
    "        dataY.append(seq[i+timestep])\n",
    "    return np.array(dataY)\n",
    "\n",
    "\n",
    "\n",
    "def get_lstm_model():\n",
    "    K.clear_session() #清除之前的模型，省得压满内存\n",
    "    inputs = Input(shape=(timestep, dim,))\n",
    "    lstm_units1 = 12\n",
    "    lstm_units2 = 36\n",
    "    lstm_out1 = LSTM(lstm_units1, return_sequences=True)(inputs)\n",
    "    lstm_out2 = LSTM(lstm_units2, return_sequences=True)(lstm_out1)\n",
    "    dropout_out = Dropout(0.5)(lstm_out2)\n",
    "    lstm_out = Flatten()(dropout_out)\n",
    "    output = Dense(1, activation='relu')(lstm_out)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#预测\n",
    "pres=[]\n",
    "# 将数据拆分成训练和测试，7/9作为训练数据\n",
    "train_size = int(len(dataset) * 0.89)\n",
    "test_size = len(dataset) - train_size\n",
    "trainX, testX = X_scaler[0:train_size], X_scaler[train_size:len(dataset)]\n",
    "trainY, testY = Y_scaler[0:train_size], Y_scaler[train_size:len(dataset)]\n",
    "print(\"原始训练集的长度：\",train_size)\n",
    "print(\"原始测试集的长度：\",test_size)\n",
    "#print(trainX,trainX.shape)\n",
    "#print(trainY,trainY.shape)\n",
    "#print(testX,testX.shape)\n",
    "#print(testY,testY.shape)\n",
    "\n",
    "train_X=create_X(trainX,timestep)#(246,6)\n",
    "#print(train_X,train_X.shape)\n",
    "train_Y=create_Y(trainY,timestep)#(246,)\n",
    "#print(train_Y,train_Y.shape)\n",
    "test_X=create_X(testX,timestep)#(66,6,6)\n",
    "#print(test_X,test_X.shape)\n",
    "test_Y=create_Y(testY,timestep)#(66,)\n",
    "#print(test_Y,test_Y.shape)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        \n",
    "    model = get_lstm_model()\n",
    "    optimizer = Adam(0.01)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(train_X, train_Y, epochs=1000, batch_size=64)\n",
    "\n",
    "    \n",
    "    # 开始预测\n",
    "    trainPredict = model.predict(train_X)\n",
    "    testPredict = model.predict(test_X)\n",
    "\n",
    "\n",
    "    # 逆缩放预测值\n",
    "    dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD.csv', engine='python')\n",
    "    dataset = dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "    Y = dataset[:,0]\n",
    "    #trainPredict = trainPredict*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori = trainY*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori=trainY_re.reshape((246,1))\n",
    "    testPre = testPredict*((np.max(Y)-np.min(Y)))+np.min(Y)\n",
    "    testPre = testPre.reshape(-1)\n",
    "    Y = dataset[:,0]\n",
    "    Y = Y[train_size+timestep:len(dataset)]\n",
    "    #testY_ori = testY*((np.max(dataset)-np.min(dataset)))+np.min(dataset)\n",
    "    #testY_ori = testY_ori.reshape((-1,1))\n",
    "    testY_ori = Y.reshape(-1)\n",
    "\n",
    "    \n",
    "\n",
    "    # 计算误差\n",
    "    #trainScore = math.sqrt(mean_squared_error(trainY_ori[:,0], trainPredict[:,0]))\n",
    "    #print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    #testScore = math.sqrt(mean_squared_error(testY_ori[:,0], testPredict[:,0]))\n",
    "    #print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "\n",
    "    error = []#Y-Y'\n",
    "    error1= []#abs((Y-Y')/Y)\n",
    "    error2= []#Y*Y'\n",
    "    squared1 =[]#Y*Y\n",
    "    squared2 =[]#Y'*Y'\n",
    "    for i in range(len(testY_ori)):\n",
    "        error.append(testY_ori[i] - testPre[i])\n",
    "        error1.append(abs((testY_ori[i] - testPre[i])/testY_ori[i]))\n",
    "        error2.append(testY_ori[i]*testPre[i])\n",
    "        squared1.append(testY_ori[i]*testY_ori[i])\n",
    "        squared2.append(testPre[i]*testPre[i])\n",
    "        \n",
    "        \n",
    "    squaredError = []#(Y-Y')^2\n",
    "    absError = []#abs(Y-Y')\n",
    "    for val in error:    \n",
    "        squaredError.append(val * val)#target-prediction之差平方     \n",
    "        absError.append(abs(val))#误差绝对值\n",
    "        \n",
    "    MSE=sum(squaredError) / len(squaredError)\n",
    "    meannn=np.mean(testY_ori)\n",
    "    NMSEerror = []\n",
    "    IAerror = []\n",
    "    for i in range(len(testY_ori)):\n",
    "        NMSEerror.append(squaredError[i]/error2[i])\n",
    "        IAerror.append((abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn))*(abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn)))\n",
    "    \n",
    "    U2error1 = []\n",
    "    U2error2 = []\n",
    "    for i in range(len(testY_ori)-1):\n",
    "        U2error1.append(((testY_ori[i+1] - testPre[i+1])/testY_ori[i])*((testY_ori[i+1] - testPre[i+1])/testY_ori[i]))\n",
    "        U2error2.append(((testY_ori[i+1] - testPre[i])/testY_ori[i])*((testY_ori[i+1] - testPre[i])/testY_ori[i]))\n",
    "    print('\\n==========================')\n",
    "    MAE=sum(absError)/len(absError)\n",
    "    print('MAE=',MAE)\n",
    "    from math import sqrt\n",
    "    RMSE=sqrt(MSE)\n",
    "    print(\"RMSE = \", RMSE)#均方根误差RMSE\n",
    "    NMSE=sum(NMSEerror)\n",
    "    print(\"NMSE = \", NMSE)#误差平方的归一化平均值NMSE\n",
    "    MAPE=sum(error1)/len(error1)\n",
    "    print('MAPE=',MAPE)\n",
    "    IA=1-sum(squaredError)/sum(IAerror)\n",
    "    print('IA=',IA)#一致性指数\n",
    "    U1index=RMSE/(sqrt(sum(squared1)/len(squared1))+sqrt(sum(squared2)/len(squared2)))\n",
    "    print('U1=',U1index)\n",
    "    U2index=(sqrt(sum(U2error1)/len(testY_ori)))/(sqrt(sum(U2error2)/len(testY_ori)))\n",
    "    print('U2=',U2index)\n",
    "\n",
    "    #print(testPredict)\n",
    "    testPre=testPre.reshape(-1)\n",
    "    testPre=pd.DataFrame(testPre)\n",
    "    testPre.to_csv('C:/Users/Administrator/Desktop/XI-2/data/pre_result/lstmoutput.csv', header=False)\n",
    "    import csv\n",
    "    Evaluation_index = [MAE,RMSE,NMSE,MAPE,IA,U1index,U2index]\n",
    "    with open(\"C:/Users/Administrator/Desktop/XI-2/data/pre_result/lstmEvaluation.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file ,delimiter=',')\n",
    "        writer.writerow(Evaluation_index)\n",
    "    \n",
    "    plt.figure(facecolor='white')\n",
    "    plt.plot(testPre,color='green', label='Predict')\n",
    "    plt.plot(testY_ori,color='pink', label='Original')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    pres.append(testPre)\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))\n",
    "\n",
    "#LSTM-SED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a995d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bi-LSTM-SED\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import merge\n",
    "from tensorflow.python.keras.layers.merge import Multiply\n",
    "from tensorflow.python.keras.layers.core import *\n",
    "from tensorflow.python.keras.models import *\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python.keras.layers import Input,Dense,Reshape,Dropout, Embedding, LSTM, Bidirectional,Permute\n",
    "from tensorflow.python.keras.layers import RepeatVector, TimeDistributed\n",
    "from tensorflow.python.keras.layers.recurrent import GRU\n",
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam,SGD\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pandas import concat, read_csv\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sklearn.metrics as skm\n",
    "import math\n",
    "from math import sqrt\n",
    "import time\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "\n",
    "#1. load dataset\n",
    "dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD_DJH.csv', engine='python')\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "data = dataset.reshape(-1,6)\n",
    "\n",
    "\n",
    "timestep = 6\n",
    "dim = 2\n",
    "\n",
    "#数据缩放 拆分输入X（7维）&输出Y（1维）\n",
    "X = dataset[:,0:2]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "for i in range(dim):\n",
    "    Xdata = X[:,i]\n",
    "    Xdata = Xdata.reshape(-1,1)\n",
    "    Xdata = scaler.fit_transform(Xdata)\n",
    "    Xdata = Xdata.flatten()\n",
    "    X[:,i] = Xdata\n",
    "X_scaler = X.reshape(324,-1)\n",
    "print(X_scaler.shape)\n",
    "\n",
    "Y = dataset[:,0]\n",
    "Y_scaler= (Y-np.min(Y))/(np.max(Y)-np.min(Y))\n",
    "Y_scaler = Y_scaler.reshape(-1)\n",
    "print(Y_scaler.shape)\n",
    "\n",
    "\n",
    "#重构数据集\n",
    "##timestep为时间步长\n",
    "def create_X(seq, timestep):\n",
    "    dataX = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        a = seq[i:(i+timestep)]\n",
    "        # X按照顺序取值 每次在后面增加一个数据\n",
    "        dataX.append(a)\n",
    "    return np.array(dataX)\n",
    "\n",
    "def create_Y(seq, timestep):\n",
    "    dataY = []\n",
    "    for i in range(len(seq)-timestep):\n",
    "        # Y向后移动一位取值\n",
    "        dataY.append(seq[i+timestep])\n",
    "    return np.array(dataY)\n",
    "\n",
    "\n",
    "\n",
    "def get_bilstm_model():\n",
    "    K.clear_session() #清除之前的模型，省得压满内存\n",
    "    inputs = Input(shape=(timestep, dim,))\n",
    "    lstm_units1 = 12\n",
    "    lstm_units2 = 36\n",
    "    bilstm_out1 = Bidirectional(LSTM(lstm_units1,return_sequences=True),merge_mode='concat')(inputs)\n",
    "    bilstm_out2 = LSTM(lstm_units2, return_sequences=True)(bilstm_out1)\n",
    "    dropout_out = Dropout(0.5)(bilstm_out2)\n",
    "    bilstm_out = Flatten()(dropout_out)\n",
    "    output = Dense(1, activation='relu')(bilstm_out)\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "#预测\n",
    "pres=[]\n",
    "# 将数据拆分成训练和测试，7/9作为训练数据\n",
    "train_size = int(len(dataset) * 0.89)\n",
    "test_size = len(dataset) - train_size\n",
    "trainX, testX = X_scaler[0:train_size], X_scaler[train_size:len(dataset)]\n",
    "trainY, testY = Y_scaler[0:train_size], Y_scaler[train_size:len(dataset)]\n",
    "print(\"原始训练集的长度：\",train_size)\n",
    "print(\"原始测试集的长度：\",test_size)\n",
    "#print(trainX,trainX.shape)\n",
    "#print(trainY,trainY.shape)\n",
    "#print(testX,testX.shape)\n",
    "#print(testY,testY.shape)\n",
    "\n",
    "train_X=create_X(trainX,timestep)#(246,6)\n",
    "#print(train_X,train_X.shape)\n",
    "train_Y=create_Y(trainY,timestep)#(246,)\n",
    "#print(train_Y,train_Y.shape)\n",
    "test_X=create_X(testX,timestep)#(66,6,6)\n",
    "#print(test_X,test_X.shape)\n",
    "test_Y=create_Y(testY,timestep)#(66,)\n",
    "#print(test_Y,test_Y.shape)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "        \n",
    "    model = get_bilstm_model()\n",
    "    optimizer = Adam(0.01)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    print(model.summary())\n",
    "    \n",
    "    model.fit(train_X, train_Y, epochs=1000, batch_size=64)\n",
    "\n",
    "    # 开始预测\n",
    "    trainPredict = model.predict(train_X)\n",
    "    testPredict = model.predict(test_X)\n",
    "\n",
    "\n",
    "    # 逆缩放预测值\n",
    "    dataframe = read_csv('C:/Users/Administrator/Desktop/XI-2/data/try2ed/PD.csv', engine='python')\n",
    "    dataset = dataframe.values\n",
    "    dataset = dataset.astype('float32')\n",
    "    Y = dataset[:,0]\n",
    "    #trainPredict = trainPredict*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori = trainY*((numpy.max(Y)-numpy.min(Y)))+numpy.min(Y)\n",
    "    #trainY_ori=trainY_re.reshape((246,1))\n",
    "    testPre = testPredict*((np.max(Y)-np.min(Y)))+np.min(Y)\n",
    "    testPre = testPre.reshape(-1)\n",
    "    Y = dataset[:,0]\n",
    "    Y = Y[train_size+timestep:len(dataset)]\n",
    "    #testY_ori = testY*((np.max(dataset)-np.min(dataset)))+np.min(dataset)\n",
    "    #testY_ori = testY_ori.reshape((-1,1))\n",
    "    testY_ori = Y.reshape(-1)\n",
    "\n",
    "    \n",
    "\n",
    "    # 计算误差\n",
    "    #trainScore = math.sqrt(mean_squared_error(trainY_ori[:,0], trainPredict[:,0]))\n",
    "    #print('Train Score: %.2f RMSE' % (trainScore))\n",
    "    #testScore = math.sqrt(mean_squared_error(testY_ori[:,0], testPredict[:,0]))\n",
    "    #print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n",
    "\n",
    "    error = []#Y-Y'\n",
    "    error1= []#abs((Y-Y')/Y)\n",
    "    error2= []#Y*Y'\n",
    "    squared1 =[]#Y*Y\n",
    "    squared2 =[]#Y'*Y'\n",
    "    for i in range(len(testY_ori)):\n",
    "        error.append(testY_ori[i] - testPre[i])\n",
    "        error1.append(abs((testY_ori[i] - testPre[i])/testY_ori[i]))\n",
    "        error2.append(testY_ori[i]*testPre[i])\n",
    "        squared1.append(testY_ori[i]*testY_ori[i])\n",
    "        squared2.append(testPre[i]*testPre[i])\n",
    "        \n",
    "        \n",
    "    squaredError = []#(Y-Y')^2\n",
    "    absError = []#abs(Y-Y')\n",
    "    for val in error:    \n",
    "        squaredError.append(val * val)#target-prediction之差平方     \n",
    "        absError.append(abs(val))#误差绝对值\n",
    "        \n",
    "    MSE=sum(squaredError) / len(squaredError)\n",
    "    meannn=np.mean(testY_ori)\n",
    "    NMSEerror = []\n",
    "    IAerror = []\n",
    "    for i in range(len(testY_ori)):\n",
    "        NMSEerror.append(squaredError[i]/error2[i])\n",
    "        IAerror.append((abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn))*(abs(testY_ori[i] - meannn)+abs(testPre[i] - meannn)))\n",
    "    \n",
    "    U2error1 = []\n",
    "    U2error2 = []\n",
    "    for i in range(len(testY_ori)-1):\n",
    "        U2error1.append(((testY_ori[i+1] - testPre[i+1])/testY_ori[i])*((testY_ori[i+1] - testPre[i+1])/testY_ori[i]))\n",
    "        U2error2.append(((testY_ori[i+1] - testPre[i])/testY_ori[i])*((testY_ori[i+1] - testPre[i])/testY_ori[i]))\n",
    "    print('\\n==========================')\n",
    "    MAE=sum(absError)/len(absError)\n",
    "    print('MAE=',MAE)\n",
    "    from math import sqrt\n",
    "    RMSE=sqrt(MSE)\n",
    "    print(\"RMSE = \", RMSE)#均方根误差RMSE\n",
    "    NMSE=sum(NMSEerror)\n",
    "    print(\"NMSE = \", NMSE)#误差平方的归一化平均值NMSE\n",
    "    MAPE=sum(error1)/len(error1)\n",
    "    print('MAPE=',MAPE)\n",
    "    IA=1-sum(squaredError)/sum(IAerror)\n",
    "    print('IA=',IA)#一致性指数\n",
    "    U1index=RMSE/(sqrt(sum(squared1)/len(squared1))+sqrt(sum(squared2)/len(squared2)))\n",
    "    print('U1=',U1index)\n",
    "    U2index=(sqrt(sum(U2error1)/len(testY_ori)))/(sqrt(sum(U2error2)/len(testY_ori)))\n",
    "    print('U2=',U2index)\n",
    "\n",
    "    #print(testPredict)\n",
    "    testPre=testPre.reshape(-1)\n",
    "    testPre=pd.DataFrame(testPre)\n",
    "    testPre.to_csv('C:/Users/Administrator/Desktop/XI-2/data/pre_result/lstmoutput.csv', header=False)\n",
    "    import csv\n",
    "    Evaluation_index = [MAE,RMSE,NMSE,MAPE,IA,U1index,U2index]\n",
    "    with open(\"C:/Users/Administrator/Desktop/XI-2/data/pre_result/lstmEvaluation.csv\", \"a\", newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file ,delimiter=',')\n",
    "        writer.writerow(Evaluation_index)\n",
    "    \n",
    "    plt.figure(facecolor='white')\n",
    "    plt.plot(testPre,color='green', label='Predict')\n",
    "    plt.plot(testY_ori,color='pink', label='Original')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    pres.append(testPre)\n",
    "\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print('Running time: %d seconds'%(end - start))\n",
    "\n",
    "#Bi-LSTM-SED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095ac58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
